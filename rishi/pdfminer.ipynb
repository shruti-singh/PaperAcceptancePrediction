{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletxt_1 = convert_pdf_to_txt('pdf/accepted/B1ae1lZRb.pdf')\n",
    "sampletxt_2 = convert_pdf_to_txt('pdf/accepted/B1al7jg0b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "APPRENTICE: USING KNOWLEDGE DISTILLATION\n",
      "TECHNIQUES TO IMPROVE LOW-PRECISION NET-\n",
      "WORK ACCURACY\n",
      "\n",
      "Asit Mishra & Debbie Marr\n",
      "Accelerator Architecture Lab\n",
      "Intel Labs\n",
      "{asit.k.mishra,debbie.marr}@intel.com\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Deep learning networks have achieved state-of-the-art accuracies on computer vi-\n",
      "sion workloads like image classiﬁcation and object detection. The performant\n",
      "systems, however, typically involve big models with numerous parameters. Once\n",
      "trained, a challenging aspect for such top performing models is deployment on re-\n",
      "source constrained inference systems — the models (often deep networks or wide\n",
      "networks or both) are compute and memory intensive. Low-precision numerics\n",
      "and model compression using knowledge distillation are popular techniques to\n",
      "lower both the compute requirements and memory footprint of these deployed\n",
      "models. In this paper, we study combination of these two techniques and show\n",
      "that the performance of low-precision networks can be signiﬁcantly improved\n",
      "by using knowledge distillation techniques. Our approach, Apprentice, achieves\n",
      "state-of-the-art accuracies using ternary precision and 4-bit precision for variants\n",
      "of ResNet architecture on ImageNet dataset. We present three schemes using\n",
      "which one can apply knowledge distillation techniques to various stages of the\n",
      "train-and-deploy pipeline.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Background: Today’s high performing deep neural networks (DNNs) for computer vision applica-\n",
      "tions comprise of multiple layers and involve numerous parameters. These networks have O(Giga-\n",
      "FLOPS) compute requirements and generate models which are O(Mega-Bytes) in storage (Canziani\n",
      "et al., 2016). Further, the memory and compute requirements during training and inference are quite\n",
      "different (Mishra et al., 2017). Training is performed on big datasets with large batch-sizes where\n",
      "memory footprint of activations dominates the model memory footprint. On the other hand, batch-\n",
      "size during inference is typically small and the model’s memory footprint dominates the runtime\n",
      "memory requirements.\n",
      "\n",
      "Because of complexity in compute, memory and storage requirements, training phase of the net-\n",
      "works is performed on CPU and/or GPU clusters in a distributed computing environment. Once\n",
      "trained, a challenging aspect is deployment of trained models on resource constrained inference\n",
      "systems such as portable devices or sensor networks, and for applications in which real-time predic-\n",
      "tions are required. Performing inference on edge-devices comes with severe constraints on memory,\n",
      "compute and power. Additionally, ensemble based methods, which one can potentially use to get\n",
      "improved accuracy predictions, become prohibitive in resource constrained systems.\n",
      "\n",
      "Quantization using low-precision numerics (Vanhoucke et al., 2011; Zhou et al., 2016; Lin et al.,\n",
      "2015; Miyashita et al., 2016; Gupta et al., 2015; Zhu et al., 2016; Rastegari et al., 2016; Courbariaux\n",
      "et al., 2015; Umuroglu et al., 2016; Mishra et al., 2017) and model compression (Buciluˇa et al.,\n",
      "2006; Hinton et al., 2015; Romero et al., 2014) have emerged as popular solutions for resource\n",
      "constrained deployment scenarios. With quantization, a low-precision version of network model is\n",
      "generated and deployed on the device. Operating in lower precision mode reduces compute as well\n",
      "as data movement and storage requirements. However, majority of existing works in low-precision\n",
      "DNNs sacriﬁce accuracy over baseline full-precision networks. With model compression, a smaller\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "low memory footprint network is trained to mimic the behaviour of the original complex network.\n",
      "During this training, a process called, knowledge distillation is used to “transfer knowledge” from\n",
      "the complex network to the smaller network. Work by Hinton et al. (2015) shows that the knowledge\n",
      "distillation scheme can yield networks at comparable or slightly better accuracy than the original\n",
      "complex model. However, to the best of our knowledge, all prior works using model compression\n",
      "techniques target compression at full-precision.\n",
      "\n",
      "Our proposal: In this paper, we study the combination of network quantization with model com-\n",
      "pression and show that accuracies of low-precision networks can be signiﬁcantly improved by using\n",
      "knowledge distillation techniques. Previous studies on model compression use a large network as\n",
      "the teacher network and a small network as the student network. The small student network learns\n",
      "from teacher network using distillation process. The network architecture of the student network\n",
      "is typically different from that of the teacher network – for e.g. Hinton et al. (2015) investigate a\n",
      "student network that has fewer number of neurons in the hidden layers compared to the teacher net-\n",
      "work. In our work, the student network has similar topology as that of teacher network, except that\n",
      "the student network has low-precision neurons compared to the teacher network which has neurons\n",
      "operating at full-precision.\n",
      "\n",
      "We call our approach Apprentice1 and study three schemes which produce low-precision net-\n",
      "works using knowledge distillation techniques. Each of these three schemes produce state-of-the-art\n",
      "ternary precision and 4-bit precision models.\n",
      "\n",
      "In the ﬁrst scheme, a low-precision network and a full-precision network are jointly trained from\n",
      "scratch using knowledge distillation scheme. Later in the paper we describe the rationale behind\n",
      "this approach. Using this scheme, a new state-of-the-art accuracy is obtained for ternary and 4-bit\n",
      "precision for ResNet-18, ResNet-34 and ResNet-50 on ImageNet dataset. In fact, using this scheme\n",
      "the accuracy of the full-precision model also slightly improves. This scheme then serves as the new\n",
      "baseline for the other two schemes we investigate.\n",
      "\n",
      "In the second scheme, we start with a full-precision trained network and transfer knowledge from\n",
      "this trained network continuously to train a low-precision network from scratch. We ﬁnd that the\n",
      "low-precision network converges faster (albeit to similar accuracies as the ﬁrst scheme) when a\n",
      "trained complex network guides its training.\n",
      "\n",
      "In the third scheme, we start with a trained full-precision large network and an apprentice network\n",
      "that has been initialised with full-precision weights. The apprentice network’s precision is lowered\n",
      "and is ﬁne-tuned using knowledge distillation techniques. We ﬁnd that the low-precision network’s\n",
      "accuracy marginally improves and surpasses the accuracy obtained via the ﬁrst scheme. This scheme\n",
      "then sets the new state-of-the-art accuracies for the ResNet models at ternary and 4-bit precision.\n",
      "\n",
      "Overall, the contributions of this paper are the techniques to obtain low-precision DNNs using\n",
      "knowledge distillation technique. Each of our scheme produces a low-precision model that sur-\n",
      "passes the accuracy of the equivalent low-precision model published to date. One of our schemes\n",
      "also helps a low-precision model converge faster. We envision these accurate low-precision models\n",
      "to simplify the inference deployment process on resource constrained systems and even otherwise\n",
      "on cloud-based deployment systems.\n",
      "\n",
      "2 MOTIVATION FOR LOW-PRECISION MODEL PARAMETERS\n",
      "\n",
      "Lowering precision of model parameters: Resource constrained inference systems impose signif-\n",
      "icant restrictions on memory, compute and power budget. With regard to storage, model (or weight)\n",
      "parameters and activation maps occupy memory during the inference phase of DNNs. During this\n",
      "phase memory is allocated for input (IFM) and output feature maps (OFM) required by a single\n",
      "layer in the DNN, and these dynamic memory allocations are reused for other layers. The total\n",
      "memory allocation during inference is then the maximum of IFM and maximum of OFM memory\n",
      "required across all the layers plus the sum of all weight tensors (Mishra et al., 2017). When infer-\n",
      "ence phase for DNNs is performed with a small batch size, the memory footprint of the weights\n",
      "\n",
      "1Dictionary deﬁnes apprentice as a person who is learning a trade from a skilled employer, having agreed to\n",
      "work for a ﬁxed period at low wages. In our work, the apprentice is a low-precision network which is learning\n",
      "the knowledge of a high precision network (skilled employer) during a ﬁxed number of epochs.\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "exceeds the footprint of the activation maps. This aspect is shown in Figure 1 for 4 different net-\n",
      "works (AlexNet (Krizhevsky et al., 2012), Inception-Resnet-v2 (Szegedy et al., 2016), ResNet-50\n",
      "and ResNet-101 (He et al., 2015)) running 224x224 image patches. Thus lowering the precision of\n",
      "the weight tensors helps lower the memory requirements during deployment. One other aspect of\n",
      "lowering memory footprint is that the working set size of the workload starts to ﬁt on chip and by\n",
      "reducing accesses to DRAM (off-chip) memory, the compute core starts to see better performance\n",
      "and energy savings (DRAM accesses are expensive in latency and energy).\n",
      "\n",
      "Beneﬁt of low-precision compute: Low-precision\n",
      "compute simpliﬁes hardware implementation. For\n",
      "example, the compute unit to perform the convolu-\n",
      "tion operation (multiplication of two operands) in-\n",
      "volves a ﬂoating-point multiplier when using full-\n",
      "precision weights and activations. The ﬂoating-\n",
      "point multiplier can be replaced with a much sim-\n",
      "pler circuitry (xnor and popcount logic elements)\n",
      "when using binary precision for weights and activa-\n",
      "tions (Courbariaux & Bengio, 2016; Rastegari et al.,\n",
      "2016; Courbariaux et al., 2015). Similarly, when us-\n",
      "ing ternary precision for weights and full-precision\n",
      "for activations, the multiplier unit can be replaced\n",
      "with a sign comparator unit. Simpler hardware also\n",
      "helps lower the inference latency and energy bud-\n",
      "get. Thus, operating in lower precision mode re-\n",
      "duces compute as well as data movement and storage\n",
      "requirements.\n",
      "\n",
      "Figure 1: Memory footprint of activations\n",
      "(ACTs) and weights (W) during inference for\n",
      "mini-batch sizes 1 and 8.\n",
      "\n",
      "The drawback of low-precision models, however, is degraded accuracy. We discuss later in the paper\n",
      "the network accuracies obtained using methods proposed in literature. These accuracies serve as the\n",
      "starting point and baselines we compare to in our work.\n",
      "\n",
      "3 RELATED WORK\n",
      "\n",
      "Low-precision networks: Low-precision DNNs are an active area of research. Most low-precision\n",
      "networks acknowledge the over parameterization aspect of today’s DNN architectures and/or the\n",
      "aspect that lowering the precision of neurons post-training often does not impact the ﬁnal perfor-\n",
      "mance. Reducing precision of weights for efﬁcient inference pipeline has been very well studied.\n",
      "Works like Binary connect (BC) (Courbariaux et al., 2015), Ternary-weight networks (TWN) (Li &\n",
      "Liu, 2016), ﬁne-grained ternary quantization (Mellempudi et al., 2017) and INQ (Zhou et al., 2017)\n",
      "target precision reduction of network weights. Accuracy is almost always affected when quantizing\n",
      "the weights signiﬁcantly below 8-bits of precision. For AlexNet on ImageNet, TWN loses 5% Top-1\n",
      "accuracy. Schemes like INQ, work in Sung et al. (2015) and Mellempudi et al. (2017) do ﬁne-tuning\n",
      "to quantize the network weights.\n",
      "\n",
      "Work in XNOR-NET (Rastegari et al., 2016), binary neural networks (Courbariaux & Bengio, 2016),\n",
      "DoReFa (Zhou et al., 2016) and trained ternary quantization (TTQ) (Zhu et al., 2016) target training\n",
      "pipeline. While TTQ targets weight quantization, most works targeting activation quantization show\n",
      "that quantizing activations always hurt accuracy. XNOR-NET approach degrades Top-1 accuracy\n",
      "by 12% and DoReFa by 8% when quantizing both weights and activations to 1-bit (for AlexNet\n",
      "on ImageNet). Work by Gupta et al. (2015) advocates for low-precision ﬁxed-point numbers for\n",
      "training. They show 16-bits to be sufﬁcient for training on CIFAR10 dataset. Work by Seide et al.\n",
      "(2014) quantizes gradients in a distributed computing system.\n",
      "\n",
      "Knowledge distillation methods: The general technique in distillation based methods involves us-\n",
      "ing a teacher-student strategy, where a large deep network trained for a given task teaches shallower\n",
      "student network(s) on the same task. The core concepts behind knowledge distillation or transfer\n",
      "technique have been around for a while. Buciluˇa et al. (2006) show that one can compress the\n",
      "information in an ensemble into a single network. Ba & Caurana (2013) extend this approach to\n",
      "study shallow, but wide, fully connected topologies by mimicking deep neural networks. To facil-\n",
      "\n",
      "3\n",
      "\n",
      "97.7%91.3%79.6%32.8%94.1%66.5%96.5%77.6%2.3%8.7%20.4%67.2%5.9%33.5%3.5%22.4%18181818AlexnetIRv2ResNet-50ResNet-101% Memory footprint% Ws% ACTs\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "itate learning, the authors introduce the concepts of learning on logits rather than the probability\n",
      "distribution.\n",
      "\n",
      "Hinton et al. (2015) propose a framework to transfer knowledge by introducing the concept of tem-\n",
      "perature. The key idea is to divide the logits by a temperature factor before performing a Softmax\n",
      "function. By using a higher temperature factor the activations of incorrect classes are boosted. This\n",
      "then facilitates more information ﬂowing to the model parameters during back-propagation opera-\n",
      "tion. FitNets (Romero et al., 2014) extend this work by using intermediate hidden layer outputs as\n",
      "target values for training a deeper, but thinner, student model. Net2Net (Chen et al., 2015a) also\n",
      "uses a teacher-student network system with a function-preserving transformation approach to ini-\n",
      "tialize the parameters of the student network. The goal in Net2Net approach is to accelerate the\n",
      "training of a larger student network. Zagoruyko & Komodakis (2016) use attention as a mechanism\n",
      "for transferring knowledge from one network to another. In a similar theme, Yim et al. (2017) pro-\n",
      "pose an information metric using which a teacher DNN can transfer the distilled knowledge to other\n",
      "student DNNs. In N2N learning work, Ashok et al. (2017) propose a reinforcement learning based\n",
      "approach for compressing a teacher network into an equally capable student network. They achieve\n",
      "a compression factor of 10x for ResNet-34 on CIFAR datasets.\n",
      "\n",
      "Sparsity and hashing: Few other popular techniques for model compression are pruning (LeCun\n",
      "et al., 1990; Han et al., 2015a; Wen et al., 2016; Han et al., 2015b), hashing (Weinberger et al., 2009)\n",
      "and weight sharing (Chen et al., 2015b; Denil et al., 2013). Pruning leads to removing neurons\n",
      "entirely from the ﬁnal trained model making the model a sparse structure. With hashing and weight\n",
      "sharing schemes a hash function is used to alias several weight parameters into few hash buckets,\n",
      "effectively lowering the parameter memory footprint. To realize beneﬁts of sparsity and hashing\n",
      "schemes during runtime, efﬁcient hardware support is required (e.g. support for irregular memory\n",
      "accesses (Han et al., 2016; Venkatesh et al., 2016; Parashar et al., 2017)).\n",
      "\n",
      "4 KNOWLEDGE DISTILLATION\n",
      "\n",
      "We introduce the concept of knowledge distillation in this section. Buciluˇa et al. (2006), Hinton\n",
      "et al. (2015) and Urban et al. (2016) analyze this topic in great detail.\n",
      "\n",
      "Figure 2 shows the schematic of the knowledge distillation setup. Given an input image x, a teacher\n",
      "DNN maps this image to predictions pT . The C class predictions are obtained by applying Softmax\n",
      "function on the un-normalized log probability values z (the logits), i.e. pT = ezT\n",
      "j . The\n",
      "same image is fed to the student network and it predicts pA = ezA\n",
      "cost function, L, is given as:\n",
      "\n",
      "j . During training, the\n",
      "\n",
      "k /(cid:80)C\n",
      "\n",
      "k /(cid:80)C\n",
      "\n",
      "j ezA\n",
      "\n",
      "j ezT\n",
      "\n",
      "L(x; WT , WA) = αH(y, pT ) + βH(y, pA) + γH(zT , pA)\n",
      "\n",
      "(1)\n",
      "\n",
      "where, WT and WA are the parameters of the teacher and the student (apprentice) network, respec-\n",
      "tively, y is the ground truth, H(·) denotes a loss function and, α, β and γ are weighting factors to\n",
      "prioritize the output of a certain loss function over the other.\n",
      "\n",
      "In equation 1, lowering the ﬁrst term of the cost function gives a better teacher network and lowering\n",
      "the second term gives a better student network. The third term is the knowledge distillation term\n",
      "whereby the student network attempts to mimic the knowledge in the teacher network. In Hinton\n",
      "et al. (2015), the logits of the teacher network are divided by a temperature factor τ . Using a higher\n",
      "value for τ produces a softer probability distribution when taking the Softmax of the logits. In our\n",
      "studies, we use cross-entropy function for H(·), set α = 1, β = 0.5 and γ = 0.5 and, perform the\n",
      "transfer learning process using the logits (inputs to the Softmax function) of the teacher network. In\n",
      "our experiments we study the effect of varying the depth of the teacher and the student network, and\n",
      "the precision of the neurons in the student network.\n",
      "\n",
      "5 OUR APPROACH - APPRENTICE NETWORK\n",
      "\n",
      "Low-precision DNNs target the storage and compute efﬁciency aspects of the network. Model com-\n",
      "pression targets the same efﬁciency parameters from the point of view of network architecture. With\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "Figure 2: Schematic of the knowledge distillation setup. The teacher network is a high precision network and\n",
      "the apprentice network is a low-precision network.\n",
      "\n",
      "Apprentice we combine both these techniques to improve the network accuracy as well as the\n",
      "runtime efﬁciency of DNNs. Using the teacher-student setup described in the last section, we inves-\n",
      "tigate three schemes using which one can obtain a low-precision model for the student network. The\n",
      "ﬁrst scheme (scheme-A) jointly trains both the networks - full-precision teacher and low-precision\n",
      "student network. The second scheme (scheme-B) trains only the low-precision student network but\n",
      "distills knowledge from a trained full-precision teacher network throughout the training process.\n",
      "The third scheme (scheme-C) starts with a trained full-precision teacher and a full-precision student\n",
      "network but ﬁne-tunes the student network after lowering its precision. Before we get into the details\n",
      "of each of these schemes, we discuss the accuracy numbers obtained using low-precision schemes\n",
      "described in literature. These accuracy ﬁgures serve as the baseline for comparative analysis.\n",
      "\n",
      "5.1 TOP-1 ERROR WITH PRIOR PROPOSALS FOR LOW-PRECISION NETWORKS\n",
      "\n",
      "We focus on sub 8-bits precision for inference deployments, speciﬁcally ternary and 4-bits precision.\n",
      "We found TTQ (Zhu et al., 2016) scheme achieving the state-of-the-art accuracy with ternary pre-\n",
      "cision for weights and full-precision (32-bits ﬂoating-point) for activations. On Imagenet-1K (Rus-\n",
      "sakovsky et al., 2015), TTQ achieves 33.4% Top-1 error rate with a ResNet-18 model. We imple-\n",
      "mented TTQ scheme for ResNet-34 and ResNet-50 models trained on Imagenet-1K and achieved\n",
      "28.3% and 25.6% Top-1 error rates, respectively. This scheme is our baseline for 2-bits weight and\n",
      "full-precision activations. For 2-bits weight and 8-bits activation, we ﬁnd work by Mellempudi et al.\n",
      "(2017) to achieve the best accuracies reported in literature. For ResNet-50, Mellempudi et al. (2017)\n",
      "obtain 29.24% Top-1 error. We consider this work to be our baseline for 2-bits weight and 8-bits\n",
      "activation models.\n",
      "\n",
      "For 4-bits precision, we ﬁnd WRPN scheme (Mishra et al., 2017) to report the highest accuracy. We\n",
      "implemented this scheme for 4-bits weight and 8-bits activations. For ResNet-34 and ResNet-50\n",
      "models trained on Imagenet-1K, we achieve 29.7% and 28.4% Top-1 error rates, respectively.\n",
      "\n",
      "5.2 SCHEME-A: JOINT TRAINING OF TEACHER-STUDENT NETWORKS\n",
      "\n",
      "In the ﬁrst scheme that we investigate, a full-precision teacher network is jointly trained with a low-\n",
      "precision student network. Figure 2 shows the overall training framework. We use ResNet topology\n",
      "for both the teacher and student network. When using a certain depth for the student network, we\n",
      "pick the teacher network to have either the same or larger depth.\n",
      "\n",
      "In Buciluˇa et al. (2006) and Hinton et al. (2015), only the student network trains while distilling\n",
      "knowledge from the teacher network. In our case, we jointly train with the rationale that the teacher\n",
      "network would continuously guide the student network not only with the ﬁnal trained logits, but also\n",
      "on what path the teacher takes towards generating those ﬁnal higher accuracy logits.\n",
      "\n",
      "5\n",
      "\n",
      "Input imagexsoftmaxsoftmaxTeacher networkApprentice networkzTpTzApAHard label!Knowledge distillationWTWAFilter bankFilter bank\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "We implement pre-activation version of ResNet (He et al., 2016) in TensorFlow (Abadi et al., 2015).\n",
      "The training process closely follows the recipe mentioned in Torch implementation of ResNet - we\n",
      "use a batch size of 256 and no hyper-parameters are changed from what is mentioned in the recipe.\n",
      "For the teacher network, we experiment with ResNet-34, ResNet-50 and ResNet-101 as options.\n",
      "For the student network, we experiment with low-precision variants of ResNet-18, ResNet-34 and\n",
      "ResNet-50.\n",
      "\n",
      "For low-precision numerics, when using ternary precision we use the ternary weight network\n",
      "scheme (Li & Liu, 2016) where the weight tensors are quantized into {−1, 0, 1} with a per-layer\n",
      "scaling coefﬁcient computed based on the mean of the positive terms in the weight tensor. We use\n",
      "the WRPN scheme (Mishra et al., 2017) to quantize weights and activations to 4-bits or 8-bits. We\n",
      "do not lower the precision of the ﬁrst layer and the ﬁnal layer in the apprentice network. This is based\n",
      "on the observation in almost all prior works that lowering the precision of these layers degrades the\n",
      "accuracy dramatically. While training and during ﬁne-tuning, the gradients are still maintained at\n",
      "full-precision.\n",
      "\n",
      "Table 1: Top-1 validation set error rate (%) on ImageNet-1K for ResNet-18 stu-\n",
      "dent network as precision of activations (A) and weight (W) changes. The last\n",
      "three columns show error rate when the student ResNet-18 is paired with ResNet-34,\n",
      "ResNet-50 and ResNet-101.\n",
      "\n",
      "ResNet-18\n",
      "Baseline\n",
      "\n",
      "ResNet-18\n",
      "\n",
      "ResNet-18\n",
      "\n",
      "ResNet-18\n",
      "\n",
      "with ResNet-34 with ResNet-50 with ResNet-101\n",
      "\n",
      "32A, 32W\n",
      "32A, 2W\n",
      "8A, 4W\n",
      "8A, 2W\n",
      "\n",
      "30.4\n",
      "33.4\n",
      "33.6\n",
      "33.9\n",
      "\n",
      "30.2\n",
      "31.7\n",
      "29.6\n",
      "32.0\n",
      "\n",
      "30.1\n",
      "31.5\n",
      "29.6\n",
      "32.2\n",
      "\n",
      "30.1\n",
      "31.8\n",
      "29.9\n",
      "32.4\n",
      "\n",
      "Results with ResNet-18: Table 1 shows the ef-\n",
      "fect of lowering precision on the accuracy (Top-\n",
      "1 error) of ResNet-18 with baseline (no teacher)\n",
      "and with ResNet-34, ResNet-50 and ResNet-101\n",
      "as teachers. In the table, A denotes the precision\n",
      "of the activation maps (in bits) and W denotes the\n",
      "precision of the weights. The baseline Top-1 error\n",
      "for full-precision ResNet-18 is 30.4%. By low-\n",
      "ering the precision without using any help from\n",
      "a teacher network, the accuracy drops by 3.5%\n",
      "when using ternary and 4-bits precision (the col-\n",
      "umn corresponding to “Res-18 Baseline” in the\n",
      "table). With distillation based technique, the ac-\n",
      "curacy of low-precision conﬁgurations improves\n",
      "signiﬁcantly.\n",
      "In fact, the accuracy of the full-\n",
      "precision ResNet-18 also improves when paired\n",
      "with a larger full-precision ResNet model (the row\n",
      "corresponding to “32A, 32W” in Table 1). The\n",
      "best full-precision accuracy was achieved with a\n",
      "student ResNet-18 and ResNet-101 as the teacher\n",
      "(improvement by 0.35% over the baseline). The\n",
      "gap between full-precision ResNet-18 and the best\n",
      "achieved ternary weight ResNet-18 is only 1% (improvement of 2% over previous best). With “8A,\n",
      "4W”, we ﬁnd the accuracy of the student ResNet-18 model to beat the baseline accuracy. We hy-\n",
      "pothesize regularization with low-precision (and distillation) to be the reason for this. “8A, 4W”\n",
      "improving the accuracy beyond baseline ﬁgure is only seen for ResNet-18.\n",
      "\n",
      "Figure 3: Difference in Top-1 error rate for low-\n",
      "precision variants of ResNet-18 with (blue bars) and\n",
      "without (red bars) distillation scheme. The differ-\n",
      "ence is calculated from the accuracy of ResNet-18\n",
      "with full-precision numerics. Higher % difference\n",
      "denotes a better network conﬁguration.\n",
      "\n",
      "Figure 3 shows the difference in Top-1 error rate achieved by our best low-precision student networks\n",
      "(when trained under the guidance of a teacher network) versus not using any help from a teacher\n",
      "network. For this ﬁgure, the difference in Top-1 error of the best low-precision student network is\n",
      "calculated from the baseline full-precision network (i.e. ResNet-18 with 30.4% Top-1 error), i.e. we\n",
      "want to see how close a low-precision student network can come to a full-precision baseline model.\n",
      "\n",
      "6\n",
      "\n",
      "0.3% -1.0% 0.8% -1.5% 0.0% -3.0% -3.2% -3.5% -6% -4% -2% 0% 2% 32A,\t32W32A,\t2W8A,\t4W8A,\t2WDifference\t(!)\tin\tTop-1\terror\tfor\tRes-18\tfrom\tbaseline!from\t32A,\t32W\twithout\tApprentice!from\t32A,\t32W\twith\tApprentice\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "We ﬁnd our low-precision network accuracies to signiﬁcantly close the gap between full-precision\n",
      "accuracy (and for some conﬁgurations even beat the baseline accuracy).\n",
      "\n",
      "Hinton et al. (2015) mention improving the baseline full-precision accuracy when a student network\n",
      "is paired with a teacher network. They mention improving the accuracy of a small model on MNIST\n",
      "dataset. We show the efﬁcacy of distillation based techniques on a much bigger model (ResNet)\n",
      "with much larger dataset (ImageNet).\n",
      "\n",
      "Table 2: Top-1 validation set error rate (%) on ImageNet-1K for ResNet-34 stu-\n",
      "dent network as precision of activations (A) and weight (W) changes. The last\n",
      "three columns show error rate when the student ResNet-34 is paired with ResNet-34,\n",
      "ResNet-50 and ResNet-101.\n",
      "\n",
      "ResNet-34\n",
      "Baseline\n",
      "\n",
      "ResNet-34\n",
      "\n",
      "ResNet-34\n",
      "\n",
      "ResNet-34\n",
      "\n",
      "with ResNet-34 with ResNet-50 with ResNet-101\n",
      "\n",
      "32A, 32W\n",
      "32A, 2W\n",
      "8A, 4W\n",
      "8A, 2W\n",
      "\n",
      "26.4\n",
      "28.3\n",
      "29.7\n",
      "30.8\n",
      "\n",
      "26.3\n",
      "27.6\n",
      "27.0\n",
      "28.8\n",
      "\n",
      "26.1\n",
      "27.2\n",
      "26.9\n",
      "28.8\n",
      "\n",
      "26.1\n",
      "27.2\n",
      "26.9\n",
      "28.5\n",
      "\n",
      "Table 3: Top-1 validation set error rate (%) on ImageNet-1K for\n",
      "ResNet-50 student network as precision of activations (A) and\n",
      "weight (W) changes. The ﬁnal two columns show error rate when\n",
      "the student ResNet-50 is paired with ResNet-50 and ResNet-101.\n",
      "\n",
      "ResNet-50\n",
      "Baseline\n",
      "\n",
      "ResNet-50\n",
      "\n",
      "ResNet-50\n",
      "\n",
      "with ResNet-50 with ResNet-101\n",
      "\n",
      "32A, 32W\n",
      "32A, 2W\n",
      "8A, 4W\n",
      "8A, 2W\n",
      "\n",
      "23.8\n",
      "26.1\n",
      "28.5\n",
      "29.2\n",
      "\n",
      "23.7\n",
      "25.4\n",
      "25.5\n",
      "27.3\n",
      "\n",
      "23.5\n",
      "25.3\n",
      "25.3\n",
      "27.2\n",
      "\n",
      "(a) Apprentice versus baseline accuracy for ResNet-34. (b) Apprentice versus baseline accuracy for ResNet-50.\n",
      "\n",
      "Figure 4: Difference in Top-1 error rate for low-precision variants of ResNet-34 and ResNet-50 with (blue\n",
      "bars) and without (red bars) distillation scheme. The difference is calculated from the accuracy of the baseline\n",
      "network (ResNet-34 for (a) and ResNet-50 for (b)) operating at full-precision. Higher % difference denotes a\n",
      "better network conﬁguration.\n",
      "\n",
      "Results with ResNet-34 and ResNet-50: Table 2 and Table 3 show the effect of lowering precision\n",
      "on the accuracy of ResNet-34 and ResNet-50, respectively, with distillation based technique. With\n",
      "a student ResNet-34 network, we use ResNet-34, ResNet-50 and ResNet-101 as teachers. With\n",
      "a student ResNet-50 network, we use ResNet-50 and ResNet-101 as teachers. The Top-1 error\n",
      "for full-precision ResNet-34 is 26.4%. Our best 4-bits weight and 8-bits activation ResNet-34 is\n",
      "within 0.5% of this number (26.9% error rate with ResNet-34 student and ResNet-50 teacher). This\n",
      "\n",
      "7\n",
      "\n",
      "0.3% -0.8% -0.5% -2.1% 0.0% -1.9% -3.3% -4.4% -6% -4% -2% 0% 2% 32A,\t32W32A,\t2W8A,\t4W8A,\t2WDifference\t(!)\tin\tTop-1\terror\tfor\tRes-34\tfrom\tbaseline!from\t32A,\t32W\twithout\tApprentice!from\t32A,\t32W\twith\tApprentice0.3% -1.5% -1.5% -3.4% 0.0% -2.3% -4.7% -5.5% -6% -4% -2% 0% 2% 32A,\t32W32A,\t2W8A,\t4W8A,\t2WDifference\t(!)\tin\tTop-1\terror\tfor\tRes-50\tfrom\tbaseline!from\t32A,\t32W\twithout\tApprentice!from\t32A,\t32W\twith\tApprentice\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "signiﬁcantly improves upon the previously reported error rate of 29.7%. 4-bits weight and 8-bits\n",
      "activation for ResNet-50 gives us a model that is within 1.5% of full-precision model accuracy\n",
      "(25.3% vs. 23.8%). Figure 4a and Figure 4b show the difference in Top-1 error achieved by our best\n",
      "low-precision ResNet-34 and ResNet-50 student networks, respectively, and compares with results\n",
      "obtained using methods proposed in literature. Our Apprentice scheme signiﬁcantly closes the gap\n",
      "between full-precision baseline networks and low-precision variants of the same networks. In most\n",
      "cases we see our scheme to better the previously reported accuracy numbers by 1.5%-3%.\n",
      "\n",
      "Discussion: In scheme-A, we use a teacher network that is always as large or larger in number of\n",
      "parameters than the student network. We experimented with a ternary ResNet-34 student network\n",
      "which was paired with a full-precision ResNet-18. The ternary model for ResNet-34 is about 8.5x\n",
      "smaller in size compared to the full-precision ResNet-18 model. The ﬁnal trained accuracy of the\n",
      "ResNet-34 ternary model with this setup is 2.7% worse than that obtained by pairing the ternary\n",
      "ResNet-34 network with a ResNet-50 teacher network. This suggests that the distillation scheme\n",
      "works only when the teacher network is higher in accuracy than the student network (and not neces-\n",
      "sarily bigger in capacity). Further, the beneﬁt from using a larger teacher network saturates at some\n",
      "point. This can be seen by picking up a precision point, say “32A, 2W” and looking at the error\n",
      "rates along the row in Table 1, 2 and 3.\n",
      "\n",
      "One concern, we had in the early stages of our investigation, with joint training of a low-precision\n",
      "small network and a high precision large network was the inﬂuence of the small network’s accuracy\n",
      "on the accuracy of the large network. When using the joint cost function, the smaller network’s\n",
      "probability scores are matched with the predictions from the teacher network. The joint cost is added\n",
      "as a term to the total loss function (equation 1). This led us to posit that the larger network’s learning\n",
      "capability will be affected by the inherent impairment in the smaller low-precision network. Further,\n",
      "since the smaller student network learns form the larger teacher network, a vicious cycle might\n",
      "form where the student network’s accuracy will further drop because the teacher network’s learning\n",
      "capability is being impeded. However, in practice, we did not see this phenomenon occurring - in\n",
      "each case where the teacher network was jointly trained with a student network, the accuracy of the\n",
      "teacher network was always within 0.1% to 0.2% of the accuracy of the teacher network without it\n",
      "jointly supervising a student network. This could be because of our choice of α, β and γ values.\n",
      "\n",
      "In Section 4, we mentioned about temperature, τ , for Softmax function and hyper-parameters α = 1,\n",
      "β = 0.5 and γ = 0.5. Since, we train directly on the logits of the teacher network, we did not have to\n",
      "experiment with the appropriate value of τ . τ is required when training on the soft targets produced\n",
      "by the teacher network. Although we did not do extensive studies experimenting with training on\n",
      "soft targets as opposed to logits, we ﬁnd that τ = 1 gives us best results when training on soft\n",
      "targets. Hinton et al. (2015) mention that when the student network is signiﬁcantly smaller than the\n",
      "teacher network, small values of τ are more effective than large values. For few of the low-precision\n",
      "conﬁgurations, we experimented with α = β = γ = 1, and, α = 0.9, β = 1 and γ = 0.1 or 0.3.\n",
      "Each of these conﬁgurations, yielded a lower performance model compared to our original choice\n",
      "for these parameters.\n",
      "\n",
      "For the third term in equation 1, we experimented with a mean-squared error loss function and also a\n",
      "loss function with logits from both the student and the teacher network (i.e. H(zT , zA)). We did not\n",
      "ﬁnd any improvement in accuracy compared to our original choice of the cost function formulation.\n",
      "A thorough investigation of the behavior of the networks with other values of hyper-parameters and\n",
      "different loss functions is an agenda for our future work.\n",
      "\n",
      "Overall, we ﬁnd the distillation process to be quite effective in getting us high accuracy low-precision\n",
      "models. All our low-precision models surpass previously reported low-precision accuracy ﬁgures.\n",
      "For example, TTQ scheme achieves 33.4% Top-1 error rate for ResNet-18 with 2-bits weight. Our\n",
      "best ResNet-18 model, using scheme-A, with 2-bits weight achieves ∼31.5% error rate, improving\n",
      "the model accuracy by ∼2% over TTQ. Similarly, the scheme in Mellempudi et al. (2017) achieves\n",
      "29.2% Top-1 error with 2-bits weight and 8-bits activation. The best performing Apprentice net-\n",
      "work at this precision achieves 27.2% Top-1 error. For Scheme-B and Scheme-C, which we describe\n",
      "next, Scheme-A serves as the new baseline.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "5.3 SCHEME-B: DISTILLING KNOWLEDGE FROM A TEACHER\n",
      "\n",
      "In this scheme, we start with a trained teacher network. Referring back to Figure 2, the input image\n",
      "is passed to both the teacher and the student network, except that the learning with back-propagation\n",
      "happens only in the low precision student network which is trained from scratch. This is the scheme\n",
      "used by Buciluˇa et al. (2006) and Hinton et al. (2015) for training their student networks. In this\n",
      "scheme, the ﬁrst term in equation 1 zeroes out and only the last two terms in the equation contribute\n",
      "toward the loss function.\n",
      "\n",
      "Figure 5: Top-1 error rate versus epochs of four student networks using scheme-A and scheme-B.\n",
      "\n",
      "With scheme-B, one can pre-compute and store the logit values for the input images on disk and\n",
      "access them during training the student network. This saves the forward pass computations in the\n",
      "teacher network. Scheme-B might also help the scenario where a student network attempts to learn\n",
      "the “dark knowledge” from a teacher network that has already been trained on some private or\n",
      "sensitive data (in addition to the data the student network is interested in training on).\n",
      "\n",
      "With scheme-A, we had the hypothesis that the student network would be inﬂuenced by not only the\n",
      "“dark knowledge” in the teacher network but also the path the teacher adopts to learn the knowledge.\n",
      "With scheme-B we ﬁnd, that the student network gets to similar accuracy numbers as the teacher\n",
      "network albeit at fewer number of epochs.\n",
      "\n",
      "With this scheme, the training accuracies are similar to that reported in Table 1, 2 and 3. The\n",
      "low-precision student networks, however, learn in fewer number of epochs. Figure 5 plots the Top-1\n",
      "error rates for few of the conﬁgurations from our experiment suite. In each of these plots, the student\n",
      "network in scheme-B converges around 80th-85th epoch compared to about 105 epochs in scheme-\n",
      "A. In general, we ﬁnd the student networks with scheme-B to learn in about 10%-20% fewer epochs\n",
      "than the student networks trained using scheme-A.\n",
      "\n",
      "5.4 SCHEME-C: FINE-TUNING THE STUDENT MODEL\n",
      "\n",
      "Scheme-C is very similar to scheme-B, except that the student network is primed with full precision\n",
      "training weights before the start of the training process. At the beginning of the training process,\n",
      "the weights and activations are lowered and the student network is sort of ﬁne-tuned on the dataset.\n",
      "\n",
      "9\n",
      "\n",
      "00.10.20.30.40.50.60.70.80.9105101520253035404550556065707580859095100105Top-1\terrorEpochsScheme-AScheme-BResNet-34student\twith\tResNet-50\tteacher,\t2W\t32A00.10.20.30.40.50.60.70.80.9105101520253035404550556065707580859095100105Top-1\terrorEpochsScheme-AScheme-BResNet-34student\twith\tResNet-50\tteacher,\t4W\t8A00.10.20.30.40.50.60.70.80.9105101520253035404550556065707580859095100105Top-1\terrorEpochsScheme-AScheme-BResNet-50student\twith\tResNet-101\tteacher,\t2W\t32A00.10.20.30.40.50.60.70.80.9105101520253035404550556065707580859095100105Top-1\terrorEpochsScheme-AScheme-BResNet-50student\twith\tResNet-101\tteacher,\t4W\t8A\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "Similar to scheme-B, only the ﬁnal two terms in equation 1 comprise the loss function and the low-\n",
      "precision student network is trained with back-propagation algorithm. Since, the network starts from\n",
      "a good initial point, comparatively low learning rate is used throughout the training process. There is\n",
      "no clear recipe for learning rates (and change of learning rate with epochs) which works across all the\n",
      "conﬁgurations. In general, we ﬁnd training with a learning rate of 1e-3 for 10 to 15 epochs, followed\n",
      "by 1e-4 for another 5 to 10 epochs, followed by 1e-5 for another 5 epochs to give us the best accuracy.\n",
      "Some conﬁgurations run for about 40 to 50 epochs before stabilizing. For these conﬁgurations, we\n",
      "ﬁnd training using scheme-B with warm startup (train the student network at full-precision for about\n",
      "25-30 epochs before lowering the precision) to be equally good. Wu (2016) investigate a similar\n",
      "scheme for binary precision on AlexNet. Our experiments show that distillation is an overkill for\n",
      "AlexNet and one can get comparable accuracies using techniques proposed in (Tang et al., 2017;\n",
      "Mishra et al., 2017). Further, Wu (2016) hypothesize that distillation scheme will work on larger\n",
      "networks, we show in this paper how to make it work. Tann et al. (2017) use a similar scheme for\n",
      "AlexNet and mention starting from a non-global optimal checkpoint gives better accuracy, though\n",
      "we did not ﬁnd this observation to hold in our experiments.\n",
      "\n",
      "We ﬁnd the ﬁnal accuracy of the models obtained using scheme-C to be (marginally) better than\n",
      "those obtained using scheme-A or scheme-B. Table 4 shows error rates of few conﬁgurations of\n",
      "low-precision student network obtained using scheme-A (or scheme-B) and scheme-C. For ResNet-\n",
      "50 student network, the accuracy with ternary weights is further improved by 0.6% compared to that\n",
      "obtained using scheme-A. Note that the performance of ternary networks obtained using scheme-\n",
      "A are already state-of-the-art. Hence, for ResNet-50 ternary networks, 24.7% Top-1 error rate is\n",
      "the new state-of-the-art. With this, ternary ResNet-50 is within 0.9% of baseline accuracy (23.8%\n",
      "vs. 24.7%). Similarly, with 4-bits weight and 8-bits activations, ResNet-50 model obtained using\n",
      "scheme-C is 0.4% better than that obtained with scheme-A (closing the gap to be within 1.3% of\n",
      "full-precision ResNet-50 accuracy).\n",
      "\n",
      "Table 4: Top-1 ImageNet-1K validation set error rate (%) with scheme-A and scheme-C for\n",
      "ResNet-34 and ResNet-50 student networks with ternary and 4-bits precision.\n",
      "\n",
      "ResNet-34 student with ResNet-50 teacher\n",
      "ResNet-50 student with ResNet101 teacher\n",
      "\n",
      "ResNet-34 student with ResNet-50 teacher\n",
      "ResNet-50 student with ResNet101 teacher\n",
      "\n",
      "32A, 2W\n",
      "\n",
      "32A, 2W\n",
      "\n",
      "with scheme-A or B with scheme-C\n",
      "\n",
      "27.2\n",
      "25.3\n",
      "\n",
      "8A, 4W\n",
      "\n",
      "26.9\n",
      "25.5\n",
      "\n",
      "26.9\n",
      "24.7\n",
      "\n",
      "8A, 4W\n",
      "\n",
      "26.8\n",
      "25.1\n",
      "\n",
      "with scheme-A or B with scheme-C\n",
      "\n",
      "Scheme-C is useful when one already has a trained network which can be ﬁne-tuned using knowl-\n",
      "edge distillation schemes to produce a low-precision variant of the trained network.\n",
      "\n",
      "5.5 DISCUSSION - TERNARY PRECISION VERSUS SPARSITY\n",
      "\n",
      "As mentioned earlier, low-precision is a form of model compression. There are many works which\n",
      "target network sparsiﬁcation and pruning techniques to compress a model. With ternary preci-\n",
      "sion models, the model size reduces by a factor of 2/32 compared to full-precision models. With\n",
      "Apprentice, we show how one can get a performant model with ternary precision. Many works tar-\n",
      "geting network pruning and sparsiﬁcation target a full-precision model to implement their scheme.\n",
      "To be comparable in model size to ternary networks, a full-precision model needs to be sparsiﬁed\n",
      "by 93.75%. Further, to be effective, a sparse model needs to store a key for every non-zero value de-\n",
      "noting the position of the value in the weight tensor. This adds storage overhead and a sparse model\n",
      "needs to be about 95% sparse to be at-par in memory size as a 2-bit model. Note that ternary preci-\n",
      "sion also has inherent sparsity (zero is a term in the ternary symbol dictionary) – we ﬁnd our ternary\n",
      "models to be about 50% sparse. In work by Wen et al. (2016) and Han et al. (2015b), sparsiﬁcation\n",
      "of full-precision networks is proposed but the sparsity achieved is less than 93.75%. Further, the\n",
      "network accuracy using techniques in both these works lead to larger degradation in accuracy com-\n",
      "pared to our ternary models. Overall, we believe, our ternary precision models to be state-of-the-art\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "not only in accuracy (we better the accuracy compared to prior ternary precision models) but also\n",
      "when one considers the size of the model at the accuracy level achieved by low-precision or sparse\n",
      "networks.\n",
      "\n",
      "6 CONCLUSIONS\n",
      "\n",
      "While low-precision networks have system-level beneﬁts, the drawback of such models is degraded\n",
      "accuracy when compared to full-precision models. We present three schemes based on knowledge\n",
      "distillation concept to improve the accuracy of low-precision networks and close the gap between\n",
      "the accuracy of these models and full-precision models. Each of the three schemes improve the\n",
      "accuracy of the low-precision network conﬁguration compared to prior proposals. We motivate the\n",
      "need for a smaller model size in low batch, real-time and resource constrained inference deployment\n",
      "systems. We envision the low-precision models produced by our schemes to simplify the inference\n",
      "deployment process on resource constrained systems and on cloud-based deployment systems where\n",
      "low latency is a critical requirement.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\n",
      "Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\n",
      "Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\n",
      "Kudlur, Josh Levenberg, Dan Man´e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\n",
      "Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-\n",
      "cent Vanhoucke, Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten-\n",
      "berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\n",
      "on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software avail-\n",
      "able from tensorﬂow.org.\n",
      "\n",
      "A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani. N2N Learning: Network to Network Com-\n",
      "\n",
      "pression via Policy Gradient Reinforcement Learning. ArXiv e-prints, September 2017.\n",
      "\n",
      "Lei Jimmy Ba and Rich Caurana. Do deep nets really need to be deep? CoRR, abs/1312.6184, 2013.\n",
      "\n",
      "URL http://arxiv.org/abs/1312.6184.\n",
      "\n",
      "Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceed-\n",
      "ings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Min-\n",
      "ing, KDD ’06, pp. 535–541, New York, NY, USA, 2006. ACM. ISBN 1-59593-339-5. doi: 10.\n",
      "1145/1150402.1150464. URL http://doi.acm.org/10.1145/1150402.1150464.\n",
      "\n",
      "Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network\n",
      "models for practical applications. CoRR, abs/1605.07678, 2016. URL http://arxiv.org/\n",
      "abs/1605.07678.\n",
      "\n",
      "Tianqi Chen, Ian J. Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge\n",
      "\n",
      "transfer. CoRR, abs/1511.05641, 2015a. URL http://arxiv.org/abs/1511.05641.\n",
      "\n",
      "Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing\n",
      "neural networks with the hashing trick. CoRR, abs/1504.04788, 2015b. URL http://arxiv.\n",
      "org/abs/1504.04788.\n",
      "\n",
      "Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights\n",
      "and activations constrained to +1 or -1. CoRR, abs/1602.02830, 2016. URL http://arxiv.\n",
      "org/abs/1602.02830.\n",
      "\n",
      "Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural\n",
      "networks with binary weights during propagations. CoRR, abs/1511.00363, 2015. URL http:\n",
      "//arxiv.org/abs/1511.00363.\n",
      "\n",
      "Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting\n",
      "parameters in deep learning. CoRR, abs/1306.0543, 2013. URL http://arxiv.org/abs/\n",
      "1306.0543.\n",
      "\n",
      "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with\n",
      "limited numerical precision. CoRR, abs/1502.02551, 2015. URL http://arxiv.org/abs/\n",
      "1502.02551.\n",
      "\n",
      "Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network\n",
      "with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015a. URL\n",
      "http://arxiv.org/abs/1510.00149.\n",
      "\n",
      "Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for\n",
      "In Advances in Neural Information Processing Systems 28: Annual\n",
      "efﬁcient neural network.\n",
      "Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,\n",
      "Quebec, Canada, pp. 1135–1143, 2015b.\n",
      "\n",
      "Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J.\n",
      "Dally. EIE: efﬁcient inference engine on compressed deep neural network. In 43rd ACM/IEEE\n",
      "Annual International Symposium on Computer Architecture, ISCA 2016, Seoul, South Korea, June\n",
      "18-22, 2016, pp. 243–254, 2016. doi: 10.1109/ISCA.2016.30. URL https://doi.org/10.\n",
      "1109/ISCA.2016.30.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n",
      "\n",
      "nition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "\n",
      "Identity mappings in deep residual\n",
      "\n",
      "networks. CoRR, abs/1603.05027, 2016. URL http://arxiv.org/abs/1603.05027.\n",
      "\n",
      "G. Hinton, O. Vinyals, and J. Dean. Distilling the Knowledge in a Neural Network. ArXiv e-prints,\n",
      "\n",
      "March 2015.\n",
      "\n",
      "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n",
      "\n",
      "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-\n",
      "lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),\n",
      "Advances in Neural Information Processing Systems 25, pp. 1097–1105. Curran Associates, Inc.,\n",
      "2012.\n",
      "\n",
      "Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In D. S. Touretzky (ed.),\n",
      "Advances in Neural Information Processing Systems 2, pp. 598–605. Morgan-Kaufmann, 1990.\n",
      "URL http://papers.nips.cc/paper/250-optimal-brain-damage.pdf.\n",
      "\n",
      "Fengfu Li and Bin Liu. Ternary weight networks. CoRR, abs/1605.04711, 2016. URL http:\n",
      "\n",
      "//arxiv.org/abs/1605.04711.\n",
      "\n",
      "Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural networks with\n",
      "few multiplications. CoRR, abs/1510.03009, 2015. URL http://arxiv.org/abs/1510.\n",
      "03009.\n",
      "\n",
      "N. Mellempudi, A. Kundu, D. Mudigere, D. Das, B. Kaul, and P. Dubey. Ternary Neural Networks\n",
      "\n",
      "with Fine-Grained Quantization. ArXiv e-prints, May 2017.\n",
      "\n",
      "A. Mishra, E. Nurvitadhi, J. J Cook, and D. Marr. WRPN: Wide Reduced-Precision Networks.\n",
      "\n",
      "ArXiv e-prints, September 2017.\n",
      "\n",
      "Daisuke Miyashita, Edward H. Lee, and Boris Murmann. Convolutional neural networks using\n",
      "logarithmic data representation. CoRR, abs/1603.01025, 2016. URL http://arxiv.org/\n",
      "abs/1603.01025.\n",
      "\n",
      "Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan Venkatesan,\n",
      "Brucek Khailany, Joel S. Emer, Stephen W. Keckler, and William J. Dally. SCNN: an acceler-\n",
      "ator for compressed-sparse convolutional neural networks. CoRR, abs/1708.04485, 2017. URL\n",
      "http://arxiv.org/abs/1708.04485.\n",
      "\n",
      "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet\n",
      "classiﬁcation using binary convolutional neural networks. CoRR, abs/1603.05279, 2016. URL\n",
      "http://arxiv.org/abs/1603.05279.\n",
      "\n",
      "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and\n",
      "Yoshua Bengio. Fitnets: Hints for thin deep nets. CoRR, abs/1412.6550, 2014. URL http:\n",
      "//arxiv.org/abs/1412.6550.\n",
      "\n",
      "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\n",
      "Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\n",
      "ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision\n",
      "(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.\n",
      "\n",
      "Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and\n",
      "application to data-parallel distributed training of speech dnns. In Interspeech 2014, September\n",
      "2014.\n",
      "\n",
      "Wonyong Sung, Sungho Shin, and Kyuyeon Hwang. Resiliency of deep neural networks under\n",
      "quantization. CoRR, abs/1511.06488, 2015. URL http://arxiv.org/abs/1511.06488.\n",
      "\n",
      "Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke.\n",
      "\n",
      "Inception-v4, inception-resnet and\n",
      "the impact of residual connections on learning. CoRR, abs/1602.07261, 2016. URL http:\n",
      "//arxiv.org/abs/1602.07261.\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "Wei Tang, Gang Hua, and Liang Wang. How to train a compact binary neural network with high\n",
      "accuracy? In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, February\n",
      "4-9, 2017, San Francisco, California, USA., 2017.\n",
      "\n",
      "Hokchhay Tann, Soheil Hashemi, Iris Bahar, and Sherief Reda. Hardware-software codesign of\n",
      "\n",
      "accurate, multiplier-free deep neural networks. CoRR, abs/1705.04288, 2017.\n",
      "\n",
      "Torch implementation of ResNet. https://github.com/facebook/fb.resnet.torch.\n",
      "\n",
      "Yaman Umuroglu, Nicholas J. Fraser, Giulio Gambardella, Michaela Blott, Philip Heng Wai Leong,\n",
      "Magnus Jahre, and Kees A. Vissers. FINN: A framework for fast, scalable binarized neural\n",
      "network inference. CoRR, abs/1612.07119, 2016. URL http://arxiv.org/abs/1612.\n",
      "07119.\n",
      "\n",
      "G. Urban, K. J. Geras, S. Ebrahimi Kahou, O. Aslan, S. Wang, R. Caruana, A. Mohamed, M. Phili-\n",
      "pose, and M. Richardson. Do Deep Convolutional Nets Really Need to be Deep and Convolu-\n",
      "tional? ArXiv e-prints, March 2016.\n",
      "\n",
      "Vincent Vanhoucke, Andrew Senior, and Mark Z. Mao. Improving the speed of neural networks on\n",
      "\n",
      "cpus. In Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.\n",
      "\n",
      "Ganesh Venkatesh, Eriko Nurvitadhi, and Debbie Marr. Accelerating deep convolutional networks\n",
      "using low-precision and sparsity. CoRR, abs/1610.00324, 2016. URL http://arxiv.org/\n",
      "abs/1610.00324.\n",
      "\n",
      "Kilian Q. Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, and Alexander J. Smola.\n",
      "Feature hashing for large scale multitask learning. CoRR, abs/0902.2206, 2009. URL http:\n",
      "//arxiv.org/abs/0902.2206.\n",
      "\n",
      "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity\n",
      "in deep neural networks. CoRR, abs/1608.03665, 2016. URL http://arxiv.org/abs/\n",
      "1608.03665.\n",
      "\n",
      "Xundong Wu. High performance binarized neural networks trained on the imagenet classiﬁcation\n",
      "\n",
      "task. CoRR, abs/1604.03058, 2016.\n",
      "\n",
      "Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast\n",
      "optimization, network minimization and transfer learning. In The IEEE Conference on Computer\n",
      "Vision and Pattern Recognition (CVPR), July 2017.\n",
      "\n",
      "Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the per-\n",
      "formance of convolutional neural networks via attention transfer. CoRR, abs/1612.03928, 2016.\n",
      "URL http://arxiv.org/abs/1612.03928.\n",
      "\n",
      "Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quanti-\n",
      "zation: Towards lossless cnns with low-precision weights. CoRR, abs/1702.03044, 2017. URL\n",
      "http://arxiv.org/abs/1702.03044.\n",
      "\n",
      "Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training\n",
      "low bitwidth convolutional neural networks with low bitwidth gradients. CoRR, abs/1606.06160,\n",
      "2016. URL http://arxiv.org/abs/1606.06160.\n",
      "\n",
      "Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. CoRR,\n",
      "\n",
      "abs/1612.01064, 2016. URL http://arxiv.org/abs/1612.01064.\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "7 APPENDIX: ANALYSIS WITH RESNET ON CIFAR-10 DATASET\n",
      "\n",
      "(a) Top-1 error without Apprentice scheme.\n",
      "\n",
      "(b) Top-1 error using Apprentice scheme-A.\n",
      "\n",
      "Figure 6: Comparison of various conﬁgurations of ResNet on CIFAR-10 with and without Apprentice\n",
      "scheme.\n",
      "\n",
      "In addition to ImageNet dataset, we also experiment with Apprentice scheme on CIFAR-10 dataset.\n",
      "CIFAR-10 dataset (Krizhevsky, 2009) consists of 50K training images and 10K testing images in 10\n",
      "classes. We use various depths of ResNet topology for this study. Our implementation of ResNet\n",
      "for CIFAR-10 closely follows the conﬁguration in He et al. (2015). The network inputs are 32×32\n",
      "images. The ﬁrst layer is a 3×3 convolutional layer followed by a stack of 6n layers with 3×3\n",
      "convolutions on feature map sizes 32, 16 and 8; with 2n layers for each feature map size. The\n",
      "numbers of ﬁlters are 16, 32 and 64 in each set of 2n layers. This is followed by a global average\n",
      "pooling, a 10-way fully connected layer and a softmax layer. Thus, in total there are 6n+2 weight\n",
      "layers.\n",
      "\n",
      "Figure 6a shows the impact of lowering precision as the depth of ResNet varies. As the network\n",
      "becomes larger in size, the impact of lowering precision is diminished (relative to the accuracy of\n",
      "the network at that depth when using full-precision). For example, with ResNet-110, full-precision\n",
      "Top-1 error rate is 6.19%. At the same depth, ternarizing the model gives similar error rate (6.24%).\n",
      "Comparing this with ResNet-20, the gap between full-precision and ternary model (2-bits weight\n",
      "and 32-bits activations) is 0.8% (7.9% vs. 8.7% Top-1 error). Overall, we ﬁnd that ternarizing a\n",
      "model closely follows accuracy of baseline full-precision model. However, lowering both weights\n",
      "and activations almost always leads to large accuracy degradation. Accuracy of 2-bits weight and\n",
      "8-bits activation network is 0.8%-1.6% worse than full-precision model. Using Apprentice scheme\n",
      "this gap is considerably lowered.\n",
      "\n",
      "Figure 6b shows the impact of lowering precision when a low-precision (student) network is paired\n",
      "with a full-precision (teacher) network. For this analysis we use scheme-A where we jointly train\n",
      "both the teacher and student network. The mix of ResNet depths we used for this study are ResNet-\n",
      "20, 32, 44, 56, 110 and 182. ResNet-20 student network was paired with deeper ResNets from\n",
      "this mix, i.e. ResNet-32, 44, 56, 110 and 182 (as ﬁve separate experiments). Similarly, ResNet-44\n",
      "\n",
      "15\n",
      "\n",
      "8.1% 7.9% 7.0% 6.9% 6.9% 9.6% 8.0% 7.4% 7.2% 7.0% 8.7% 7.6% 6.8% 6.3% 6.2% 7.9% 7.2% 6.5% 6.2% 6.2% 4% 5% 6% 7% 8% 9% 10% ResNet-20ResNet-32ResNet-44ResNet-56ResNet-110Top-1\terror\t(%)7.4% 6.8% 6.4% 5.9% 5.4% 8.2% 7.1% 6.6% 6.1% 5.6% 8.0% 6.6% 6.3% 5.8% 5.2% 7.4% 6.6% 6.2% 5.5% 5.1% 4% 5% 6% 7% 8% 9% 10% ResNet-20ResNet-32ResNet-44ResNet-56ResNet-110Top-1\terror\t(%)32-bits\tweights,\t32-bits\tactivations2-bits\tweights,\t32-bits\tactivations2-bits\tweights,\t\t\t\t\t8-bits\tactivations4-bits\tweights,\t\t\t\t\t8-bits\tactivations\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "student network was paired with deeper ResNet-56 and 110 (as two different set of experiments).\n",
      "ResNet-110 student network used ResNet-182 as its teacher network. For a particular ResNet depth,\n",
      "the ﬁgure plots the minimum error rate across each of the experiments.\n",
      "\n",
      "We ﬁnd Apprentice scheme to improve the baseline full-precision accuracy. The scheme also helps\n",
      "close the gap between the new improved baseline accuracy and the accuracy when lowering the\n",
      "precision of the weights and activations. The gap between 2-bits weight and 8-bits activation net-\n",
      "work is now 0.4%-0.8% worse than full-precision model. With ImageNet dataset, the accuracy of\n",
      "full-precision networks also improves but very marginally (by 0.3%). However, the impact of dis-\n",
      "tillation technique on CIFAR-10 is quite pronounced - for example, top-1 error lowers by 1.1% for\n",
      "ResNet-110.\n",
      "\n",
      "Figure 7: Distillation followed by quantization.\n",
      "\n",
      "We experimented with a variation of scheme-C where a network is ﬁrst compressed using distillation\n",
      "scheme (using a deeper ResNet as the teacher network) followed by lowering the precision and ﬁne-\n",
      "tuning. The ﬁne-tuning is done for 35-40 epochs with a very low learning-rate without the inﬂuence\n",
      "of any teacher network (no distillation). For this experiment, the student network starts from a higher\n",
      "accuracy compressed model compared to scheme-C (since distillation improves accuracy of student\n",
      "network at full-precision as well). Figure 7 shows the results with this experimental setting. For\n",
      "each conﬁguration, we ﬁnd the error-rate to lie in between the error-rates shown in Figure 6a and\n",
      "Figure 6b for the corresponding conﬁguration, i.e. this scheme is better than low-precision training\n",
      "from scratch but not as good as training with methodology described in scheme-A. On an average,\n",
      "we ﬁnd scheme-A to give 0.7% better accuracy at low-precision conﬁgurations compared to the\n",
      "scheme mentioned here highlighting the beneﬁts of “joint” low-precision training from scratch with\n",
      "distillation (Apprentice scheme). Many works proposing low-precision knobs advocate for training\n",
      "from scratch or training (for a signiﬁcant number of epochs) with warm-startup – the conclusions\n",
      "from this experiment are in line with the observations in these papers.\n",
      "\n",
      "16\n",
      "\n",
      "7.4% 6.6% 6.2% 5.5% 5.1% 8.4% 7.4% 6.7% 6.3% 5.8% 9.2% 7.9% 7.2% 7.0% 6.7% 7.7% 7.7% 6.8% 6.6% 6.5% 4% 5% 6% 7% 8% 9% 10% ResNet-20ResNet-32ResNet-44ResNet-56ResNet-110Top-1\terror\t(%)32-bits\tweights,\t32-bits\tactivations2-bits\tweights,\t\t\t32-bits\tactivations2-bits\tweights,\t\t\t\t\t8-bits\tactivations4-bits\tweights,\t\t\t\t\t8-bits\tactivations\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "8 FUTURE RESEARCH\n",
      "\n",
      "Some works proposing low-precision networks advocate for making the layers wider (or the model\n",
      "larger) to recover accuracy at low-precision. These works propose making the layers wider by 2x\n",
      "or 3x. While these works show the beneﬁts of low-precision, making the model larger increases the\n",
      "number of raw computations. Future work could investigate low-precision and less layer widening\n",
      "factor (say 1.10x or 1.25x). This would help inference latency while maintaining accuracy at-par\n",
      "with baseline full-precision networks.\n",
      "\n",
      "As mentioned in section 5.5, sparsifying a model more than a certain percentage leads to accu-\n",
      "racy loss. Investigating hyper-sparse network models without accuracy loss using distillation based\n",
      "schemes is another interesting avenue of further research.\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sampletxt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "OVERCOMING CATASTROPHIC INTERFERENCE USING\n",
      "CONCEPTOR-AIDED BACKPROPAGATION\n",
      "\n",
      "Xu He, Herbert Jaeger\n",
      "Department of Computer Science and Electrical Engineering\n",
      "Jacobs University Bremen\n",
      "Bremen, 28759, Germany\n",
      "{x.he,h.jaeger}@jacobs-university.de\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Catastrophic interference has been a major roadblock in the research of con-\n",
      "tinual learning. Here we propose a variant of the back-propagation algorithm,\n",
      "“conceptor-aided backprop” (CAB), in which gradients are shielded by concep-\n",
      "tors against degradation of previously learned tasks. Conceptors have their ori-\n",
      "gin in reservoir computing, where they have been previously shown to overcome\n",
      "catastrophic forgetting. CAB extends these results to deep feedforward networks.\n",
      "On the disjoint and permuted MNIST tasks, CAB outperforms two other methods\n",
      "for coping with catastrophic interference that have recently been proposed.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Agents with general artiﬁcial intelligence are supposed to learn and perform well on multiple tasks.\n",
      "Continual learning refers to the scenarios where a machine learning system can retain previously\n",
      "acquired skills while learning new ones. However, when trained on a sequence of tasks, neural\n",
      "networks usually forget about previous tasks after their weights are adjusted for a new task. This\n",
      "notorious problem known as catastrophic interference (CI) (McCloskey & Cohen, 1989; Ratcliff,\n",
      "1990; French, 1999; Kumaran et al., 2016) poses a serious challenge towards continual learning.\n",
      "\n",
      "Many approaches have been proposed to overcome or mitigate the problem of CI in the last three\n",
      "decades (Hinton & Plaut, 1987; French, 1991; Ans & Rousset, 1997; French, 1997; Srivastava et al.,\n",
      "2014). Especially recently, an avalanche of new methods in the deep learning ﬁeld has brought\n",
      "about dramatic improvements in continual learning in neural networks. Kirkpatrick et al. (2017)\n",
      "introduced a regularization-based method called elastic weight consolidation (EWC), which uses\n",
      "the posterior distribution of parameters for the old tasks as a prior for the new task. They approx-\n",
      "imated the posterior by a Gaussian distribution with the parameters for old tasks as the mean and\n",
      "the inverse diagonal of the Fisher information matrix as the variance. Lee et al. (2017) introduced\n",
      "two incremental moment matching (IMM) methods called mean-IMM and mode-IMM. Mean-IMM\n",
      "approximates the distribution of parameters for both old and new tasks by a Gaussian distribution,\n",
      "which is estimated by minimizing its KL-divergence from the mixture of two Gaussian posteriors,\n",
      "one for the old task and the other one for the new task. Mode-IMM estimates the mode of this\n",
      "mixture of two Gaussians and uses it as the optimal parameters for both tasks.\n",
      "\n",
      "In the ﬁeld of Reservoir Computing (Jaeger, 2001; Maass et al., 2002), an effective solution to CI\n",
      "using conceptors was proposed by Jaeger (2014) to incrementally train a recurrent neural network\n",
      "to generate spatial-temporal signals. Conceptors are a general-purpose neuro-computational mecha-\n",
      "nism that can be used in a diversity of neural information processing tasks including temporal pattern\n",
      "classiﬁcation, one-shot learning, human motion pattern generation, de-noising and signal separation\n",
      "(Jaeger, 2017).\n",
      "In this paper, we adopt and extend the method introduced in Jaeger (2014) and\n",
      "propose a conceptor-aided backpropagation (CAB) algorithm to train feed-forward networks. For\n",
      "each layer of a network, CAB computes a conceptor to characterize the linear subspace spanned by\n",
      "the neural activations in that layer that have appeared in already learned tasks. When the network\n",
      "is trained on a new task, CAB uses the conceptor to adjust the gradients given by backpropagation\n",
      "so that the linear transformation restricted to the characterized subspace will be preserved after the\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "gradient descent procedure. Experiment results of two benchmark tests showed highly competitive\n",
      "performance of CAB.\n",
      "\n",
      "The rest of this paper is structured as follows. Section 2 introduces conceptors and their application\n",
      "to incremental learning by ridge regression. Section 3 extends the method to stochastic gradient\n",
      "descent and describes the CAB algorithm. Section 4 compares its performance on the permuted and\n",
      "disjoint MNIST tasks to recent methods that address the same problem. Finally we conclude our\n",
      "paper in Section 5.\n",
      "\n",
      "2\n",
      "\n",
      "INCREMENTAL RIDGE REGRESSION BY CONCEPTORS\n",
      "\n",
      "This section reviews the basics of conceptor theory and its application to incrementally training\n",
      "linear readouts of recurrent neural networks as used in reservoir computing. A comprehensive treat-\n",
      "ment can be found in (Jaeger, 2014).\n",
      "\n",
      "2.1 CONCEPTORS\n",
      "\n",
      "Figure 1: 3D point clouds (black dots) and their corresponding conceptors, represented by ellipsoids\n",
      "whose axes are the singular vectors of conceptors and the lengths of these axes match the singular\n",
      "values of conceptors. Each edge of the plot boxes range from −1 to +1 admitted by neural dynamics\n",
      "with a tanh nonlinearity; conceptor ellipsiods lie inside the unit sphere.\n",
      "\n",
      "In brief, a matrix conceptor C for some vector-valued random variable x ∈ RN is deﬁned as a linear\n",
      "transformation that minimizes the following loss function.\n",
      "\n",
      "Ex[||x − Cx||2] + α−2||C||2\n",
      "fro\n",
      "\n",
      "(1)\n",
      "\n",
      "where α is a control parameter called aperture and || · ||fro is the Frobenius norm. This optimization\n",
      "problem has a closed-form solution\n",
      "\n",
      "C = R(R + α−2I)−1\n",
      "\n",
      "(2)\n",
      "where R = Ex[xx(cid:62)] is the N × N correlation matrix of x, and I is the N × N identity matrix. This\n",
      "result given in (2) can be understood by studying the singular value decomposition (SVD) of C. If\n",
      "R = U ΣU (cid:62) is the SVD of R, then the SVD of C is given as U SU (cid:62), where the singular values\n",
      "si of C can be written in terms of the singular values σi of R: si = σi/(σi + α−2) ∈ [0, 1). In\n",
      "intuitive terms, C is a soft projection matrix on the linear subspace where the samples of x lie. For\n",
      "a vector y in this subspace, C acts like the identity: Cy ≈ y, and when some noise (cid:15) orthogonal to\n",
      "the subspace is added to y, C de-noises: C(y + (cid:15)) ≈ y. Figure 1 shows the ellipsoids corresponding\n",
      "to three sets of R3 points. We deﬁne the quota Q(C) of a conceptor to be the mean singular values:\n",
      "Q(C) := 1\n",
      "i=1 si. Intuitively, the quota measures the fraction of the total dimensions of the\n",
      "N\n",
      "entire vector space that is claimed by C.\n",
      "\n",
      "(cid:80)N\n",
      "\n",
      "Moreover, logic operations that satisfy most laws of Boolean logic can be deﬁned on matrix concep-\n",
      "tors as the following:\n",
      "\n",
      "¬C :=I − C,\n",
      "\n",
      "C i ∨ C j :=(Ri + Rj)(Ri + Rj + α−2I)−1\n",
      "C i ∧ C j :=¬(¬C i ∨ ¬C j)\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "where ¬C softly projects onto a linear subspace that can be roughly understood as the orthogonal\n",
      "complement of the subspace characterized by C. C i ∨ C j is the conceptor computed from the union\n",
      "of the two sets of sample points from which C i and C j are computed. It describes a space that is\n",
      "approximately the sum of linear subspaces characterized by C i and C j, respectively. The deﬁnition\n",
      "of C i ∧ C j reﬂects de Morgan’s law. Figure 2 illustrates the geometry of these operations.\n",
      "\n",
      "Figure 2: Geometry of Boolean operations on 2-dimensional conceptors. The OR (resp. AND) op-\n",
      "eration gives a conceptor whose ellipsoid approximately is the smallest (largest) ellipsoid enclosing\n",
      "(contained in) the argument conceptor’s ellipsoids.\n",
      "\n",
      "2.2\n",
      "\n",
      "INCREMENTAL RIDGE REGRESSION\n",
      "\n",
      "This subsection explains how conceptors can be applied to master continual learning in a simple\n",
      "linear model trained on a supervised task by ridge regression. The training is done sequentially on\n",
      "multiple input-to-output mapping tasks. This simpliﬁed scenario illustrates the working principle of\n",
      "continual learning with conceptors and will later be used repeatedly as a sub-procedure in the CAB\n",
      "algorithm for training multilayer feed-forward networks.\n",
      "\n",
      "n, yj\n",
      "\n",
      "1, yj\n",
      "\n",
      "1), · · · , (xj\n",
      "\n",
      "n)}, where xj\n",
      "\n",
      "i ∈ RN are input vectors and yj\n",
      "\n",
      "Consider a sequence of m incoming tasks indexed by j. We denote the training dataset for the j-th\n",
      "task by {(xj\n",
      "i ∈ RM their correspond-\n",
      "ing target outputs. Whenever the training dataset for a new task is available, the incremental learning\n",
      "method will compute a matrix conceptor C j for the input variable of the new task using Equation\n",
      "2 and update the linear model, resulting in a sequence of linear models W 1, . . . W m such that W j\n",
      "solves not only the j-th task but also all previous tasks: for k ≤ j, yk ≈ W jxk. The conceptor C j is\n",
      "a soft projection matrix onto the linear subspace spanned by input patterns from the j-th task. Then,\n",
      "Aj−1 = C 1 ∨ · · · ∨ C j−1 characterizes the memory space already claimed by the tasks 1, . . . , j − 1\n",
      "and F j = ¬Aj−1, the orthogonal complement of Aj − 1, represents the memory space still free for\n",
      "the j-th task. Here “memory space” refers to the linear space of input vectors. In detail, this method\n",
      "proceeds in the following way:\n",
      "\n",
      "• Initialization (no task trained yet): W 0 = 0M ×N , A0 = 0N ×N .\n",
      "• Incremental task learning: For tasks j = 1, . . . , m do:\n",
      "\n",
      "1. Store the input vectors from the j-th training dataset of size n into a N × n sized input\n",
      "collection matrix X j, and store the output vectors into a M ×n sized output collection\n",
      "matrix Y j.\n",
      "\n",
      "2. Compute the conceptor for this task by C j = Rj(Rj + α−2I)−1, where Rj =\n",
      "\n",
      "inc (to be added to W j−1, yielding W j), with the crucial\n",
      "\n",
      ":= ¬Aj−1 (comment: this conceptor characterizes the “still disposable”\n",
      "\n",
      "(b) T := Y j − (W j−1X j) (comment: this matrix consists of target values for a linear\n",
      "\n",
      "(c) S := F jX j (comment: this matrix consists of input arguments for the linear\n",
      "\n",
      "1\n",
      "\n",
      "n X jX j(cid:62)\n",
      "\n",
      "3. Train an increment matrix W j\n",
      "aid of a helper conceptor F j:\n",
      "(a) F j\n",
      "\n",
      "memory space for the j-th task),\n",
      "\n",
      "regression to compute W j\n",
      "\n",
      "inc),\n",
      "\n",
      "regression),\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "(d) W j\n",
      "\n",
      "inc = ((SS(cid:62)/n + λ−2I)−1ST (cid:62)/n)(cid:62) (comment: carry out the regression,\n",
      "\n",
      "regularized by λ−2),\n",
      "\n",
      "4. Update W j: W j = W j−1 + W j\n",
      "5. Update A : Aj = Aj−1 ∨ C j (comment: this is possible due to the associativity of the\n",
      "\n",
      "inc.\n",
      "\n",
      "∨ operation on conceptors)\n",
      "\n",
      "The weight increment W j\n",
      "inc does not interfere much with the previously learned weights W j−1\n",
      "because the regularization in step 3(d) constrains the row space of W j\n",
      "inc to be only the linear sub-\n",
      "space spanned by input arguments deﬁned in 3(c), which are inside the kernel of W j−1 due to the\n",
      "projection by F j. Intuitively speaking, when learning a new task, this algorithm exploits only the\n",
      "components of input vectors in the still unused space (kernel of W j−1, characterized by F j) to\n",
      "compensate errors for the new task and leaves the directions in the already used memory space (row\n",
      "space of W j−1, characterized by Aj−1) intact.\n",
      "\n",
      "3 CONCEPTOR-AIDED SGD AND BACK-PROP\n",
      "\n",
      "In this section, we ﬁrst derive a stochastic gradient descent version of the algorithm described in the\n",
      "previous section, then present the procedure of CAB.\n",
      "\n",
      "3.1 SGD\n",
      "\n",
      "In the algorithm introduced in the previous section, W j\n",
      "offers a closed-form solution to minimize the following cost function\n",
      "incs − t|2] + λ−2|W j\n",
      "\n",
      "inc) := E[|W j\n",
      "\n",
      "J (W j\n",
      "\n",
      "inc|2\n",
      "fro\n",
      "\n",
      "inc is computed by ridge regression, which\n",
      "\n",
      "where t = yj − W j−1xj, s = F jxj. One can also minimize this cost function by stochastic\n",
      "gradient descent (SGD), which starts from an initial guess of W j\n",
      "inc and repeatedly performs the\n",
      "following update\n",
      "\n",
      "inc − η∇W j\n",
      "where η is the learning rate and the gradient is given by:\n",
      "\n",
      "inc ← W j\n",
      "\n",
      "W j\n",
      "\n",
      "inc\n",
      "\n",
      "J (W j\n",
      "\n",
      "inc)\n",
      "\n",
      "∇W j\n",
      "\n",
      "inc\n",
      "\n",
      "J (W j\n",
      "\n",
      "inc) = 2E[(W j\n",
      "\n",
      "incs − t)s(cid:62)] + 2λ−2W j\n",
      "\n",
      "inc\n",
      "\n",
      "Substituting t by yj − W j−1xj and s by F jxj = (I − Aj−1)xj in (8), we get\n",
      "\n",
      "∇W j\n",
      "\n",
      "inc\n",
      "\n",
      "J (W j\n",
      "\n",
      "inc) = 2E[(W j\n",
      "\n",
      "inc(I − Aj−1)xj − yj + W j−1xj)s(cid:62)] + 2λ−2W j\n",
      "\n",
      "inc\n",
      "\n",
      "= 2E[(−W j\n",
      "\n",
      "incAj−1xj + (W j−1 + W j\n",
      "\n",
      "inc)xj − yj)s(cid:62)] + 2λ−2W j\n",
      "\n",
      "inc\n",
      "\n",
      "Due to the regularization term in the cost function, as the optimization goes on, eventually Winc\n",
      "will null the input components that are not inside the linear subspace characterized by F j, hence\n",
      "W j\n",
      "inc,\n",
      "(10) can be simpliﬁed to\n",
      "\n",
      "incAj−1xj will converge to 0 as the algorithm proceeds. In addition, since W j = W j−1 + W j\n",
      "\n",
      "∇W j\n",
      "\n",
      "inc\n",
      "\n",
      "J (W j\n",
      "\n",
      "inc) = 2E[(W jxj − yj)s(cid:62)] + 2λ−2W j\n",
      "\n",
      "inc\n",
      "\n",
      "Adding W j−1 to both sides of (7), we obtain the update rule for W j:\n",
      "\n",
      "W j ← W j − 2ηE[es(cid:62)] + 2ηλ−2W j\n",
      "\n",
      "inc\n",
      "\n",
      "where e := W jxj − yj. In practice, at every iteration, the expected value can be approximated by a\n",
      "mini-batch of size nB, indexed by iB:\n",
      "\n",
      "ˆE[es(cid:62)] =\n",
      "\n",
      "(W jxj\n",
      "iB\n",
      "\n",
      "− yj\n",
      "iB\n",
      "\n",
      ")(F jxj\n",
      "iB\n",
      "\n",
      ")(cid:62) =\n",
      "\n",
      "(W jxj\n",
      "iB\n",
      "\n",
      "− yj\n",
      "iB\n",
      "\n",
      ")xj(cid:62)\n",
      "iB\n",
      "\n",
      "F j\n",
      "\n",
      "(13)\n",
      "\n",
      "1\n",
      "nB\n",
      "\n",
      "L\n",
      "(cid:88)\n",
      "\n",
      "iB =0\n",
      "\n",
      "1\n",
      "nB\n",
      "\n",
      "L\n",
      "(cid:88)\n",
      "\n",
      "iB =0\n",
      "\n",
      "where the transpose for F j can be dropped since it is symmetric.\n",
      "\n",
      "(6)\n",
      "\n",
      "(7)\n",
      "\n",
      "(8)\n",
      "\n",
      "(9)\n",
      "\n",
      "(10)\n",
      "\n",
      "(11)\n",
      "\n",
      "(12)\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "If we only train the j−th task without considering the previous tasks, the update rule given by normal\n",
      "SGD is\n",
      "\n",
      "W j ← W j − 2ηE[exj(cid:62)] + 2ηλ−2W j\n",
      "\n",
      "(14)\n",
      "\n",
      "Comparing this to the update rule in (12), we notice two modiﬁcations when a conceptor is adopted\n",
      "to avoid CI: ﬁrst, the gradient of weights are calculated using the conceptor-projected input vector\n",
      "s = F jxj instead of the original input vector xj; second, regularization is done on the weight\n",
      "increment W j\n",
      "inc rather than the ﬁnal weight W j. These two modiﬁcations lead to our design of the\n",
      "conceptor-aided algorithm for training multilayer feed-forward networks.\n",
      "\n",
      "3.2 BACKPROP\n",
      "\n",
      "The basic idea of CAB is to guide the gradients of the loss function on every linear component\n",
      "of the network by a matrix conceptor computed from previous tasks during error back-propagation\n",
      "(Rumelhart et al., 1986), repeatedly applying the conceptor-aided SGD technique introduced in the\n",
      "previous section in every layer.\n",
      "\n",
      "Consider a feed-forward network with L + 1 layers, indexed by l = 0, . . . L, such that the 0-th and\n",
      "the L-th layers are the input and output layers respectively. W (l) represents the linear connections\n",
      "between the (l − 1)-th and the l-th layer, where we refer to the former as the pre-synaptic layer with\n",
      "respect to W (l), and to the latter as the post-synaptic layer. We denote by N (l) the size of the l-th\n",
      "layer (excluding the bias unit) and A(l)j\n",
      "a conceptor characterizing the memory space in the l-th\n",
      "layer used up by the ﬁrst j tasks. Let σ(·) be the activation function of the nonlinear neurons and\n",
      "θ all the parameters of the network to be trained. Then the incremental training method with CAB\n",
      "proceeds as follows:\n",
      "\n",
      "• Initialization (no task trained yet): ∀l = 0, . . . , L − 1, A(l)0\n",
      "\n",
      ":= 0(N (l)+1)×(N (l)+1), and\n",
      "\n",
      "randomly initialize W (l+1)0\n",
      "\n",
      "to be a matrix of size N (l+1) × (N (l) + 1).\n",
      "\n",
      "• Incremental task learning: For j = 1, . . . , m do:\n",
      "\n",
      "1. ∀l = 0, . . . , L − 1, F (l)j\n",
      "\n",
      "= ¬A(l)(j−1)\n",
      "\n",
      ". (This conceptor characterizes the still dispos-\n",
      "\n",
      "able vector space in layer l for learning task j)\n",
      "\n",
      "2. Update the network parameters θ(j−1) obtained after training the ﬁrst j − 1 tasks to\n",
      "θj by stochastic gradient descent, where the gradients are computed by CAB instead\n",
      "of the classical backprop. Algorithms 1 and 2 detail the forward and backward pass\n",
      "of CAB, respectively. Different from classical backprop, the gradients are guided by a\n",
      "matrix conceptor F (l)j\n",
      ", such that in each layer only the activity in the still disposable\n",
      "memory space will contribute to the gradient. Note that the conceptors remain the\n",
      "same until convergence of the network for task j.\n",
      "\n",
      "3. After training on the j-th task, run the forward procedure again on a batch of nB input\n",
      "j\n",
      "vectors, indexed by iB, taken from the j-th training dataset, to collect activations h(l)\n",
      "iB\n",
      "of each layer into a N (l) × nB sized matrix H (l)j\n",
      ", and set the correlation matrix\n",
      "R(l)j\n",
      "\n",
      "(H (l)j\n",
      "\n",
      "H (l)j\n",
      "\n",
      ")(cid:62).\n",
      "\n",
      "= 1\n",
      "nB\n",
      "\n",
      "4. Compute a conceptor on the l-th layer for the j-th pattern by C (l)j\n",
      "\n",
      "+\n",
      "α−2IN (l)×N (l))−1, ∀l = 0, . . . , L − 1. Finding an optimal aperture can be done by a\n",
      "cross-validation search1.\n",
      "\n",
      "= R(l)j\n",
      "\n",
      "(R(l)j\n",
      "\n",
      "5. Update the conceptor for already used space in every layer: A(l)j\n",
      "\n",
      "= A(l)j\n",
      "\n",
      "∨\n",
      "\n",
      "C (l)j\n",
      "\n",
      ", ∀l = 0, . . . , L − 1.\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "Algorithm 1 The forward procedure of conceptor-aided backprop, adapted from the traditional\n",
      "backprop. Input vectors are passed through a feed-forward network to compute the cost function.\n",
      "L(ˆyj, yj) denotes the loss for the j-th task, to which a regularizer Ω(θj\n",
      "inc) = Ω(θj − θj−1) =\n",
      "||θj − θj−1||2\n",
      "fro is added to obtain the total cost J , where θ contains all the weights (biases are\n",
      "considered as weights connected to the bias units). The increment of parameters rather than the\n",
      "parameters themselves are regularized, similar to the conceptor-aided SGD.\n",
      "Require: Network depth, l\n",
      "Require: W (l)j\n",
      "Require: xj, one input vector of the j-th task\n",
      "Require: yj, the target output for xj\n",
      "1: h(0) = xj\n",
      "2: for l = 1, . . . L do\n",
      "3:\n",
      "\n",
      ", l ∈ {1, . . . , L}, the weight matrices of the network\n",
      "\n",
      "b(l) = [h(l−1)(cid:62), 1](cid:62), include the bias unit\n",
      "a(l) = W (l)j\n",
      "h(l) = σ(a(l))\n",
      "\n",
      "b(l)\n",
      "\n",
      "4:\n",
      "5:\n",
      "6: end for\n",
      "7: ˆyj = h(l)\n",
      "8: J = L(ˆyj, yj) + λΩ(θj\n",
      "\n",
      "inc)\n",
      "\n",
      "Algorithm 2 The backward procedure of conceptor-aided backprop for the j-th task, adapted from\n",
      "the traditional backprop. The gradient g of the loss function L on the activations a(l) represents\n",
      "the error for the linear transformation W (l)j\n",
      "between the (l − 1)-th and the l−th layers. In the\n",
      "standard backprop algorithm, the gradient of L on W (l)j\n",
      "is computed as an outer product of the\n",
      "post-synaptic errors g and the pre-synaptic activities h(l−1). This resembles the computation of the\n",
      "gradient in the linear SGD algorithm, which motivates us to apply conceptors in a similar fashion as\n",
      "in the conceptor-aided SGD. Speciﬁcally, we project the gradient ∇W (l)j L by the matrix conceptor\n",
      "F (l−1)j\n",
      "1:\n",
      "\n",
      "that indicates the free memory space on the pre-synaptic layer.\n",
      "\n",
      "2: for l = L, L − 1, . . . , 1 do\n",
      "3:\n",
      "\n",
      "((cid:12) denotes element-wise multiplication):\n",
      "\n",
      "Convert the gradient on the layer’s output into a gradient on the pre-nonlinearity activation\n",
      "\n",
      "g ← ∇ˆyJ = ∇ˆyL(ˆy, y)\n",
      "\n",
      "g ← ∇a(l) J = g (cid:12) σ(cid:48)(a(l))\n",
      "\n",
      "4:\n",
      "\n",
      "Compute the gradient of weights, project it by F (l−1)j\n",
      "\n",
      ", and add it to the regularization term\n",
      "\n",
      "on the increment:\n",
      "\n",
      "∇W (l)j J =g(F (l−1)j\n",
      "=gb(l−1)(cid:62)\n",
      "\n",
      "b(l−1))(cid:62) + λ∇W (l)j Ω(θj\n",
      "F (l−1)j\n",
      "\n",
      "+ 2λ(W (l)j\n",
      "\n",
      "− W (l)j−1\n",
      "\n",
      ")\n",
      "\n",
      "inc) = gb(l−1)(cid:62)\n",
      "\n",
      "F (l−1)j\n",
      "\n",
      "+ 2λW (l)\n",
      "inc\n",
      "\n",
      "j\n",
      "\n",
      "5:\n",
      "\n",
      "Propagate the gradients w.r.t. the next lower-level hidden layers activations:\n",
      "\n",
      "6: end for\n",
      "\n",
      "g ← ∇h(l−1) J = W (l)j (cid:62)\n",
      "\n",
      "g\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "Figure 3: Average performance across already learned permuted MNIST tasks using CAB or EWC\n",
      "\n",
      "4 EXPERIMENTS\n",
      "\n",
      "4.1 PERMUTED MNIST EXPERIMENT\n",
      "\n",
      "To test the performance of CAB, we evaluated it on the permuted MNIST experiment (Srivastava\n",
      "et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2017; Lee et al., 2017), where a sequence of\n",
      "pattern recognition tasks are created from the MNIST dataset (LeCun et al., 1998). For each task, a\n",
      "random permutation of input image pixels is generated and applied to all images in MNIST to obtain\n",
      "a new shufﬂed dataset, equally difﬁcult to recognize as the original one, the objective of each task is\n",
      "to recognize these images with shufﬂed pixels.\n",
      "\n",
      "For a proof-of-concept demonstration, we trained a simple but sufﬁcient feed-forward network with\n",
      "[784-100-10] of neurons to classify 10 permuted MNIST datasets. The network has logistic sigmoid\n",
      "neurons in both hidden and output layers, and is trained with mean squared error as the cost function.\n",
      "Vanilla SGD was used in all experiments to optimize the cost function. Learning rate and aperture\n",
      "were set to 0.1 and 4, respectively. For comparison, we also tested EWC on the same task with the\n",
      "same network architecture, based on the implementation by Seff (2017). The parameters chosen\n",
      "for the EWC algorithm were 0.01 for the learning rate and 15 for the weight of the Fisher penalty\n",
      "term. Figure 3 shows the performance of CAB on this task, the average testing accuracy is 95.2%\n",
      "after learning all 10 tasks sequentially. Although a fair amount of effort was spent on searching for\n",
      "optimal parameters for EWC, the accuracies shown here might still not reﬂect its best performance.\n",
      "However, the same experiment with EWC was also conducted in Kemker et al. (2017), where the\n",
      "authors reimplemented EWC on a network with higher capacity (2 hidden layers and 400 ReLU\n",
      "neurons per layer) and the resulting average accuracy after learning 10 tasks sequentially was shown\n",
      "to be around 93%.\n",
      "\n",
      "Since all tasks are generated by permuting the same dataset, the portion of the input space occupied\n",
      "by each of them should have the same size. However, as more tasks are learned, the chance that the\n",
      "space of a new task will overlap with the already used input space increases. Figure 4 shows the\n",
      "singular value spectra and quota of the input and hidden layer conceptors every time after a new task\n",
      "is learned. As the incremental learning proceeds, it becomes less likely for a new task to be in the\n",
      "free space. For example, the second task increases the quota of the input layer memory space by 0.1,\n",
      "whereas the 10th task increases it by only 0.03. However, CAB still manages to make the network\n",
      "learn new tasks based on their input components in the non-overlapping space.\n",
      "\n",
      "1Jaeger (2014) proposes a number of methods for analytical aperture optimization. It remains for future\n",
      "\n",
      "work to determine how these methods transfer to our situation.\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "(a) Singular value spectra of conceptors A(0)j\n",
      "\n",
      "on the input layer.\n",
      "\n",
      "(b) Singular value spectra of conceptors A(1)j\n",
      "\n",
      "on the hidden layer.\n",
      "\n",
      "Figure 4: The development of singular value spectra of conceptors for “used-up” space on the input\n",
      "layer and hidden layer during incremental learning of 10 permuted MNIST tasks. Quota of these\n",
      "conceptors are displayed in the legends.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "4.2 DISJOINT MNIST EXPERIMENT\n",
      "\n",
      "We then applied CAB to categorize the disjoint MNIST datasets into 10 classes (Srivastava et al.,\n",
      "2013; Lee et al., 2017). In this experiment, the original MNIST dataset is divided into two disjoint\n",
      "datasets with the ﬁrst one consisting of data for the ﬁrst ﬁve digits (0 to 4), and the second one\n",
      "of the remaining ﬁve digits (5 to 9). This task requires a network to learn these two datasets one\n",
      "after the other, then examines its performance of classifying the entire MNIST testing images into\n",
      "10 classes. The current state-of-the-art accuracy on this task, averaged over 10 learning trials, is\n",
      "94.12(±0.27)%, achieved by Lee et al. (2017) using IMM. They also tested EWC on the same task\n",
      "and the average accuracy was 52.72(±1.36)%.\n",
      "\n",
      "To test our method, we trained a feed-forward network with [784-800-10] neurons. Logistic sigmoid\n",
      "nonlinearities were used in both hidden and output layers, and the network was trained with vanilla\n",
      "SGD to minimize mean squared errors. The aperture α = 9 was used for all conceptors on all layers,\n",
      "learning rate η and regularization coefﬁcient λ were chosen to be 0.1 and 0.005 respectively. The\n",
      "accuracy of CAB on this task, measured by repeating the experiment 10 times, is 94.91(±0.30)%.\n",
      "It is worth mentioning that the network used by Lee et al. (2017) for testing IMM and EWC had\n",
      "[784-800-800-10] rectiﬁed linear units (ReLU), so CAB achieved better performance with fewer\n",
      "layers and neurons.\n",
      "\n",
      "4.3 COMPUTATIONAL COST\n",
      "\n",
      "If a conceptor is computed by ridge regression, the time complexity is O(nN 2 + N 3) when the\n",
      "design matrix is dense, where n is the number of samples and N the number of features. In terms of\n",
      "wall time measures, the time taken to compute a conceptor from the entire MNIST training set (in\n",
      "this case, n = 55000 images and N = 784 pixels, corresponding to the input layer in our networks)\n",
      "is 0.42 seconds of standard notebook CPU time on average. Although we did not implement it in\n",
      "these experiments, incremental online adaptation of conceptors by gradient descent is also possible\n",
      "in principle and would come at a cost of O(N 2) per update.\n",
      "\n",
      "5 CONCLUSION\n",
      "\n",
      "In this work, we ﬁrst reviewed the conceptor-based incremental ridge regression algorithm, intro-\n",
      "duced in section 3.11 of Jaeger (2014) for memory management in recurrent neural networks. Then\n",
      "we derived its stochastic gradient descent version for optimizing the same objective. Finally we\n",
      "designed a conceptor-aided backprop algorithm by applying a conceptor to every linear layer of a\n",
      "feed-forward network. This method uses conceptors to guide gradients of parameters during the\n",
      "backpropagation procedure. As a result, learning a new task interferes only minimally with pre-\n",
      "viously learned tasks, and the amount of already used network capacity can be monitored via the\n",
      "singular value spectra and quota of conceptors.\n",
      "\n",
      "In Jaeger (2014), different scenarios for continual learning are investigated in a reservoir computing\n",
      "setting. Two extreme cases are obtained when (i) the involved learning tasks are entirely unrelated\n",
      "to each other, versus (ii) all tasks come from the same parametric family of learning tasks. The two\n",
      "cases differ conspicuously with regards to the geometry of involved conceptors, and with regards\n",
      "to opportunities to re-use previously acquired functionality in subsequent learning episodes. The\n",
      "permuted MNIST task is an example of (i) while the disjoint MNIST task rather is of type (ii).\n",
      "Conceptors provide an analytical tool to discuss the “family relatedness” and enabling/disabling\n",
      "conditions for continual learning in geometrical terms. Ongoing and future research is devoted to\n",
      "a comprehensive mathematical analysis of these phenomena which in our view lie at the heart of\n",
      "understanding continual learning.\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "The work reported in this article was partly funded through the European H2020 collaborative\n",
      "project NeuRAM3 (grant Nr 687299).\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Bernard Ans and St´ephane Rousset. Avoiding catastrophic forgetting by coupling two reverberating\n",
      "neural networks. Comptes Rendus de l’Acad´emie des Sciences-Series III-Sciences de la Vie, 320\n",
      "(12):989–997, 1997.\n",
      "\n",
      "Robert M French. Using semi-distributed representations to overcome catastrophic forgetting in\n",
      "connectionist networks. Proceedings of the 13th Annual Cognitive Science Society Conference,\n",
      "pp. 173178, 1991.\n",
      "\n",
      "Robert M French. Pseudo-recurrent connectionist networks: An approach to the ‘sensitivity-\n",
      "\n",
      "stability’ dilemma. Connection Science, 9(4):353–380, 1997.\n",
      "\n",
      "Robert M French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences,\n",
      "\n",
      "3(4):128–135, 1999.\n",
      "\n",
      "Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical inves-\n",
      "tigation of catastrophic forgetting in gradient-based neural networks. International Conference\n",
      "on Learning Representations, 2014.\n",
      "\n",
      "Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of\n",
      "the Ninth Annual Conference of the Cognitive Science Society, pp. 177–186. Lawrence Erlbaum\n",
      "Associates, 1987.\n",
      "\n",
      "Herbert Jaeger. The echo state approach to analysing and training recurrent neural networks-with\n",
      "an erratum note. German National Research Center for Information Technology GMD Technical\n",
      "Report, 148(34):13, 2001.\n",
      "\n",
      "Herbert Jaeger. Controlling recurrent neural networks by conceptors. Jacobs University Technical\n",
      "\n",
      "Reports, (31), 2014. https://arxiv.org/abs/1403.3369.\n",
      "\n",
      "Herbert Jaeger. Using conceptors to manage neural long-term memories for temporal patterns. Jour-\n",
      "nal of Machine Learning Research, 18(13):1–43, 2017. URL http://jmlr.org/papers/\n",
      "v18/15-449.html.\n",
      "\n",
      "Ronald Kemker, Angelina Abitino, Marc McClure, and Christopher Kanan. Measuring catastrophic\n",
      "forgetting in neural networks. Computing Research Repository, abs/1708.02072, 2017. http:\n",
      "//arxiv.org/abs/1708.02072.\n",
      "\n",
      "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.\n",
      "Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-\n",
      "abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting\n",
      "in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521, 2017.\n",
      "\n",
      "Dharshan Kumaran, Demis Hassabis, and James L McClelland. What learning systems do intelligent\n",
      "agents need? complementary learning systems theory updated. Trends in Cognitive Sciences, 20\n",
      "(7):512–534, 2016.\n",
      "\n",
      "Yann LeCun, Corinna Cortes, and Christopher JC Burges. The MNIST database of handwritten\n",
      "\n",
      "digits. 1998. http://yann.lecun.com/exdb/mnist/.\n",
      "\n",
      "Sang-Woo Lee, Jin-Hwa Kim, JungWoo Ha, and Byoung-Tak Zhang. Overcoming catastrophic\n",
      "forgetting by incremental moment matching. Computing Research Repository, abs/1703.08475,\n",
      "2017. http://arxiv.org/abs/1703.08475.\n",
      "\n",
      "Wolfgang Maass, Thomas Natschl¨ager, and Henry Markram. Real-time computing without stable\n",
      "states: A new framework for neural computation based on perturbations. Neural Computation,\n",
      "14(11):2531–2560, 2002.\n",
      "\n",
      "Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\n",
      "\n",
      "sequential learning problem. Psychology of Learning and Motivation, 24:109–165, 1989.\n",
      "\n",
      "Roger Ratcliff. Connectionist models of recognition memory: Constraints imposed by learning and\n",
      "\n",
      "forgetting functions. Psychological Review, 97(2):285–308, 1990.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2018\n",
      "\n",
      "David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-\n",
      "\n",
      "propagating errors. Nature, 323:533–535, 1986.\n",
      "\n",
      "Ari Seff.\n",
      "\n",
      "Implementation of overcoming catastrophic forgetting in neural networks\n",
      "https://github.com/ariseff/\n",
      "\n",
      "GitHub Repository,\n",
      "\n",
      "2017.\n",
      "\n",
      "in tensorﬂow.\n",
      "overcoming-catastrophic.\n",
      "\n",
      "Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and J¨urgen Schmid-\n",
      "huber. Compete to compute. In Advances in Neural Information Processing Systems, pp. 2310–\n",
      "2318, 2013. http://papers.nips.cc/paper/5059-compete-to-compute.pdf.\n",
      "\n",
      "Vipin Srivastava, Suchitra Sampath, and David J Parker. Overcoming catastrophic interference in\n",
      "connectionist networks using Gram-Schmidt orthogonalization. PloS ONE, 9(9):e105619, 2014.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sampletxt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"Figure \\d:[ A-Za-z0-9,;']*\\.\"\n",
    "matches = re.finditer(regex, sampletxt_2, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    print(match[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "di = {'Hashtags':[], 'Frequency':[]}\n",
    "\n",
    "\n",
    "for tweet in dftest2['2019-06-05']['text']:\n",
    "    matches = re.finditer(regex, tweet, re.MULTILINE)\n",
    "    for match in matches:\n",
    "        if match[0] in di['Hashtags']:\n",
    "            i=di['Hashtags'].index(match[0])\n",
    "            di['Frequency'][i] = di['Frequency'][i]+1\n",
    "        else:\n",
    "            di['Hashtags'].append(match[0])\n",
    "            di['Frequency'].append(1)\n",
    "            \n",
    "\n",
    "#create a dataframe out of dict\n",
    "x = pd.DataFrame.from_dict(di)\n",
    "x = x[x['Hashtags']!='#']\n",
    "x = x[x['Hashtags']!=' #']\n",
    "x = x[x['Hashtags']!='# ']\n",
    "x.sort_values([\"Frequency\", \"Hashtags\"], axis=0, ascending=[False,False]).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
