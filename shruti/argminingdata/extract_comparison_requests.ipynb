{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/data/naacl_dataset/iclr_anno_final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_files = glob.glob(data_path + \"*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " ['/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/data/naacl_dataset/iclr_anno_final/S1ufxZqlG_rating_4.txt',\n",
       "  '/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/data/naacl_dataset/iclr_anno_final/SkKuc-Kef_rating_4.txt',\n",
       "  '/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/data/naacl_dataset/iclr_anno_final/ryBakJUlz_rating_8.txt',\n",
       "  '/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/data/naacl_dataset/iclr_anno_final/H1NffmKgz_rating_6.txt'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotated_files), annotated_files[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct review dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### {paper_id: [s1, s2, s3 ...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = {}\n",
    "\n",
    "for rev_arg_file in annotated_files:\n",
    "    with open(rev_arg_file, \"r\") as f:\n",
    "        paper_id = rev_arg_file.split(\"/\")[-1].split(\"_\")[0]\n",
    "        file_cont = []\n",
    "        for line in f:\n",
    "            file_cont.append(line)\n",
    "        arg_dict[paper_id] = file_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\thow ’s the score of the proposed model compared with the above paper as well as [ tang et al. 2016 ] ?\n",
      "\n",
      "request\t% % % % other points % % % % - tables 1 , 2 , and 3 are not worth spending more than a few lines on , let alone nearly a page .\n",
      "\n",
      "request\twhen restricting attention to neural network methods , it would be more correct to give credit to collobert et al. ( 2011 ) .\n",
      "\n",
      "request\tin fact , it would be interesting to study which level of these variables could be analytically collapsed ( such as done in the semi-supervised learning work by kingma et al 2014 ) and which ones can be sampled effectively using a form of reparametrization .\n",
      "\n",
      "request\ti would recommend the authors to use something more general , like graph convolutional neural networks ( kipf & welling , 2017 ) or graph gated neural networks ( li et al. , 2016 ) .\n",
      "\n",
      "request\talso , key experiments are missing : 1 ) nms baseline 2 ) comparison with vqa counting work ( chattopadhyay et al. , 2017 ) .\n",
      "\n",
      "request\t10 . make a pass through the paper , and decouple it from the van seijen et al. , 2017a\n",
      "\n",
      "request\tp7 : for which statement is ( kim et al. 2016 ) the reference ?\n",
      "\n",
      "request\tfor example , does one really need to use the gating mechanism borrowed from the dauphin et al. paper ?\n",
      "\n",
      "request\t4 . the paper should be compared with infogan ( chen et al. 2016 ) ,\n",
      "\n",
      "request\t- some questions about monotonic attention : did you use the training-time \" soft \" monotonic attention algorithm from raffel et al. during training and inference , or did you use the \" hard \" monotonic attention at inference time ?\n",
      "\n",
      "request\t* typos * recent surveys on graph embeddings have been published in 2017 and should be cited as \" a comprehensive survey of graph embedding ... \" by cai et al\n",
      "\n",
      "request\t( e.g. i would love to see results from the method by zintgraf et al. 2017 which unlike all included prior methods seems to produce much crisper visualizations and also is very related because it learns from the data , too ) .\n",
      "\n",
      "request\tit would be interesting to discuss how one would balance the advantages of choosing actions that improve localization with those in the context of a higher-level task ( or at least including a cost on actions as with the baseline method of fox et al. ) .\n",
      "\n",
      "request\tthe authors may want to be more explicit about their claim in the related work section that the running time of their attack is “ 40x ” less than that of chen et al .\n",
      "\n",
      "request\tit would be good to use reinforce to do a fair comparison with ( yu et al. , 2017 ) to see the benefit of using small rnn .\n",
      "\n",
      "request\ti would add spirtes et al 2000 , heckermann et al 1999 , peters et al 2016 , and chickering et al 2002 .\n",
      "\n",
      "request\t- the work “ efficient supervised learning in networks with binary synapses ” by baldassi et al. ( pnas 2007 ) should be cited .\n",
      "\n",
      "request\tthe authors should further consider contrasting their approach with “ neural slam ” by zhang et al .\n",
      "\n",
      "request\tcomparison to existing baselines : given that the paper addresses the stability problem , i would expect some empirical comparison to at least one or two of the stability methods cited in the introduction , e.g. gulrajani et al. , 2017 or roth et al. , 2017 .\n",
      "\n",
      "request\t- it would be interesting to replicate the analysis of danihelka et al. ( 2017 ) on the thin-8 dataset .\n",
      "\n",
      "request\tbut i think it would be much more forceful and clear if they would present van or villegas et al method separately and then put the pieces together for their method in a separate section .\n",
      "\n",
      "request\tthey should clearly list what the contributions are w.r.t to the work by vinyals et al 2016 .\n",
      "\n",
      "request\tone interesting source of information they could tap into for tf-tf interactions is cap-selex ( jolma et al , nature 2015 ) .\n",
      "\n",
      "request\t>> probably should cite sadeghi et al. and tobin et al. in regard to randomization , both of which precede james ' 17 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request_mentions = []\n",
    "count_requests = 0\n",
    "just_etal = 0\n",
    "\n",
    "for k in arg_dict:\n",
    "    for l in arg_dict[k]:\n",
    "        if l.startswith(\"request\"):\n",
    "            l = l.lower()\n",
    "            if l.find(\"et al\") > 0:\n",
    "                count_requests += 1\n",
    "                print(l)\n",
    "            request_mentions.append((k, l))\n",
    "        if l.find(\"et al\") > 0:\n",
    "            just_etal += 1\n",
    "            #print(l)\n",
    "        elif re.search('[^\\s]* \\(?\\s?[0-9][0-9][0-9][0-9]', l):\n",
    "            just_etal += 1\n",
    "            #print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare/comparison/compared\n",
    "contrast/consider_contrasting\n",
    "baseline\n",
    "result(s)\n",
    "analysis\n",
    "replicate\n",
    "add"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
