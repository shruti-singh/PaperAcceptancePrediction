{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../ICLR data/masterdata_unbalanced/off_rev_dict_2019.pkl', '../ICLR data/masterdata_unbalanced/off_rev_dict_2020.pkl', '../ICLR data/masterdata_unbalanced/off_rev_dict_2017.pkl', '../ICLR data/masterdata_unbalanced/off_rev_dict_2018.pkl']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"../ICLR data/masterdata_unbalanced/off*\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "papers_data = defaultdict(dict)\n",
    "off_rev_data = defaultdict(dict)\n",
    "\n",
    "for y in years:\n",
    "    papers_data[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))\n",
    "    off_rev_data[y] = pd.read_pickle(data_path + \"off_rev_dict_{}.pkl\".format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 911 1419 2213\n"
     ]
    }
   ],
   "source": [
    "print(len(papers_data[2017]), len(papers_data[2018]), len(papers_data[2019]), len(papers_data[2020]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 984 1565 2561\n"
     ]
    }
   ],
   "source": [
    "print(len(off_rev_data[2017]), len(off_rev_data[2018]), len(off_rev_data[2019]), len(off_rev_data[2020]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17028"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(x) for x in off_rev_data[2017].values()) + sum(len(x) for x in off_rev_data[2018].values()) + sum(len(x) for x in off_rev_data[2019].values()) + sum(len(x) for x in off_rev_data[2020].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19989"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(x[\"content\"][\"authorids\"]) for x in papers_data[2017].values()) + sum(len(x[\"content\"][\"authorids\"]) for x in papers_data[2018].values()) + sum(len(x[\"content\"][\"authorids\"]) for x in papers_data[2019].values()) + sum(len(x[\"content\"][\"authorids\"]) for x in papers_data[2020].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9715875223524737"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19989/(490 + 911 + 1419 + 2213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ByG4hz5le',\n",
       "  {'cdate': None,\n",
       "   'content': {'TL;DR': '',\n",
       "    'abstract': \"Previous models for video captioning often use the output from a specific layer of a Convolutional Neural Network (CNN) as video representations, preventing them from modeling rich, varying context-dependent semantics in video descriptions. In this paper, we propose a new approach to generating adaptive spatiotemporal representations  of videos for a captioning task.  For this purpose, novel attention mechanisms with spatiotemporal alignment is employed to adaptively and sequentially focus on different layers of CNN features (levels of feature ``abstraction''), as well as local spatiotemporal regions of the feature maps at each layer. Our approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT.  Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.\",\n",
       "    'authorids': ['yunchen.pu@duke.edu',\n",
       "     'renqiang@nec-labs.com',\n",
       "     'zhe.gan@duke.edu',\n",
       "     'lcarin@duke.edu'],\n",
       "    'authors': ['Yunchen Pu',\n",
       "     'Martin Renqiang Min',\n",
       "     'Zhe Gan',\n",
       "     'Lawrence Carin'],\n",
       "    'conflicts': ['duke.edu', 'nec-labs.com', 'virginia.edu'],\n",
       "    'keywords': ['Computer vision', 'Deep learning'],\n",
       "    'paperhash': 'pu|adaptive_feature_abstraction_for_translating_video_to_language',\n",
       "    'pdf': '/pdf/b3577d9d5856be9fae6325e96b947a046b416ec7.pdf',\n",
       "    'title': 'Adaptive Feature Abstraction for Translating Video to Language'},\n",
       "   'ddate': None,\n",
       "   'details': {'replyCount': 22},\n",
       "   'forum': 'ByG4hz5le',\n",
       "   'id': 'ByG4hz5le',\n",
       "   'invitation': 'ICLR.cc/2017/conference/-/submission',\n",
       "   'nonreaders': [],\n",
       "   'number': 182,\n",
       "   'original': None,\n",
       "   'readers': ['everyone'],\n",
       "   'referent': None,\n",
       "   'replyto': None,\n",
       "   'signatures': ['~Yunchen_Pu1'],\n",
       "   'tcdate': 1478269994296,\n",
       "   'tmdate': 1488741406460,\n",
       "   'writers': []}),\n",
       " ('SygGlIBcel',\n",
       "  {'cdate': None,\n",
       "   'content': {'TL;DR': '',\n",
       "    'abstract': 'This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.',\n",
       "    'authorids': ['labeau@limsi.fr', 'allauzen@limsi.fr'],\n",
       "    'authors': ['Matthieu Labeau', 'Alexandre Allauzen'],\n",
       "    'conflicts': ['limsi.fr'],\n",
       "    'keywords': ['Natural language processing', 'Deep learning'],\n",
       "    'paperhash': 'labeau|opening_the_vocabulary_of_neural_language_models_with_characterlevel_word_representations',\n",
       "    'pdf': '/pdf/7d82838c1d50616a7351a39970340912d2f1804c.pdf',\n",
       "    'title': 'Opening the vocabulary of  neural language models with character-level word representations'},\n",
       "   'ddate': None,\n",
       "   'details': {'replyCount': 20},\n",
       "   'forum': 'SygGlIBcel',\n",
       "   'id': 'SygGlIBcel',\n",
       "   'invitation': 'ICLR.cc/2017/conference/-/submission',\n",
       "   'nonreaders': [],\n",
       "   'number': 250,\n",
       "   'original': None,\n",
       "   'readers': ['everyone'],\n",
       "   'referent': None,\n",
       "   'replyto': None,\n",
       "   'signatures': ['~Matthieu_Labeau1'],\n",
       "   'tcdate': 1478280682172,\n",
       "   'tmdate': 1481503393441,\n",
       "   'writers': []})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(papers_data[2017].items())[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1809"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(x[\"content\"][\"authors\"]) for x in papers_data[2017].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425 1809\n"
     ]
    }
   ],
   "source": [
    "a_17 = defaultdict(int)\n",
    "for x in papers_data[2017].values():\n",
    "    for auth in x[\"content\"][\"authors\"]:\n",
    "        a_17[auth] += 1\n",
    "print(len(a_17), sum(list(a_17.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3488"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(x[\"content\"][\"authors\"]) for x in papers_data[2018].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2762 3488\n"
     ]
    }
   ],
   "source": [
    "a_18 = defaultdict(int)\n",
    "for x in papers_data[2018].values():\n",
    "    for auth in x[\"content\"][\"authors\"]:\n",
    "        a_18[auth] += 1\n",
    "print(len(a_18), sum(list(a_18.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5655"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(x[\"content\"][\"authors\"]) for x in papers_data[2019].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4367 5655\n"
     ]
    }
   ],
   "source": [
    "a_19 = defaultdict(int)\n",
    "for x in papers_data[2019].values():\n",
    "    for auth in x[\"content\"][\"authors\"]:\n",
    "        a_19[auth] += 1\n",
    "print(len(a_19), sum(list(a_19.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(x[\"content\"][\"authors\"]) for x in papers_data[2020].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866 9155\n"
     ]
    }
   ],
   "source": [
    "a_20 = defaultdict(int)\n",
    "for x in papers_data[2020].values():\n",
    "    for auth in x[\"content\"][\"authors\"]:\n",
    "        a_20[auth] += 1\n",
    "print(len(a_20), sum(list(a_20.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11692\n"
     ]
    }
   ],
   "source": [
    "all_authors = defaultdict(int)\n",
    "\n",
    "for y in [a_17, a_18, a_19, a_20]:\n",
    "    for auth in y:\n",
    "        all_authors[auth] += y[auth]\n",
    "\n",
    "print(len(all_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
