{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obMjO_OEV9v-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scholarly\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "import ast\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xmlkh0WcWEs6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘author_name_index’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "mkdir author_name_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mauthor_name_index\u001b[0m/  create_name_index.ipynb  \u001b[01;34mindividual_year_filies\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create empty alphabetical index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dict = defaultdict(list)\n",
    "\n",
    "for i in range(97, 123):\n",
    "    with open(\"author_name_index/\" + chr(i) + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(init_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.pkl  d.pkl  g.pkl  j.pkl  m.pkl  p.pkl  s.pkl  v.pkl  y.pkl\r\n",
      "b.pkl  e.pkl  h.pkl  k.pkl  n.pkl  q.pkl  t.pkl  w.pkl  z.pkl\r\n",
      "c.pkl  f.pkl  i.pkl  l.pkl  o.pkl  r.pkl  u.pkl  x.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls author_name_index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read individual files and add to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buffer = {chr(x): defaultdict(list) for x in range(97, 123)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"individual_year_filies/2017_authors_gscholar_scrapped.pickle\", \"rb\") as f:\n",
    "    temp_auths_2017 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', []),\n",
       " ('Donghui Hu',\n",
       "  [(<scholarly.scholarly.Author at 0x7f23d5d1f7b8>,\n",
       "    <scholarly.scholarly.Author at 0x7f23d5d1f7b8>)]),\n",
       " ('Dawei Li',\n",
       "  [(<scholarly.scholarly.Author at 0x7f23e95be358>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e95be358>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e95bef28>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e95bef28>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e95de2e8>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e95de2e8>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e9570a20>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e9570a20>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e957a7f0>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e957a7f0>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e957acc0>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e957acc0>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e9586128>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e9586128>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e95864e0>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e95864e0>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e9586f60>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e9586f60>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e958e588>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e958e588>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e958edd8>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e958edd8>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e9597978>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e9597978>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e95a2198>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e95a2198>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e95a2860>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e95a2860>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e95a2be0>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e95a2be0>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e952c198>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e952c198>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e952c390>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e952c390>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e952c400>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e952c400>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23e952c588>,\n",
       "    <scholarly.scholarly.Author at 0x7f23e952c588>)])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(temp_auths_2017.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []\n"
     ]
    }
   ],
   "source": [
    "for auth in temp_auths_2017:\n",
    "    \n",
    "    if auth:    \n",
    "        rm_ast = auth.replace(\"*\", \"\") \n",
    "        unaacented_auth = unidecode.unidecode(rm_ast)\n",
    "        clean_auth = unaacented_auth.lower()\n",
    "        \n",
    "        key = clean_auth[0]\n",
    "        \n",
    "        if ord(key) <= 122 and ord(key) >=97:\n",
    "            temp_buffer[key][clean_auth] += temp_auths_2017[auth]\n",
    "            \n",
    "            if len(temp_buffer[key]) > 1000:\n",
    "                read_to_update = None\n",
    "                with open(\"author_name_index/\" + key + \".pkl\", \"rb\") as f:\n",
    "                    read_to_update = pickle.load(f)\n",
    "                for indi_auth in temp_buffer[key]:\n",
    "                    read_to_update[indi_auth] += temp_buffer[key][indi_auth]\n",
    "                with open(\"author_name_index/\" + key + \".pkl\", \"wb\") as f:\n",
    "                    pickle.dump(read_to_update, f)\n",
    "                temp_buffer[key] = defaultdict(list)\n",
    "        else:\n",
    "            print(\"Non alpha key: \", auth, clean_auth)\n",
    "    else:\n",
    "        print(auth, temp_auths_2017[auth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394\n"
     ]
    }
   ],
   "source": [
    "print(sum(len(temp_buffer[x]) for x in temp_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in temp_buffer:\n",
    "    read_to_update = None\n",
    "    \n",
    "    with open(\"author_name_index/\" + key + \".pkl\", \"rb\") as f:\n",
    "        read_to_update = pickle.load(f)\n",
    "    \n",
    "    for indi_auth in temp_buffer[key]:\n",
    "        read_to_update[indi_auth] += temp_buffer[key][indi_auth]\n",
    "    \n",
    "    with open(\"author_name_index/\" + key + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(read_to_update, f)\n",
    "    \n",
    "    temp_buffer[key] = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RAM\n",
    "# temp_buffer also empty by now\n",
    "temp_auths_2017 = None\n",
    "read_to_update = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"individual_year_filies/2018_pickle_file/author_info_scholar_2018.pickle\", \"rb\") as f:\n",
    "    temp_auths_2018 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Min-Hwan Oh',\n",
       "  [(<scholarly.scholarly.Author at 0x7f23eae05ba8>,\n",
       "    <scholarly.scholarly.Author at 0x7f23eae05ba8>)]),\n",
       " ('Sina Tootoonian', []),\n",
       " ('James Hays',\n",
       "  [(<scholarly.scholarly.Author at 0x7f23da975fd0>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da975fd0>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23da987908>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da987908>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23da9991d0>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da9991d0>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23da93e908>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da93e908>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23da945438>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da945438>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23da94fb00>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da94fb00>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23da957240>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da957240>),\n",
       "   (<scholarly.scholarly.Author at 0x7f23da957b00>,\n",
       "    <scholarly.scholarly.Author at 0x7f23da957b00>)])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(temp_auths_2018.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non alpha key:  \u0006 Daniel Crawford \u0006 daniel crawford\n"
     ]
    }
   ],
   "source": [
    "for auth in temp_auths_2018:\n",
    "    \n",
    "    if auth:    \n",
    "        rm_ast = auth.replace(\"*\", \"\") \n",
    "        unaacented_auth = unidecode.unidecode(rm_ast)\n",
    "        clean_auth = unaacented_auth.lower()\n",
    "        \n",
    "        key = clean_auth[0]\n",
    "        \n",
    "        if ord(key) <= 122 and ord(key) >=97:\n",
    "            temp_buffer[key][clean_auth] += temp_auths_2018[auth]\n",
    "            \n",
    "            if len(temp_buffer[key]) > 1000:\n",
    "                read_to_update = None\n",
    "                with open(\"author_name_index/\" + key + \".pkl\", \"rb\") as f:\n",
    "                    read_to_update = pickle.load(f)\n",
    "                for indi_auth in temp_buffer[key]:\n",
    "                    read_to_update[indi_auth] += temp_buffer[key][indi_auth]\n",
    "                with open(\"author_name_index/\" + key + \".pkl\", \"wb\") as f:\n",
    "                    pickle.dump(read_to_update, f)\n",
    "                temp_buffer[key] = defaultdict(list)\n",
    "        else:\n",
    "            print(\"Non alpha key: \", auth, clean_auth)\n",
    "    else:\n",
    "        print(auth, temp_auths_2018[auth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048\n"
     ]
    }
   ],
   "source": [
    "print(sum(len(temp_buffer[x]) for x in temp_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for auth in temp_auths_2018:\n",
    "    if auth.endswith(\"Crawford\"):\n",
    "        inspect_auth = auth\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x06 Daniel Crawford'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t', 'q', 'd', 'o', 'j', 'u', 'g', 'w', 'x', 'e', 'n', 'f', 'y', 'l', 'b', 's', 'i', 'h', 'z', 'r', 'p', 'c', 'k', 'v', 'm', 'a'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_buffer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buffer[\"d\"][\"daniel crawford\"] += temp_auths_2018[inspect_auth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<scholarly.scholarly.Author at 0x7f23db0f4710>,\n",
       "  <scholarly.scholarly.Author at 0x7f23db0f4710>),\n",
       " (<scholarly.scholarly.Author at 0x7f23db10e4e0>,\n",
       "  <scholarly.scholarly.Author at 0x7f23db10e4e0>),\n",
       " (<scholarly.scholarly.Author at 0x7f23db10eb00>,\n",
       "  <scholarly.scholarly.Author at 0x7f23db10eb00>),\n",
       " (<scholarly.scholarly.Author at 0x7f23db1180b8>,\n",
       "  <scholarly.scholarly.Author at 0x7f23db1180b8>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_buffer[\"d\"][\"daniel crawford\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in temp_buffer:\n",
    "    read_to_update = None\n",
    "    \n",
    "    with open(\"author_name_index/\" + key + \".pkl\", \"rb\") as f:\n",
    "        read_to_update = pickle.load(f)\n",
    "    \n",
    "    for indi_auth in temp_buffer[key]:\n",
    "        read_to_update[indi_auth] += temp_buffer[key][indi_auth]\n",
    "    \n",
    "    with open(\"author_name_index/\" + key + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(read_to_update, f)\n",
    "    \n",
    "    temp_buffer[key] = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RAM\n",
    "# temp_buffer also empty by now\n",
    "temp_auths_2018 = None\n",
    "read_to_update = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./individual_year_filies/2020_multiple_files/Copy of PARTIAL_author_info_scholar_2020_3.pickle_03', './individual_year_filies/2020_multiple_files/Copy of PARTIAL_author_info_scholar_2020_0.pickle', './individual_year_filies/2020_multiple_files/Copy of PARTIAL_author_info_scholar_2020_2.pickle', './individual_year_filies/2020_multiple_files/Copy of PARTIAL_author_info_scholar_2020_3.pickle', './individual_year_filies/2020_multiple_files/Copy of PARTIAL_author_info_scholar_2020_3.pickle_02', './individual_year_filies/2020_multiple_files/Copy of PARTIAL_author_info_scholar_2020_1.pickle']\n"
     ]
    }
   ],
   "source": [
    "# Collate all 2020 files as it looks like there is some duplicacy\n",
    "\n",
    "files_2020 = glob.glob(\"./individual_year_filies/2020_multiple_files/Copy of PARTIAL*\")\n",
    "print(files_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_auths_2020 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy of PARTIAL_author_info_scholar_2020_3.pickle_03\n",
      "Copy of PARTIAL_author_info_scholar_2020_0.pickle\n",
      "Copy of PARTIAL_author_info_scholar_2020_2.pickle\n",
      "Copy of PARTIAL_author_info_scholar_2020_3.pickle\n",
      "Copy of PARTIAL_author_info_scholar_2020_3.pickle_02\n",
      "Copy of PARTIAL_author_info_scholar_2020_1.pickle\n"
     ]
    }
   ],
   "source": [
    "for f in files_2020:\n",
    "    print(f.rsplit(\"/\", 1)[1])\n",
    "    \n",
    "    with open(f, \"rb\") as temp_2020_fc:\n",
    "        temp_auth_2020_sub_data = pickle.load(temp_2020_fc)\n",
    "        \n",
    "    temp_auth_2020_sub_data = ast.literal_eval(temp_auth_2020_sub_data)\n",
    "    \n",
    "    for auth in temp_auth_2020_sub_data:\n",
    "        if auth:    \n",
    "            rm_ast = auth.replace(\"*\", \"\") \n",
    "            unaacented_auth = unidecode.unidecode(rm_ast)\n",
    "            clean_auth = unaacented_auth.lower()\n",
    "\n",
    "            key = clean_auth[0]\n",
    "\n",
    "            if ord(key) <= 122 and ord(key) >=97:\n",
    "                temp_buffer[key][clean_auth] += temp_auth_2020_sub_data[auth]\n",
    "\n",
    "                if len(temp_buffer[key]) > 5:\n",
    "                    read_to_update = None\n",
    "                    with open(\"author_name_index/\" + key + \".pkl\", \"rb\") as f:\n",
    "                        read_to_update = pickle.load(f)\n",
    "                    for indi_auth in temp_buffer[key]:\n",
    "                        read_to_update[indi_auth] += temp_buffer[key][indi_auth]\n",
    "                    with open(\"author_name_index/\" + key + \".pkl\", \"wb\") as f:\n",
    "                        pickle.dump(read_to_update, f)\n",
    "                    temp_buffer[key] = defaultdict(list)\n",
    "            else:\n",
    "                print(\"Non alpha key: \", auth, clean_auth)\n",
    "        else:\n",
    "            print(auth, temp_auth_2020_sub_data[auth])\n",
    "    \n",
    "    temp_auth_2020_sub_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in temp_buffer:\n",
    "    read_to_update = None\n",
    "    \n",
    "    with open(\"author_name_index/\" + key + \".pkl\", \"rb\") as f:\n",
    "        read_to_update = pickle.load(f)\n",
    "    \n",
    "    for indi_auth in temp_buffer[key]:\n",
    "        read_to_update[indi_auth] += temp_buffer[key][indi_auth]\n",
    "    \n",
    "    with open(\"author_name_index/\" + key + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(read_to_update, f)\n",
    "    \n",
    "    temp_buffer[key] = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_to_update = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jk9BaHNMxSC2"
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle('../../features/all_data_features_17_20.pkl')\n",
    "# data_raw.head()\n",
    "data = data_raw[[\"id\", \"title\", \"label\", \"authors\"]]\n",
    "org_papers = data.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g62mC5Nix-vB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkjGJ0BJx_t6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPONEb_Px_wM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXUIhrGIyBXj"
   },
   "source": [
    "Collate all the multiple data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHtEBX5Rx_0o"
   },
   "outputs": [],
   "source": [
    "all_authors_gs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "s-erGoyjzdQc",
    "outputId": "f3091e3e-fc9c-4ad1-f04f-11cec3f0286f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ind len:  377\n",
      "Total length:  377\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    with open(f, \"rb\") as fff:\n",
    "        temp_auths = pickle.load(fff)\n",
    "    temp_auths = ast.literal_eval(temp_auths)\n",
    "    print(\"Ind len: \", len(temp_auths))\n",
    "    all_authors_gs.update(temp_auths)\n",
    "    print(\"Total length: \", len(all_authors_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y97sfG9q0Ccc"
   },
   "outputs": [],
   "source": [
    "papers_data = pd.read_pickle(\"../papers_2020.pkl\".format(y))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "rerun_and_collate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
