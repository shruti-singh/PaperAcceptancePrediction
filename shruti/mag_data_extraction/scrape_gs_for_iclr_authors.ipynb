{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scholarly\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change this to the year you are \n",
    "global_year = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle('../features/all_data_features_17_20.pkl')\n",
    "# data_raw.head()\n",
    "data = data_raw[[\"id\", \"title\", \"label\", \"authors\"]]\n",
    "org_papers = data.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segregate authors based on year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearwise_authors = {y: set() for y in range(2017, 2021)}\n",
    "\n",
    "for p in org_papers:\n",
    "    year = int(p.split(\"_\")[0])\n",
    "    for auth in org_papers[p][\"authors\"]:\n",
    "        yearwise_authors[year].add(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape GS author info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_info = {}\n",
    "count = len(author_info)\n",
    "\n",
    "p = re.compile(\" [A-Z]\\.? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in yearwise_authors[global_year]:\n",
    "    if not a in author_info:\n",
    "        if count % 10 == 0:\n",
    "            print(\"Status {} / {}\".format(count, len(all_authors)))\n",
    "        auth_candidates = []\n",
    "        try:\n",
    "            c = scholarly.search_author(a)\n",
    "            for i in c:\n",
    "                auth_candidates.append((i, i.fill()))\n",
    "\n",
    "            if not auth_candidates:\n",
    "                # no author found\n",
    "                a_clean = p.sub(\" \", a)\n",
    "                c = scholarly.search_author(a_clean)\n",
    "                for i in c:\n",
    "                    auth_candidates.append((i, i.fill()))\n",
    "\n",
    "            author_info[a] = auth_candidates\n",
    "        except Exception as ex:\n",
    "            print(\"Exception: \", a)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"author_info_scholar.pickle\", \"wb\") as f:\n",
    "    pickle.dump(author_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disambiguate authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read gs scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"author_info_scholar.pickle\", \"rb\") as f:\n",
    "    all_authors_gs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read conflicts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [global_year]\n",
    "\n",
    "for y in years:\n",
    "    papers_data = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflicts_dict = {}\n",
    "emails_dict = {}\n",
    "\n",
    "for p in papers_data:\n",
    "    conflicts_dict[p] = papers_data[p][\"content\"][\"conflicts\"]\n",
    "    emails_dict[p] = papers_data[p][\"content\"][\"authorids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_authors_info = defaultdict(dict)\n",
    "per_paper = defaultdict(list)\n",
    "\n",
    "still_not_found = 0\n",
    "total_auth_count = 0\n",
    "found_count = 0\n",
    "not_in_gs = 0\n",
    "cc = 0\n",
    "\n",
    "still_not_found_list = []\n",
    "\n",
    "for p in org_papers:\n",
    "    for auth in org_papers[p][\"authors\"]:\n",
    "        total_auth_count += 1\n",
    "        disambiguated_author = None\n",
    "        \n",
    "        if auth in all_authors_gs:\n",
    "            if len(all_authors_gs[auth]) == 1:\n",
    "                disambiguated_author = all_authors_gs[auth][0][0]\n",
    "                found_count += 1\n",
    "            else:\n",
    "                found_paper = False\n",
    "                for mauth in all_authors_gs[auth]:\n",
    "                    for pub in mauth[0].publications:\n",
    "                        org_title = ''.join(filter(str.isalpha, org_papers[p][\"title\"].lower()))\n",
    "                        gs_pub_title = ''.join(filter(str.isalpha, pub.bib[\"title\"].lower()))\n",
    "                        if org_title == gs_pub_title:\n",
    "                            found_paper = True\n",
    "                            disambiguated_author = mauth[0]\n",
    "                            found_count += 1\n",
    "                            break\n",
    "                    if found_paper:\n",
    "                        break\n",
    "                if not found_paper:\n",
    "                    # Paper not found, check via conflicts/email information\n",
    "                    matching_affils = []\n",
    "                    \n",
    "                    for conflict_affil in conflicts_dict[p]:\n",
    "                        for mauth in all_authors_gs[auth]:\n",
    "                            if mauth[0].email.find(conflict_affil) > -1:\n",
    "                                matching_affils.append(mauth)\n",
    "                    \n",
    "                    # still not found\n",
    "                    if len(matching_affils) == 0:\n",
    "                        still_not_found += 1\n",
    "                        still_not_found_list.append(auth)\n",
    "                    elif len(matching_affils) == 1:\n",
    "                        disambiguated_author = matching_affils[0][0]\n",
    "                        found_count += 1\n",
    "                    else:\n",
    "                        potential_emailids = []\n",
    "                        auth_split = auth.split(\" \")\n",
    "                        for splitname in auth_split:\n",
    "                            for authemailid in emails_dict[p]:\n",
    "                                if authemailid.find(splitname) > -1:\n",
    "                                    potential_emailids.append(authemailid)\n",
    "                                    \n",
    "                        potential_emailids = list(set(potential_emailids))\n",
    "                        if len(potential_emailids) == 1:\n",
    "                            domain = potential_emailids[0].split(\"@\")[-1]\n",
    "                            identified_auth = []\n",
    "                            for m in matching_affils:\n",
    "                                if m[0].email.find(domain) > -1:\n",
    "                                    identified_auth.append(m[0])\n",
    "                            \n",
    "                            if len(identified_auth) == 1:\n",
    "                                disambiguated_author = m[0]\n",
    "                                found_count +=1 \n",
    "                            else:\n",
    "                                still_not_found += 1\n",
    "                                still_not_found_list.append(auth)\n",
    "                        else:\n",
    "                            still_not_found += 1\n",
    "                            still_not_found_list.append(auth)\n",
    "            if not disambiguated_author is None:\n",
    "                cc +=1\n",
    "                paper_authors_info[auth] = disambiguated_author\n",
    "                per_paper[p].append(auth)\n",
    "        else:\n",
    "            not_in_gs +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found authors: \", found_count)\n",
    "print(\"Added to dict: \", cc)\n",
    "print(\"Multiple entries in GS but no publication with the same title found or empty GS: \", still_not_found)\n",
    "print(\"NOt in GS data: \", not_in_gs)\n",
    "print(\"Total authors to be disambiguated: \", total_auth_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save citation count, publication count, and hindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in org_papers.items():\n",
    "    org_papers[k][\"pub_count_gs\"] = []\n",
    "    org_papers[k][\"cit_count_gs\"] = []\n",
    "    org_papers[k][\"hindex\"] = []\n",
    "    \n",
    "    for a in v[\"authors\"]:\n",
    "        if a in paper_authors_info:\n",
    "            temp_auth_dict = paper_authors_info[a].__dict__\n",
    "            if \"citedby\" in temp_auth_dict and temp_auth_dict[\"citedby\"] > 0:\n",
    "                total_citations = temp_auth_dict[\"citedby\"]\n",
    "                if \"cites_per_year\" in temp_auth_dict:\n",
    "                    for y in temp_auth_dict[\"cites_per_year\"]:\n",
    "                        if y > (global_year-1):\n",
    "                            total_citations -= temp_auth_dict[\"cites_per_year\"][y]\n",
    "                org_papers[k][\"cit_count_gs\"].append(total_citations)\n",
    "            if \"hindex\" in temp_auth_dict:\n",
    "                if \"hindex5y\" in temp_auth_dict:\n",
    "                    org_papers[k][\"hindex\"].append((temp_auth_dict[\"hindex5y\"] + temp_auth_dict[\"hindex\"])/2)\n",
    "                else:\n",
    "                    org_papers[k][\"hindex\"].append(temp_auth_dict[\"hindex\"])\n",
    "            if \"publications\" in temp_auth_dict:\n",
    "                total_publications = 0\n",
    "                for pub in temp_auth_dict[\"publications\"]:\n",
    "                    if \"year\" in pub.__dict__[\"bib\"]:\n",
    "                        if pub.__dict__[\"bib\"][\"year\"] < global_year:\n",
    "                            total_publications += 1\n",
    "                    else:\n",
    "                        total_publications += 1\n",
    "                org_papers[k][\"pub_count_gs\"].append(total_publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
