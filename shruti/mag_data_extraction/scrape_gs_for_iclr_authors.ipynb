{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scholarly\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change this to the year you are \n",
    "global_year = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle('../features/all_data_features_17_20.pkl')\n",
    "# data_raw.head()\n",
    "data = data_raw[[\"id\", \"title\", \"label\", \"authors\"]]\n",
    "org_papers = data.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segregate authors based on year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearwise_authors = {y: set() for y in range(2017, 2021)}\n",
    "\n",
    "for p in org_papers:\n",
    "    year = int(p.split(\"_\")[0])\n",
    "    yearwise_authors[year].add(org_papers[p][\"authors\"][-1])\n",
    "#     yearwise_authors[year].update(set(org_papers[p][\"authors\"][-2:]))\n",
    "#     for auth in org_papers[p][\"authors\"]:\n",
    "#         yearwise_authors[year].add(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eric P. Xing'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_papers[p][\"authors\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yearwise_authors[global_year])#, yearwise_authors[global_year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape GS author info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONLY THE FIRST TIME\n",
    "author_info = {}\n",
    "err_authors = []\n",
    "\n",
    "with open(\"author_info_scholar_2018.pickle\", \"wb\") as f:\n",
    "    pickle.dump(author_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\" [A-Z]\\.? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTART FROM HERE AFTER COLAB DISCONNECTS\n",
    "\n",
    "with open(\"author_info_scholar_2018.pickle\", \"rb\") as f:\n",
    "    author_info = pickle.load(f)\n",
    "\n",
    "count = len(author_info)\n",
    "done_authors = list(author_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518, 10, 649)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, len(err_authors), len(yearwise_authors[global_year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n"
     ]
    }
   ],
   "source": [
    "with open(\"author_info_scholar_2018.pickle\", \"rb\") as f:\n",
    "    old_author_info = pickle.load(f)\n",
    "old_author_info.update(author_info)\n",
    "with open(\"author_info_scholar_2018.pickle\", \"wb\") as f:\n",
    "    pickle.dump(old_author_info, f)\n",
    "\n",
    "print(len(old_author_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yearwise_authors[global_year].difference(set(list(old_author_info.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Yu\n",
      "Cane Punma\n",
      "Exception:  Cane Punma\n",
      "Javad Sadri\n",
      "David Z. Pan\n",
      "Alex Nevidomsky\n",
      "Exception:  Alex Nevidomsky\n",
      "David Tse\n",
      "Ioannis Mitliagkas\n",
      "and Jan Kleindienst\n",
      "Exception:  and Jan Kleindienst\n",
      "Status 700 / 649\n",
      "Kevin Murphy\n",
      "Frank Hutter\n",
      "Daniela Rus\n",
      "Hai Li\n",
      "Rujie Liu\n",
      "Kilian Weinberger\n",
      "Balaraman Ravindran\n",
      "Yutaka Matsuo\n",
      "Sumit Gulwani\n",
      "Joonseok Lee\n",
      "Status 710 / 649\n",
      "Franz Pernkopf\n",
      "Yangang Zhang\n",
      "Exception:  Yangang Zhang\n",
      "Jian Zhang\n",
      "Ryuki Tachibana\n",
      "Sameer Singh\n",
      "Gerald Tesauro\n",
      "Fred Bertsch\n",
      "Yu Qian\n",
      "Said Ladjal\n",
      "Yoshua Bengio\n",
      "Status 720 / 649\n",
      "Virginia R. de Sa\n",
      "Exception:  Virginia R. de Sa\n",
      "Avinash Balakrishnan\n",
      "Nick Johnston\n",
      "Weizhu Chen\n",
      "Martine De Cock\n",
      "Zhouyuan Li\n",
      "Xue-Xin Wei\n",
      "Laura Balzano\n",
      "Mehran Mesbahi\n",
      "Peter I. Frazier\n",
      "Status 730 / 649\n",
      "Aaron Courville\n",
      "Jaakko Lehtinen\n",
      "Soummya Kar\n",
      "Exception:  Soummya Kar\n",
      "Sumit Baburao Tamgale\n",
      "Exception:  Sumit Baburao Tamgale\n",
      "Honglak Lee\n",
      "Teresa Ludermir\n"
     ]
    }
   ],
   "source": [
    "for a in yearwise_authors[global_year]:\n",
    "    \n",
    "    if not a in done_authors and not a in err_authors:\n",
    "        \n",
    "        if count % 10 == 0:\n",
    "            print(\"Status {} / {}\".format(count, len(yearwise_authors[global_year])))\n",
    "            with open(\"author_info_scholar_2018.pickle\", \"rb\") as f:\n",
    "                old_author_info = pickle.load(f)\n",
    "            old_author_info.update(author_info)\n",
    "            with open(\"author_info_scholar_2018.pickle\", \"wb\") as f:\n",
    "                pickle.dump(old_author_info, f)\n",
    "            author_info = {}\n",
    "            old_author_info = {}\n",
    "            \n",
    "        auth_candidates = []\n",
    "        try:\n",
    "            print(a)\n",
    "            c = scholarly.search_author(a)\n",
    "            fill_count = 0\n",
    "            for i in c:\n",
    "                if fill_count > 6:\n",
    "                    break\n",
    "                auth_candidates.append((i, i.fill()))\n",
    "                fill_count += 1\n",
    "\n",
    "            if not auth_candidates:\n",
    "                # no author found\n",
    "                a_clean = p.sub(\" \", a)\n",
    "                c = scholarly.search_author(a_clean)\n",
    "                fill_count = 0\n",
    "                for i in c:\n",
    "                    if fill_count > 6:\n",
    "                        break\n",
    "                    auth_candidates.append((i, i.fill()))\n",
    "                    fill_count += 1\n",
    "\n",
    "            author_info[a] = auth_candidates\n",
    "            done_authors.append(a)\n",
    "        except Exception as ex:\n",
    "            print(\"Exception: \", a)\n",
    "            err_authors.append(a)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yearwise_authors[2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"author_info_scholar_2018.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(author_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp author_info_scholar_2018.pickle bkp_author_info_scholar_2018.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disambiguate authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read gs scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"author_info_scholar_2018.pickle\", \"rb\") as f:\n",
    "    all_authors_gs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read conflicts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICLR data-20200322T071325Z-001.zip\n",
      "off_rev_dict_2017.pkl\n",
      "off_rev_dict_2018.pkl\n",
      "off_rev_dict_2019.pkl\n",
      "off_rev_dict_2020.pkl\n",
      "paper_decision_dict_2017.pkl\n",
      "paper_decision_dict_2018.pkl\n",
      "paper_decision_dict_2019.pkl\n",
      "paper_decision_dict_2020.pkl\n",
      "papers_2017.pkl\n",
      "papers_2018.pkl\n",
      "papers_2019.pkl\n",
      "papers_2020.pkl\n",
      "readme\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls ../../ICLR\\ data/masterdata_unbalanced/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2019]\n",
    "\n",
    "for y in years:\n",
    "    papers_data = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conflicts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-46a3574748ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpapers_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mconflicts_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conflicts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0memails_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"authorids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conflicts'"
     ]
    }
   ],
   "source": [
    "conflicts_dict = {}\n",
    "emails_dict = {}\n",
    "\n",
    "for p in papers_data:\n",
    "    conflicts_dict[p] = papers_data[p][\"content\"][\"conflicts\"]\n",
    "    emails_dict[p] = papers_data[p][\"content\"][\"authorids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TL;DR': 'We provide improved upper bounds for the number of linear regions used in network expressivity, and an highly efficient algorithm (w.r.t. exact counting) to obtain probabilistic lower bounds on the actual number of linear regions.',\n",
       " '_bibtex': '@misc{\\nserra2019empirical,\\ntitle={Empirical Bounds on Linear Regions of Deep Rectifier Networks},\\nauthor={Thiago Serra and Srikumar Ramalingam},\\nyear={2019},\\nurl={https://openreview.net/forum?id=B1MAJhR5YX},\\n}',\n",
       " 'abstract': 'One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled. We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure. However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks. In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets. In addition, we present a tighter upper bound that leverages network coefficients. We test both on trained networks. The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks. The refined upper bound is particularly stronger on networks with narrow layers.  ',\n",
       " 'authorids': ['tserra@gmail.com', 'srikumar.ramalingam@gmail.com'],\n",
       " 'authors': ['Thiago Serra', 'Srikumar Ramalingam'],\n",
       " 'keywords': ['linear regions',\n",
       "  'approximate model counting',\n",
       "  'mixed-integer linear programming'],\n",
       " 'paperhash': 'serra|empirical_bounds_on_linear_regions_of_deep_rectifier_networks',\n",
       " 'pdf': '/pdf/2cd4fcee595e00b15e892dc060154b0fe7e231e1.pdf',\n",
       " 'title': 'Empirical Bounds on Linear Regions of Deep Rectifier Networks'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data[p][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_authors_info = defaultdict(dict)\n",
    "per_paper = defaultdict(list)\n",
    "\n",
    "still_not_found = 0\n",
    "total_auth_count = 0\n",
    "found_count = 0\n",
    "not_in_gs = 0\n",
    "cc = 0\n",
    "\n",
    "still_not_found_list = []\n",
    "\n",
    "for p in org_papers:\n",
    "    for auth in org_papers[p][\"authors\"]:\n",
    "        total_auth_count += 1\n",
    "        disambiguated_author = None\n",
    "        \n",
    "        if auth in all_authors_gs:\n",
    "            if len(all_authors_gs[auth]) == 1:\n",
    "                disambiguated_author = all_authors_gs[auth][0][0]\n",
    "                found_count += 1\n",
    "            else:\n",
    "                found_paper = False\n",
    "                for mauth in all_authors_gs[auth]:\n",
    "                    for pub in mauth[0].publications:\n",
    "                        org_title = ''.join(filter(str.isalpha, org_papers[p][\"title\"].lower()))\n",
    "                        gs_pub_title = ''.join(filter(str.isalpha, pub.bib[\"title\"].lower()))\n",
    "                        if org_title == gs_pub_title:\n",
    "                            found_paper = True\n",
    "                            disambiguated_author = mauth[0]\n",
    "                            found_count += 1\n",
    "                            break\n",
    "                    if found_paper:\n",
    "                        break\n",
    "                if not found_paper:\n",
    "                    # Paper not found, check via conflicts/email information\n",
    "                    matching_affils = []\n",
    "                    \n",
    "                    for conflict_affil in conflicts_dict[p]:\n",
    "                        for mauth in all_authors_gs[auth]:\n",
    "                            if mauth[0].email.find(conflict_affil) > -1:\n",
    "                                matching_affils.append(mauth)\n",
    "                    \n",
    "                    # still not found\n",
    "                    if len(matching_affils) == 0:\n",
    "                        still_not_found += 1\n",
    "                        still_not_found_list.append(auth)\n",
    "                    elif len(matching_affils) == 1:\n",
    "                        disambiguated_author = matching_affils[0][0]\n",
    "                        found_count += 1\n",
    "                    else:\n",
    "                        potential_emailids = []\n",
    "                        auth_split = auth.split(\" \")\n",
    "                        for splitname in auth_split:\n",
    "                            for authemailid in emails_dict[p]:\n",
    "                                if authemailid.find(splitname) > -1:\n",
    "                                    potential_emailids.append(authemailid)\n",
    "                                    \n",
    "                        potential_emailids = list(set(potential_emailids))\n",
    "                        if len(potential_emailids) == 1:\n",
    "                            domain = potential_emailids[0].split(\"@\")[-1]\n",
    "                            identified_auth = []\n",
    "                            for m in matching_affils:\n",
    "                                if m[0].email.find(domain) > -1:\n",
    "                                    identified_auth.append(m[0])\n",
    "                            \n",
    "                            if len(identified_auth) == 1:\n",
    "                                disambiguated_author = m[0]\n",
    "                                found_count +=1 \n",
    "                            else:\n",
    "                                still_not_found += 1\n",
    "                                still_not_found_list.append(auth)\n",
    "                        else:\n",
    "                            still_not_found += 1\n",
    "                            still_not_found_list.append(auth)\n",
    "            if not disambiguated_author is None:\n",
    "                cc +=1\n",
    "                paper_authors_info[auth] = disambiguated_author\n",
    "                per_paper[p].append(auth)\n",
    "        else:\n",
    "            not_in_gs +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found authors: \", found_count)\n",
    "print(\"Added to dict: \", cc)\n",
    "print(\"Multiple entries in GS but no publication with the same title found or empty GS: \", still_not_found)\n",
    "print(\"NOt in GS data: \", not_in_gs)\n",
    "print(\"Total authors to be disambiguated: \", total_auth_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save citation count, publication count, and hindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in org_papers.items():\n",
    "    org_papers[k][\"pub_count_gs\"] = []\n",
    "    org_papers[k][\"cit_count_gs\"] = []\n",
    "    org_papers[k][\"hindex\"] = []\n",
    "    \n",
    "    for a in v[\"authors\"]:\n",
    "        if a in paper_authors_info:\n",
    "            temp_auth_dict = paper_authors_info[a].__dict__\n",
    "            if \"citedby\" in temp_auth_dict and temp_auth_dict[\"citedby\"] > 0:\n",
    "                total_citations = temp_auth_dict[\"citedby\"]\n",
    "                if \"cites_per_year\" in temp_auth_dict:\n",
    "                    for y in temp_auth_dict[\"cites_per_year\"]:\n",
    "                        if y > (global_year-1):\n",
    "                            total_citations -= temp_auth_dict[\"cites_per_year\"][y]\n",
    "                org_papers[k][\"cit_count_gs\"].append(total_citations)\n",
    "            if \"hindex\" in temp_auth_dict:\n",
    "                if \"hindex5y\" in temp_auth_dict:\n",
    "                    org_papers[k][\"hindex\"].append((temp_auth_dict[\"hindex5y\"] + temp_auth_dict[\"hindex\"])/2)\n",
    "                else:\n",
    "                    org_papers[k][\"hindex\"].append(temp_auth_dict[\"hindex\"])\n",
    "            if \"publications\" in temp_auth_dict:\n",
    "                total_publications = 0\n",
    "                for pub in temp_auth_dict[\"publications\"]:\n",
    "                    if \"year\" in pub.__dict__[\"bib\"]:\n",
    "                        if pub.__dict__[\"bib\"][\"year\"] < global_year:\n",
    "                            total_publications += 1\n",
    "                    else:\n",
    "                        total_publications += 1\n",
    "                org_papers[k][\"pub_count_gs\"].append(total_publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
