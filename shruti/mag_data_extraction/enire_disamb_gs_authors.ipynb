{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import unidecode\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "papers_data = defaultdict(dict)\n",
    "\n",
    "for y in years:\n",
    "    papers_data[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflicts_dict = {}\n",
    "emails_dict = {}\n",
    "\n",
    "for year in papers_data:\n",
    "    for paper_id in papers_data[year]:\n",
    "        year_app_key = str(year) + \"_\" + paper_id\n",
    "        \n",
    "        if \"conflicts\" in papers_data[year][paper_id][\"content\"]:\n",
    "            conflicts_dict[year_app_key] = papers_data[year][paper_id][\"content\"][\"conflicts\"]\n",
    "        else:\n",
    "            conflicts_dict[year_app_key] = []\n",
    "            for author_email in papers_data[year][paper_id][\"content\"][\"authorids\"]:\n",
    "                conflicts_dict[year_app_key].append(author_email.split(\"@\")[1])\n",
    "        \n",
    "        emails_dict[year_app_key] = papers_data[year][paper_id][\"content\"][\"authorids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle('../features/all_data_features_17_20.pkl')\n",
    "data = data_raw[[\"id\", \"title\", \"label\", \"authors\"]]\n",
    "org_papers = data.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017_B1-Hhnslg</th>\n",
       "      <td>2017_B1-Hhnslg</td>\n",
       "      <td>Prototypical Networks for Few-shot Learning</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Jake Snell, Kevin Swersky, Richard Zemel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017_B1-q5Pqxl</th>\n",
       "      <td>2017_B1-q5Pqxl</td>\n",
       "      <td>Machine Comprehension Using Match-LSTM and Ans...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>[Shuohang Wang, Jing Jiang]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  \\\n",
       "2017_B1-Hhnslg  2017_B1-Hhnslg   \n",
       "2017_B1-q5Pqxl  2017_B1-q5Pqxl   \n",
       "\n",
       "                                                            title   label  \\\n",
       "2017_B1-Hhnslg        Prototypical Networks for Few-shot Learning  Reject   \n",
       "2017_B1-q5Pqxl  Machine Comprehension Using Match-LSTM and Ans...  Accept   \n",
       "\n",
       "                                                   authors  \n",
       "2017_B1-Hhnslg  [Jake Snell, Kevin Swersky, Richard Zemel]  \n",
       "2017_B1-q5Pqxl                 [Shuohang Wang, Jing Jiang]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1740\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1820\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "2100\n",
      "2120\n",
      "2140\n",
      "2160\n",
      "2180\n",
      "2200\n",
      "2220\n",
      "2240\n",
      "2260\n",
      "2280\n",
      "2300\n",
      "2320\n",
      "2340\n",
      "2360\n",
      "2380\n",
      "2400\n",
      "2420\n",
      "2440\n",
      "2460\n",
      "2480\n",
      "2500\n",
      "2520\n",
      "2540\n",
      "2560\n",
      "2580\n",
      "2600\n",
      "2620\n",
      "2640\n",
      "2660\n",
      "2680\n",
      "2700\n",
      "2720\n",
      "2740\n",
      "2760\n",
      "2780\n",
      "2800\n",
      "2820\n",
      "2840\n",
      "2860\n",
      "2880\n",
      "2900\n",
      "2920\n",
      "2940\n",
      "2960\n",
      "2980\n",
      "3000\n",
      "3020\n",
      "3040\n",
      "3060\n",
      "3080\n",
      "3100\n",
      "3120\n",
      "3140\n",
      "3160\n",
      "3180\n",
      "3200\n",
      "3220\n",
      "3240\n",
      "3260\n",
      "3280\n",
      "3300\n",
      "3320\n",
      "3340\n",
      "3360\n",
      "3380\n",
      "3400\n",
      "3420\n",
      "3440\n",
      "3460\n",
      "3480\n",
      "3500\n",
      "3520\n",
      "3540\n",
      "3560\n",
      "3580\n",
      "3600\n",
      "3620\n",
      "3640\n",
      "3660\n",
      "3680\n",
      "3700\n",
      "3720\n",
      "3740\n",
      "3760\n",
      "3780\n",
      "3800\n",
      "3820\n",
      "3840\n",
      "3860\n",
      "3880\n",
      "3900\n",
      "3920\n",
      "3940\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-ec6b1674f120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mclean_auth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munaacented_auth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_auth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mall_authors_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# per_paper = defaultdict(list)\n",
    "\n",
    "# still_not_found = 0\n",
    "# total_auth_count = 0\n",
    "# found_count = 0\n",
    "# not_in_gs = 0\n",
    "# cc = 0\n",
    "\n",
    "status_track = 0\n",
    "\n",
    "for p in org_papers:\n",
    "    status_track += 1\n",
    "    if status_track % 20 == 0:\n",
    "        print(status_track)\n",
    "    if not p in done_papers:\n",
    "        new_auths = []\n",
    "        for auth in org_papers[p][\"authors\"]:\n",
    "            new_auths += auth.split(\" and \")\n",
    "        \n",
    "        org_papers[p][\"authors\"] = org_papers[p][\"authors\"] + new_auths\n",
    "        org_papers[p][\"authors\"] = list(set(org_papers[p][\"authors\"]))\n",
    "        \n",
    "        for auth in org_papers[p][\"authors\"]:\n",
    "            if True:\n",
    "#             try:\n",
    "                total_auth_count += 1\n",
    "                disambiguated_author = None\n",
    "\n",
    "                rm_ast = auth.replace(\"*\", \"\") \n",
    "                unaacented_auth = unidecode.unidecode(rm_ast)\n",
    "                clean_auth = unaacented_auth.lower()\n",
    "\n",
    "                key = clean_auth[0]\n",
    "\n",
    "                all_authors_gs = None\n",
    "\n",
    "                if ord(key) <= 122 and ord(key) >=97:\n",
    "                    with open(\"gs_collation_experiments/author_name_index/{}.pkl\".format(key), \"rb\") as index_files:\n",
    "                        all_authors_gs = pickle.load(index_files)\n",
    "                else:\n",
    "                    print(\"Non alpha author name: \", auth)\n",
    "                    all_authors_gs = {}\n",
    "\n",
    "                if clean_auth in all_authors_gs:\n",
    "                    if len(all_authors_gs[clean_auth]) == 1:\n",
    "                        disambiguated_author = all_authors_gs[clean_auth][0][0]\n",
    "                        found_count += 1\n",
    "                    else:\n",
    "                        found_paper = False\n",
    "                        for mauth in all_authors_gs[clean_auth]:\n",
    "                            if type(mauth[0]) != dict:\n",
    "                                fixed_mauth = mauth[0].__dict__\n",
    "                            else:\n",
    "                                fixed_mauth = mauth[0]\n",
    "                            for pub in fixed_mauth[\"publications\"]:\n",
    "                                if type(pub) != dict:\n",
    "                                    pub_test = pub.__dict__\n",
    "                                else:\n",
    "                                    pub_test = pub\n",
    "                                org_title = ''.join(filter(str.isalpha, org_papers[p][\"title\"].lower()))\n",
    "                                gs_pub_title = ''.join(filter(str.isalpha, pub_test[\"bib\"][\"title\"].lower()))\n",
    "\n",
    "                                if org_title == gs_pub_title:\n",
    "                                    found_paper = True\n",
    "                                    disambiguated_author = fixed_mauth\n",
    "                                    found_count += 1\n",
    "                                    break\n",
    "                                elif editdistance.eval(org_title, gs_pub_title) < 5:\n",
    "                                    found_paper = True\n",
    "                                    disambiguated_author = fixed_mauth\n",
    "                                    found_count += 1\n",
    "                                    break\n",
    "                            if found_paper:\n",
    "                                break\n",
    "                        if not found_paper:\n",
    "                            # Paper not found, check via conflicts/email information\n",
    "                            matching_affils = []\n",
    "                            \n",
    "                            for conflict_affil in conflicts_dict[p]:\n",
    "                                for mauth in all_authors_gs[clean_auth]:\n",
    "                                    if type(mauth[0]) != dict:\n",
    "#                                         print(type(mauth[0]))\n",
    "#                                         print(dir(mauth[0]))\n",
    "                                        fixed_mauth = mauth[0].__dict__\n",
    "                                    else:\n",
    "                                        fixed_mauth = mauth[0]\n",
    "                                    if fixed_mauth[\"email\"].find(conflict_affil) > -1:\n",
    "                                        matching_affils.append(mauth)\n",
    "#                                         print(\"FOUND\", conflict_affil, fixed_mauth[\"email\"], len(matching_affils))\n",
    "#                                     else:\n",
    "#                                         print(\"NO\", conflict_affil, mauth[0][\"email\"], fixed_mauth[\"email\"], fixed_mauth[\"email\"].find(conflict_affil))\n",
    "                                    #elif editdistance.eval(conflict_affil, mauth[0].email) < 4:\n",
    "                                    #    matching_affils.append(mauth)\n",
    "\n",
    "                            # still not found\n",
    "                            if len(matching_affils) == 0:\n",
    "                                still_not_found += 1\n",
    "                                #print(conflicts_dict[p])\n",
    "                                #for conflict_affil in conflicts_dict[p]:\n",
    "                                #    for mauth in all_authors_gs[clean_auth]:\n",
    "                                #        if mauth[0][\"email\"].find(conflict_affil) > -1:\n",
    "                                #            print(\"FOUND\", conflict_affil, mauth[0][\"email\"], len(matching_affils))\n",
    "                                #        else:\n",
    "                                #            print(\"NO\", conflict_affil, mauth[0][\"email\"])\n",
    "                            elif len(matching_affils) == 1:\n",
    "                                disambiguated_author = matching_affils[0][0]\n",
    "                                found_count += 1\n",
    "                            else:\n",
    "                                potential_emailids = []\n",
    "                                auth_split = clean_auth.split(\" \")\n",
    "                                for splitname in auth_split:\n",
    "                                    for authemailid in emails_dict[p]:\n",
    "                                        if authemailid.find(splitname) > -1:\n",
    "                                            potential_emailids.append(authemailid)\n",
    "\n",
    "                                potential_emailids = list(set(potential_emailids))\n",
    "                                if len(potential_emailids) == 1:\n",
    "                                    domain = potential_emailids[0].split(\"@\")[-1]\n",
    "                                    identified_auth = []\n",
    "                                    for m in matching_affils:\n",
    "                                        if type(m[0]) != dict:\n",
    "                                            temp_m_somethign = m[0].__dict__\n",
    "                                        else:\n",
    "                                            temp_m_somethign = m[0]\n",
    "                                        \n",
    "                                        if temp_m_somethign[\"email\"].find(domain) > -1:\n",
    "                                            identified_auth.append(temp_m_somethign)\n",
    "\n",
    "                                    if len(identified_auth) == 1:\n",
    "                                        disambiguated_author = temp_m_somethign\n",
    "                                        found_count +=1 \n",
    "                                    else:\n",
    "                                        still_not_found += 1\n",
    "                                        #still_not_found_list.append(auth)\n",
    "                                else:\n",
    "                                    still_not_found += 1\n",
    "                                    #still_not_found_list.append(auth)\n",
    "                    if not disambiguated_author is None:\n",
    "                        cc +=1\n",
    "                        per_paper[p].append(disambiguated_author)\n",
    "                else:\n",
    "                    not_in_gs +=1\n",
    "#             except Exception as ex:\n",
    "#                 print(p, ex)\n",
    "#                 print(mauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found authors:  8079\n",
      "Added to dict:  8079\n",
      "Multiple entries in GS but no publication with the same title found or empty GS:  1558\n",
      "NOt in GS data:  6164\n",
      "Total authors to be disambiguated:  15803\n"
     ]
    }
   ],
   "source": [
    "print(\"Found authors: \", found_count)\n",
    "print(\"Added to dict: \", cc)\n",
    "print(\"Multiple entries in GS but no publication with the same title found or empty GS: \", still_not_found)\n",
    "print(\"NOt in GS data: \", not_in_gs)\n",
    "print(\"Total authors to be disambiguated: \", total_auth_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, v in org_papers.items():\n",
    "    org_papers[p][\"pub_count_gs\"] = []\n",
    "    org_papers[p][\"cit_count_gs\"] = []\n",
    "    org_papers[p][\"hindex\"] = []\n",
    "    \n",
    "    # CHECK IF THIS NEEDS TO BE INT\n",
    "    global_year = int(p.split(\"_\")[0])\n",
    "    \n",
    "    for auth_info in per_paper[p]:\n",
    "        try:\n",
    "            if type(auth_info) == dict:\n",
    "                temp_auth_dict = auth_info\n",
    "            else:\n",
    "                temp_auth_dict = auth_info.__dict__\n",
    "            \n",
    "            if \"citedby\" in temp_auth_dict and temp_auth_dict[\"citedby\"] > 0:\n",
    "                total_citations = temp_auth_dict[\"citedby\"]\n",
    "                if \"cites_per_year\" in temp_auth_dict:\n",
    "                    for y in temp_auth_dict[\"cites_per_year\"]:\n",
    "                        if y > (global_year-1):\n",
    "                            total_citations -= temp_auth_dict[\"cites_per_year\"][y]\n",
    "                org_papers[p][\"cit_count_gs\"].append(total_citations)\n",
    "            if \"publications\" in temp_auth_dict:\n",
    "                total_publications = 0\n",
    "                for pub in temp_auth_dict[\"publications\"]:\n",
    "                    if type(pub) != dict:\n",
    "                        pub_dict = pub.__dict__\n",
    "                    else:\n",
    "                        pub_dict = pub\n",
    "                    if \"year\" in pub_dict[\"bib\"]:\n",
    "                        if int(pub_dict[\"bib\"][\"year\"]) < global_year:\n",
    "                            total_publications += 1\n",
    "                    else:\n",
    "                        total_publications += 1\n",
    "                org_papers[p][\"pub_count_gs\"].append(total_publications)\n",
    "        except Exception as ex:\n",
    "            print(p,ex)\n",
    "            break\n",
    "#     for a in v[\"authors\"]:\n",
    "#         if a in paper_authors_info:\n",
    "#             temp_auth_dict = paper_authors_info[a].__dict__\n",
    "#             if \"citedby\" in temp_auth_dict and temp_auth_dict[\"citedby\"] > 0:\n",
    "#                 total_citations = temp_auth_dict[\"citedby\"]\n",
    "#                 if \"cites_per_year\" in temp_auth_dict:\n",
    "#                     for y in temp_auth_dict[\"cites_per_year\"]:\n",
    "#                         if y > (global_year-1):\n",
    "#                             total_citations -= temp_auth_dict[\"cites_per_year\"][y]\n",
    "#                 org_papers[k][\"cit_count_gs\"].append(total_citations)\n",
    "#             if \"hindex\" in temp_auth_dict:\n",
    "#                 if \"hindex5y\" in temp_auth_dict:\n",
    "#                     org_papers[k][\"hindex\"].append((temp_auth_dict[\"hindex5y\"] + temp_auth_dict[\"hindex\"])/2)\n",
    "#                 else:\n",
    "#                     org_papers[k][\"hindex\"].append(temp_auth_dict[\"hindex\"])\n",
    "#             if \"publications\" in temp_auth_dict:\n",
    "#                 total_publications = 0\n",
    "#                 for pub in temp_auth_dict[\"publications\"]:\n",
    "#                     if \"year\" in pub.__dict__[\"bib\"]:\n",
    "#                         if pub.__dict__[\"bib\"][\"year\"] < global_year:\n",
    "#                             total_publications += 1\n",
    "#                     else:\n",
    "#                         total_publications += 1\n",
    "#                 org_papers[k][\"pub_count_gs\"].append(total_publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_papers = []\n",
    "\n",
    "for k, v in org_papers.items():\n",
    "    if v[\"pub_count_gs\"]:\n",
    "        #print(k)\n",
    "        done_papers.append(k)\n",
    "\n",
    "done_papers = list(set(done_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3294\n",
      "<class 'scholarly.scholarly.Author'>\n"
     ]
    }
   ],
   "source": [
    "for i in per_paper[\"2018_BkSDMA36Z\"]:\n",
    "    print(i.__dict__[\"citedby\"])\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for i in per_paper[\"2020_rJeW1yHYwH\"]:\n",
    "#     print(i.__dict__[\"citedby\"])\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cites': '10', 'year': '2018', 'title': 'Deep multi-scale architectures for monocular depth estimation'}\n",
      "{'cites': '0', 'year': '2019', 'title': 'n-MeRCI: A new Metric to Evaluate the Correlation Between Predictive Uncertainty and True Error'}\n",
      "{'cites': '0', 'year': '2019', 'title': \"Estimation de profondeur à partir d'images monoculaires par apprentissage profond\"}\n"
     ]
    }
   ],
   "source": [
    "for i in auth_info[\"publications\"]:\n",
    "    print(i[\"bib\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"runnnnn.pkl\", \"wb\") as run:\n",
    "    pickle.dump(org_papers, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2019_SJMnG2C9YX',\n",
       "  {'authors': ['Aditya Krishna Menon',\n",
       "    'Takashi Ishida',\n",
       "    'Masashi Sugiyama',\n",
       "    'Gang Niu'],\n",
       "   'cit_count_gs': [],\n",
       "   'hindex': [],\n",
       "   'id': '2019_SJMnG2C9YX',\n",
       "   'label': 'Reject',\n",
       "   'pub_count_gs': [],\n",
       "   'title': 'Complementary-label learning for arbitrary losses and models'}),\n",
       " ('2020_B1erJJrYPH',\n",
       "  {'authors': ['Payel Das',\n",
       "    'Prasanna Sattigeri',\n",
       "    'N. Joseph Tatro',\n",
       "    'Pin-Yu Chen',\n",
       "    'Rongjie Lai',\n",
       "    'Igor Melnyk'],\n",
       "   'cit_count_gs': [],\n",
       "   'hindex': [],\n",
       "   'id': '2020_B1erJJrYPH',\n",
       "   'label': 'Reject',\n",
       "   'pub_count_gs': [],\n",
       "   'title': 'Optimizing Loss Landscape Connectivity via Neuron Alignment'}),\n",
       " ('2019_Byxpfh0cFm',\n",
       "  {'authors': ['Virginia Smith', 'Michael Kuchnik'],\n",
       "   'cit_count_gs': [],\n",
       "   'hindex': [],\n",
       "   'id': '2019_Byxpfh0cFm',\n",
       "   'label': 'Accept',\n",
       "   'pub_count_gs': [],\n",
       "   'title': 'Efficient Augmentation via Data Subsampling'})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(org_papers.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
