{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import unidecode\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "papers_data = defaultdict(dict)\n",
    "\n",
    "for y in years:\n",
    "    papers_data[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflicts_dict = {}\n",
    "emails_dict = {}\n",
    "\n",
    "for year in papers_data:\n",
    "    for paper_id in papers_data[year]:\n",
    "        year_app_key = str(year) + \"_\" + paper_id\n",
    "        \n",
    "        if \"conflicts\" in papers_data[year][paper_id][\"content\"]:\n",
    "            conflicts_dict[year_app_key] = papers_data[year][paper_id][\"content\"][\"conflicts\"]\n",
    "        else:\n",
    "            conflicts_dict[year_app_key] = []\n",
    "            for author_email in papers_data[year][paper_id][\"content\"][\"authorids\"]:\n",
    "                conflicts_dict[year_app_key].append(author_email.split(\"@\")[1])\n",
    "        \n",
    "        emails_dict[year_app_key] = papers_data[year][paper_id][\"content\"][\"authorids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle('../features/all_data_features_17_20.pkl')\n",
    "data = data_raw[[\"id\", \"title\", \"label\", \"authors\"]]\n",
    "org_papers = data.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017_B1-Hhnslg</th>\n",
       "      <td>2017_B1-Hhnslg</td>\n",
       "      <td>Prototypical Networks for Few-shot Learning</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Jake Snell, Kevin Swersky, Richard Zemel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017_B1-q5Pqxl</th>\n",
       "      <td>2017_B1-q5Pqxl</td>\n",
       "      <td>Machine Comprehension Using Match-LSTM and Ans...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>[Shuohang Wang, Jing Jiang]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  \\\n",
       "2017_B1-Hhnslg  2017_B1-Hhnslg   \n",
       "2017_B1-q5Pqxl  2017_B1-q5Pqxl   \n",
       "\n",
       "                                                            title   label  \\\n",
       "2017_B1-Hhnslg        Prototypical Networks for Few-shot Learning  Reject   \n",
       "2017_B1-q5Pqxl  Machine Comprehension Using Match-LSTM and Ans...  Accept   \n",
       "\n",
       "                                                   authors  \n",
       "2017_B1-Hhnslg  [Jake Snell, Kevin Swersky, Richard Zemel]  \n",
       "2017_B1-q5Pqxl                 [Shuohang Wang, Jing Jiang]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_paper = defaultdict(list)\n",
    "\n",
    "still_not_found = 0\n",
    "total_auth_count = 0\n",
    "found_count = 0\n",
    "not_in_gs = 0\n",
    "cc = 0\n",
    "\n",
    "status_track = 0\n",
    "\n",
    "for p in org_papers:\n",
    "    status_track += 1\n",
    "    if status_track % 20 == 0:\n",
    "        print(status_track)\n",
    "    if True:\n",
    "        new_auths = []\n",
    "        for auth in org_papers[p][\"authors\"]:\n",
    "            new_auths += auth.split(\" and \")\n",
    "        \n",
    "        org_papers[p][\"authors\"] = org_papers[p][\"authors\"] + new_auths\n",
    "        org_papers[p][\"authors\"] = list(set(org_papers[p][\"authors\"]))\n",
    "        \n",
    "        for auth in org_papers[p][\"authors\"]:\n",
    "            if True:\n",
    "#             try:\n",
    "                total_auth_count += 1\n",
    "                disambiguated_author = None\n",
    "\n",
    "                rm_ast = auth.replace(\"*\", \"\") \n",
    "                unaacented_auth = unidecode.unidecode(rm_ast)\n",
    "                clean_auth = unaacented_auth.lower()\n",
    "\n",
    "                key = clean_auth[0]\n",
    "\n",
    "                all_authors_gs = None\n",
    "\n",
    "                if ord(key) <= 122 and ord(key) >=97:\n",
    "                    with open(\"gs_collation_experiments/author_name_index/{}.pkl\".format(key), \"rb\") as index_files:\n",
    "                        all_authors_gs = pickle.load(index_files)\n",
    "                else:\n",
    "                    print(\"Non alpha author name: \", auth)\n",
    "                    all_authors_gs = {}\n",
    "\n",
    "                if clean_auth in all_authors_gs:\n",
    "                    if len(all_authors_gs[clean_auth]) == 1:\n",
    "                        disambiguated_author = all_authors_gs[clean_auth][0][0]\n",
    "                        found_count += 1\n",
    "                    else:\n",
    "                        found_paper = False\n",
    "                        for mauth in all_authors_gs[clean_auth]:\n",
    "                            if type(mauth[0]) != dict:\n",
    "                                fixed_mauth = mauth[0].__dict__\n",
    "                            else:\n",
    "                                fixed_mauth = mauth[0]\n",
    "                            for pub in fixed_mauth[\"publications\"]:\n",
    "                                if type(pub) != dict:\n",
    "                                    pub_test = pub.__dict__\n",
    "                                else:\n",
    "                                    pub_test = pub\n",
    "                                org_title = ''.join(filter(str.isalpha, org_papers[p][\"title\"].lower()))\n",
    "                                gs_pub_title = ''.join(filter(str.isalpha, pub_test[\"bib\"][\"title\"].lower()))\n",
    "\n",
    "                                if org_title == gs_pub_title:\n",
    "                                    found_paper = True\n",
    "                                    disambiguated_author = fixed_mauth\n",
    "                                    found_count += 1\n",
    "                                    break\n",
    "                                elif editdistance.eval(org_title, gs_pub_title) < 5:\n",
    "                                    found_paper = True\n",
    "                                    disambiguated_author = fixed_mauth\n",
    "                                    found_count += 1\n",
    "                                    break\n",
    "                            if found_paper:\n",
    "                                break\n",
    "                        if not found_paper:\n",
    "                            # Paper not found, check via conflicts/email information\n",
    "                            matching_affils = []\n",
    "                            \n",
    "                            for conflict_affil in conflicts_dict[p]:\n",
    "                                for mauth in all_authors_gs[clean_auth]:\n",
    "                                    if type(mauth[0]) != dict:\n",
    "                                        fixed_mauth = dict(mauth[0])\n",
    "                                    else:\n",
    "                                        fixed_mauth = mauth[0]\n",
    "                                    if fixed_mauth[\"email\"].find(conflict_affil) > -1:\n",
    "                                        matching_affils.append(mauth)\n",
    "                                        print(\"FOUND\", conflict_affil, fixed_mauth[\"email\"], len(matching_affils))\n",
    "                                    else:\n",
    "                                        print(\"NO\", conflict_affil, mauth[0][\"email\"], fixed_mauth[\"email\"], fixed_mauth[\"email\"].find(conflict_affil))\n",
    "                                    #elif editdistance.eval(conflict_affil, mauth[0].email) < 4:\n",
    "                                    #    matching_affils.append(mauth)\n",
    "\n",
    "                            # still not found\n",
    "                            if len(matching_affils) == 0:\n",
    "                                still_not_found += 1\n",
    "                                #print(conflicts_dict[p])\n",
    "                                #for conflict_affil in conflicts_dict[p]:\n",
    "                                #    for mauth in all_authors_gs[clean_auth]:\n",
    "                                #        if mauth[0][\"email\"].find(conflict_affil) > -1:\n",
    "                                #            print(\"FOUND\", conflict_affil, mauth[0][\"email\"], len(matching_affils))\n",
    "                                #        else:\n",
    "                                #            print(\"NO\", conflict_affil, mauth[0][\"email\"])\n",
    "                            elif len(matching_affils) == 1:\n",
    "                                disambiguated_author = matching_affils[0][0]\n",
    "                                found_count += 1\n",
    "                            else:\n",
    "                                potential_emailids = []\n",
    "                                auth_split = clean_auth.split(\" \")\n",
    "                                for splitname in auth_split:\n",
    "                                    for authemailid in emails_dict[p]:\n",
    "                                        if authemailid.find(splitname) > -1:\n",
    "                                            potential_emailids.append(authemailid)\n",
    "\n",
    "                                potential_emailids = list(set(potential_emailids))\n",
    "                                if len(potential_emailids) == 1:\n",
    "                                    domain = potential_emailids[0].split(\"@\")[-1]\n",
    "                                    identified_auth = []\n",
    "                                    for m in matching_affils:\n",
    "                                        if type(m[0]) != dict:\n",
    "                                            temp_m_somethign = m[0].__dict__\n",
    "                                        else:\n",
    "                                            temp_m_somethign = m[0]\n",
    "                                        \n",
    "                                        if temp_m_somethign[\"email\"].find(domain) > -1:\n",
    "                                            identified_auth.append(temp_m_somethign)\n",
    "\n",
    "                                    if len(identified_auth) == 1:\n",
    "                                        disambiguated_author = temp_m_somethign\n",
    "                                        found_count +=1 \n",
    "                                    else:\n",
    "                                        still_not_found += 1\n",
    "                                        #still_not_found_list.append(auth)\n",
    "                                else:\n",
    "                                    still_not_found += 1\n",
    "                                    #still_not_found_list.append(auth)\n",
    "                    if not disambiguated_author is None:\n",
    "                        cc +=1\n",
    "                        per_paper[p].append(disambiguated_author)\n",
    "                else:\n",
    "                    not_in_gs +=1\n",
    "#             except Exception as ex:\n",
    "#                 print(p, ex)\n",
    "#                 print(mauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"@ibm.com\".find(\"ibm.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found authors:  0\n",
      "Added to dict:  0\n",
      "Multiple entries in GS but no publication with the same title found or empty GS:  0\n",
      "NOt in GS data:  235\n",
      "Total authors to be disambiguated:  236\n"
     ]
    }
   ],
   "source": [
    "print(\"Found authors: \", found_count)\n",
    "print(\"Added to dict: \", cc)\n",
    "print(\"Multiple entries in GS but no publication with the same title found or empty GS: \", still_not_found)\n",
    "print(\"NOt in GS data: \", not_in_gs)\n",
    "print(\"Total authors to be disambiguated: \", total_auth_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, v in org_papers.items():\n",
    "    org_papers[p][\"pub_count_gs\"] = []\n",
    "    org_papers[p][\"cit_count_gs\"] = []\n",
    "    org_papers[p][\"hindex\"] = []\n",
    "    \n",
    "    # CHECK IF THIS NEEDS TO BE INT\n",
    "    global_year = int(p.split(\"_\")[0])\n",
    "    \n",
    "    for auth_info in per_paper[p]:\n",
    "        try:\n",
    "            if type(auth_info) == dict:\n",
    "                temp_auth_dict = auth_info\n",
    "            else:\n",
    "                temp_auth_dict = auth_info.__dict__\n",
    "            \n",
    "            if \"citedby\" in temp_auth_dict and temp_auth_dict[\"citedby\"] > 0:\n",
    "                total_citations = temp_auth_dict[\"citedby\"]\n",
    "                if \"cites_per_year\" in temp_auth_dict:\n",
    "                    for y in temp_auth_dict[\"cites_per_year\"]:\n",
    "                        if y > (global_year-1):\n",
    "                            total_citations -= temp_auth_dict[\"cites_per_year\"][y]\n",
    "                org_papers[p][\"cit_count_gs\"].append(total_citations)\n",
    "            if \"publications\" in temp_auth_dict:\n",
    "                total_publications = 0\n",
    "                for pub in temp_auth_dict[\"publications\"]:\n",
    "                    if type(pub) != dict:\n",
    "                        pub_dict = pub.__dict__\n",
    "                    else:\n",
    "                        pub_dict = pub\n",
    "                    if \"year\" in pub_dict[\"bib\"]:\n",
    "                        if int(pub_dict[\"bib\"][\"year\"]) < global_year:\n",
    "                            total_publications += 1\n",
    "                    else:\n",
    "                        total_publications += 1\n",
    "                org_papers[p][\"pub_count_gs\"].append(total_publications)\n",
    "        except Exception as ex:\n",
    "            print(p,ex)\n",
    "            break\n",
    "#     for a in v[\"authors\"]:\n",
    "#         if a in paper_authors_info:\n",
    "#             temp_auth_dict = paper_authors_info[a].__dict__\n",
    "#             if \"citedby\" in temp_auth_dict and temp_auth_dict[\"citedby\"] > 0:\n",
    "#                 total_citations = temp_auth_dict[\"citedby\"]\n",
    "#                 if \"cites_per_year\" in temp_auth_dict:\n",
    "#                     for y in temp_auth_dict[\"cites_per_year\"]:\n",
    "#                         if y > (global_year-1):\n",
    "#                             total_citations -= temp_auth_dict[\"cites_per_year\"][y]\n",
    "#                 org_papers[k][\"cit_count_gs\"].append(total_citations)\n",
    "#             if \"hindex\" in temp_auth_dict:\n",
    "#                 if \"hindex5y\" in temp_auth_dict:\n",
    "#                     org_papers[k][\"hindex\"].append((temp_auth_dict[\"hindex5y\"] + temp_auth_dict[\"hindex\"])/2)\n",
    "#                 else:\n",
    "#                     org_papers[k][\"hindex\"].append(temp_auth_dict[\"hindex\"])\n",
    "#             if \"publications\" in temp_auth_dict:\n",
    "#                 total_publications = 0\n",
    "#                 for pub in temp_auth_dict[\"publications\"]:\n",
    "#                     if \"year\" in pub.__dict__[\"bib\"]:\n",
    "#                         if pub.__dict__[\"bib\"][\"year\"] < global_year:\n",
    "#                             total_publications += 1\n",
    "#                     else:\n",
    "#                         total_publications += 1\n",
    "#                 org_papers[k][\"pub_count_gs\"].append(total_publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3294\n",
      "<class 'scholarly.scholarly.Author'>\n"
     ]
    }
   ],
   "source": [
    "for i in per_paper[\"2018_BkSDMA36Z\"]:\n",
    "    print(i.__dict__[\"citedby\"])\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for i in per_paper[\"2020_rJeW1yHYwH\"]:\n",
    "#     print(i.__dict__[\"citedby\"])\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cites': '10', 'year': '2018', 'title': 'Deep multi-scale architectures for monocular depth estimation'}\n",
      "{'cites': '0', 'year': '2019', 'title': 'n-MeRCI: A new Metric to Evaluate the Correlation Between Predictive Uncertainty and True Error'}\n",
      "{'cites': '0', 'year': '2019', 'title': \"Estimation de profondeur à partir d'images monoculaires par apprentissage profond\"}\n"
     ]
    }
   ],
   "source": [
    "for i in auth_info[\"publications\"]:\n",
    "    print(i[\"bib\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"runnnnn.pkl\", \"wb\") as run:\n",
    "    pickle.dump(org_papers, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2019_SJMnG2C9YX',\n",
       "  {'authors': ['Aditya Krishna Menon',\n",
       "    'Takashi Ishida',\n",
       "    'Masashi Sugiyama',\n",
       "    'Gang Niu'],\n",
       "   'cit_count_gs': [],\n",
       "   'hindex': [],\n",
       "   'id': '2019_SJMnG2C9YX',\n",
       "   'label': 'Reject',\n",
       "   'pub_count_gs': [],\n",
       "   'title': 'Complementary-label learning for arbitrary losses and models'}),\n",
       " ('2020_B1erJJrYPH',\n",
       "  {'authors': ['Payel Das',\n",
       "    'Prasanna Sattigeri',\n",
       "    'N. Joseph Tatro',\n",
       "    'Pin-Yu Chen',\n",
       "    'Rongjie Lai',\n",
       "    'Igor Melnyk'],\n",
       "   'cit_count_gs': [],\n",
       "   'hindex': [],\n",
       "   'id': '2020_B1erJJrYPH',\n",
       "   'label': 'Reject',\n",
       "   'pub_count_gs': [],\n",
       "   'title': 'Optimizing Loss Landscape Connectivity via Neuron Alignment'}),\n",
       " ('2019_Byxpfh0cFm',\n",
       "  {'authors': ['Virginia Smith', 'Michael Kuchnik'],\n",
       "   'cit_count_gs': [],\n",
       "   'hindex': [],\n",
       "   'id': '2019_Byxpfh0cFm',\n",
       "   'label': 'Accept',\n",
       "   'pub_count_gs': [],\n",
       "   'title': 'Efficient Augmentation via Data Subsampling'})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(org_papers.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_rJeW1yHYwH\n",
      "2019_HJlWXhC5Km\n",
      "2018_BkSDMA36Z\n",
      "2020_ryx6daEtwr\n",
      "2020_Skx24yHFDr\n",
      "2019_ryG8UsR5t7\n"
     ]
    }
   ],
   "source": [
    "for k, v in org_papers.items():\n",
    "    if v[\"pub_count_gs\"]:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
