{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the year wise paper json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_papers_json(year):\n",
    "    paper_data = defaultdict(list)\n",
    "    \n",
    "    if type(year) == int:\n",
    "        year = [year]\n",
    "    \n",
    "    for y in year:\n",
    "        file_path = \"./science-parse/output/{}/{}_ICLR\".format(y, y)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                paper_data[y].append(json.loads(line))\n",
    "            \n",
    "    return paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run only for 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_data = read_papers_json(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'abstractText': 'Recent work has begun exploring neural acoustic '\n",
      "                              'word embeddings—fixeddimensional vector '\n",
      "                              'representations of arbitrary-length speech '\n",
      "                              'segments corresponding to words. Such '\n",
      "                              'embeddings are applicable to speech retrieval '\n",
      "                              'and recognition tasks, where reasoning about '\n",
      "                              'whole words may make it possible to avoid '\n",
      "                              'ambiguous sub-word representations. The main '\n",
      "                              'idea is to map acoustic sequences to '\n",
      "                              'fixed-dimensional vectors such that examples of '\n",
      "                              'the same word are mapped to similar vectors, '\n",
      "                              'while different-word examples are mapped to '\n",
      "                              'very different vectors. In this work we take a '\n",
      "                              'multi-view approach to learning acoustic word '\n",
      "                              'embeddings, in which we jointly learn to embed '\n",
      "                              'acoustic sequences and their corresponding '\n",
      "                              'character sequences. We use deep bidirectional '\n",
      "                              'LSTM embedding models and multi-view '\n",
      "                              'contrastive losses. We study the effect of '\n",
      "                              'different loss variants, including fixed-margin '\n",
      "                              'and cost-sensitive losses. Our acoustic word '\n",
      "                              'embeddings improve over previous approaches for '\n",
      "                              'the task of word discrimination. We also '\n",
      "                              'present results on other tasks that are enabled '\n",
      "                              'by the multi-view approach, including '\n",
      "                              'cross-view word discrimination and word '\n",
      "                              'similarity.',\n",
      "              'authors': ['Wanjia He', 'Weiran Wang'],\n",
      "              'creator': 'LaTeX with hyperref package',\n",
      "              'emails': ['wanjia@ttic.edu',\n",
      "                         'weiranwang@ttic.edu',\n",
      "                         'klivescu@ttic.edu'],\n",
      "              'referenceMentions': [{'context': 'Word embeddings can be '\n",
      "                                                'learned using spectral '\n",
      "                                                'methods (Deerwester et al., '\n",
      "                                                '1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al.',\n",
      "                                     'endOffset': 79,\n",
      "                                     'referenceID': 9,\n",
      "                                     'startOffset': 54},\n",
      "                                    {'context': ', 1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al., '\n",
      "                                                '2003; Mnih & Hinton, 2007; '\n",
      "                                                'Mikolov et al., 2013; '\n",
      "                                                'Pennington et al., 2014).',\n",
      "                                     'endOffset': 151,\n",
      "                                     'referenceID': 3,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': ', 1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al., '\n",
      "                                                '2003; Mnih & Hinton, 2007; '\n",
      "                                                'Mikolov et al., 2013; '\n",
      "                                                'Pennington et al., 2014).',\n",
      "                                     'endOffset': 151,\n",
      "                                     'referenceID': 32,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': ', 1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al., '\n",
      "                                                '2003; Mnih & Hinton, 2007; '\n",
      "                                                'Mikolov et al., 2013; '\n",
      "                                                'Pennington et al., 2014).',\n",
      "                                     'endOffset': 151,\n",
      "                                     'referenceID': 35,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 37,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 28,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 44,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 24,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Such embeddings could be '\n",
      "                                                'useful for tasks like spoken '\n",
      "                                                'term detection (Fiscus et '\n",
      "                                                'al., 2007), spoken '\n",
      "                                                'query-by-example search '\n",
      "                                                '(Anguera et al.',\n",
      "                                     'endOffset': 90,\n",
      "                                     'referenceID': 11,\n",
      "                                     'startOffset': 69},\n",
      "                                    {'context': ', 2007), spoken '\n",
      "                                                'query-by-example search '\n",
      "                                                '(Anguera et al., 2014), or '\n",
      "                                                'even speech recognition using '\n",
      "                                                'a whole-word approach '\n",
      "                                                '(Gemmeke et al.',\n",
      "                                     'endOffset': 62,\n",
      "                                     'referenceID': 0,\n",
      "                                     'startOffset': 40},\n",
      "                                    {'context': ', 2014), or even speech '\n",
      "                                                'recognition using a '\n",
      "                                                'whole-word approach (Gemmeke '\n",
      "                                                'et al., 2011; Bengio & '\n",
      "                                                'Heigold, 2014).',\n",
      "                                     'endOffset': 110,\n",
      "                                     'referenceID': 12,\n",
      "                                     'startOffset': 64},\n",
      "                                    {'context': 'In tasks that involve '\n",
      "                                                'comparing speech segments to '\n",
      "                                                'each other, vector embeddings '\n",
      "                                                'can allow more efficient and '\n",
      "                                                'more accurate distance '\n",
      "                                                'computation than '\n",
      "                                                'sequence-based approaches '\n",
      "                                                'such as dynamic time warping '\n",
      "                                                '(Levin et al., 2013, 2015; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 296,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 205},\n",
      "                                    {'context': 'In tasks that involve '\n",
      "                                                'comparing speech segments to '\n",
      "                                                'each other, vector embeddings '\n",
      "                                                'can allow more efficient and '\n",
      "                                                'more accurate distance '\n",
      "                                                'computation than '\n",
      "                                                'sequence-based approaches '\n",
      "                                                'such as dynamic time warping '\n",
      "                                                '(Levin et al., 2013, 2015; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 296,\n",
      "                                     'referenceID': 8,\n",
      "                                     'startOffset': 205},\n",
      "                                    {'context': 'We refer to the objective in '\n",
      "                                                '(1) as the “classifier '\n",
      "                                                'network” objective, which has '\n",
      "                                                'been used in several prior '\n",
      "                                                'studies on acoustic word '\n",
      "                                                'embeddings (Bengio & Heigold, '\n",
      "                                                '2014; Kamper et al., 2016; '\n",
      "                                                'Settle & Livescu, 2016).',\n",
      "                                     'endOffset': 214,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 145},\n",
      "                                    {'context': 'An alternative approach, '\n",
      "                                                'based on Siamese networks '\n",
      "                                                '(Bromley et al., 1993), uses '\n",
      "                                                'supervision of the form '\n",
      "                                                '“segment x(1) is similar to '\n",
      "                                                'segment x(2), and is not '\n",
      "                                                'similar to segment x(3)”, '\n",
      "                                                'where two segments are '\n",
      "                                                'considered similar if they '\n",
      "                                                'have the same word label and '\n",
      "                                                'dissimilar otherwise.',\n",
      "                                     'endOffset': 73,\n",
      "                                     'referenceID': 4,\n",
      "                                     'startOffset': 51},\n",
      "                                    {'context': 'Models based on Siamese '\n",
      "                                                'networks have been used for a '\n",
      "                                                'variety of representation '\n",
      "                                                'learning problems in NLP (Hu '\n",
      "                                                'et al., 2014; Wieting et al., '\n",
      "                                                '2016), vision (Hadsell et al.',\n",
      "                                     'endOffset': 144,\n",
      "                                     'referenceID': 23,\n",
      "                                     'startOffset': 105},\n",
      "                                    {'context': 'Models based on Siamese '\n",
      "                                                'networks have been used for a '\n",
      "                                                'variety of representation '\n",
      "                                                'learning problems in NLP (Hu '\n",
      "                                                'et al., 2014; Wieting et al., '\n",
      "                                                '2016), vision (Hadsell et al.',\n",
      "                                     'endOffset': 144,\n",
      "                                     'referenceID': 44,\n",
      "                                     'startOffset': 105},\n",
      "                                    {'context': ', 2016), vision (Hadsell et '\n",
      "                                                'al., 2006), and speech '\n",
      "                                                '(Synnaeve et al.',\n",
      "                                     'endOffset': 38,\n",
      "                                     'referenceID': 16,\n",
      "                                     'startOffset': 16},\n",
      "                                    {'context': ', 2006), and speech (Synnaeve '\n",
      "                                                'et al., 2014; Kamper et al., '\n",
      "                                                '2015) including acoustic word '\n",
      "                                                'embeddings (Kamper et al.',\n",
      "                                     'endOffset': 64,\n",
      "                                     'referenceID': 40,\n",
      "                                     'startOffset': 20},\n",
      "                                    {'context': ', 2006), and speech (Synnaeve '\n",
      "                                                'et al., 2014; Kamper et al., '\n",
      "                                                '2015) including acoustic word '\n",
      "                                                'embeddings (Kamper et al.',\n",
      "                                     'endOffset': 64,\n",
      "                                     'referenceID': 25,\n",
      "                                     'startOffset': 20},\n",
      "                                    {'context': ', 2015) including acoustic '\n",
      "                                                'word embeddings (Kamper et '\n",
      "                                                'al., 2016; Settle & Livescu, '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 88,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 43},\n",
      "                                    {'context': 'The term “Siamese” (Bromley '\n",
      "                                                'et al., 1993; Chopra et al., '\n",
      "                                                '2005) refers to the fact that '\n",
      "                                                'the triplet (x(1), x(2), '\n",
      "                                                'x(3)) share the same '\n",
      "                                                'embedding network f .',\n",
      "                                     'endOffset': 62,\n",
      "                                     'referenceID': 4,\n",
      "                                     'startOffset': 19},\n",
      "                                    {'context': 'The term “Siamese” (Bromley '\n",
      "                                                'et al., 1993; Chopra et al., '\n",
      "                                                '2005) refers to the fact that '\n",
      "                                                'the triplet (x(1), x(2), '\n",
      "                                                'x(3)) share the same '\n",
      "                                                'embedding network f .',\n",
      "                                     'endOffset': 62,\n",
      "                                     'referenceID': 7,\n",
      "                                     'startOffset': 19},\n",
      "                                    {'context': 'While many deep multi-view '\n",
      "                                                'learning objectives are '\n",
      "                                                'applicable (Ngiam et al., '\n",
      "                                                '2011; Srivastava & '\n",
      "                                                'Salakhutdinov, 2014; Sohn et '\n",
      "                                                'al., 2014; Wang et al., '\n",
      "                                                '2015), we consider the '\n",
      "                                                'multi-view contrastive loss '\n",
      "                                                'objective of (Hermann & '\n",
      "                                                'Blunsom, 2014), which is '\n",
      "                                                'simple to optimize and '\n",
      "                                                'implement and performs well '\n",
      "                                                'in practice.',\n",
      "                                     'endOffset': 154,\n",
      "                                     'referenceID': 34,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'While many deep multi-view '\n",
      "                                                'learning objectives are '\n",
      "                                                'applicable (Ngiam et al., '\n",
      "                                                '2011; Srivastava & '\n",
      "                                                'Salakhutdinov, 2014; Sohn et '\n",
      "                                                'al., 2014; Wang et al., '\n",
      "                                                '2015), we consider the '\n",
      "                                                'multi-view contrastive loss '\n",
      "                                                'objective of (Hermann & '\n",
      "                                                'Blunsom, 2014), which is '\n",
      "                                                'simple to optimize and '\n",
      "                                                'implement and performs well '\n",
      "                                                'in practice.',\n",
      "                                     'endOffset': 154,\n",
      "                                     'referenceID': 38,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'While many deep multi-view '\n",
      "                                                'learning objectives are '\n",
      "                                                'applicable (Ngiam et al., '\n",
      "                                                '2011; Srivastava & '\n",
      "                                                'Salakhutdinov, 2014; Sohn et '\n",
      "                                                'al., 2014; Wang et al., '\n",
      "                                                '2015), we consider the '\n",
      "                                                'multi-view contrastive loss '\n",
      "                                                'objective of (Hermann & '\n",
      "                                                'Blunsom, 2014), which is '\n",
      "                                                'simple to optimize and '\n",
      "                                                'implement and performs well '\n",
      "                                                'in practice.',\n",
      "                                     'endOffset': 154,\n",
      "                                     'referenceID': 43,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'This approach produced '\n",
      "                                                'improved performance on a '\n",
      "                                                'word discrimination task '\n",
      "                                                'compared to using raw DTW '\n",
      "                                                'distances, and was later also '\n",
      "                                                'applied successfully for a '\n",
      "                                                'query-by-example task (Levin '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 199,\n",
      "                                     'referenceID': 30,\n",
      "                                     'startOffset': 179},\n",
      "                                    {'context': 'Most evaluations have been '\n",
      "                                                'based on word discrimination '\n",
      "                                                '– the task of determining '\n",
      "                                                'whether two speech segments '\n",
      "                                                'correspond to the same word '\n",
      "                                                'or not – which can be seen as '\n",
      "                                                'a proxy for query-by-example '\n",
      "                                                'search (Levin et al., 2013; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 289,\n",
      "                                     'referenceID': 29,\n",
      "                                     'startOffset': 204},\n",
      "                                    {'context': 'Most evaluations have been '\n",
      "                                                'based on word discrimination '\n",
      "                                                '– the task of determining '\n",
      "                                                'whether two speech segments '\n",
      "                                                'correspond to the same word '\n",
      "                                                'or not – which can be seen as '\n",
      "                                                'a proxy for query-by-example '\n",
      "                                                'search (Levin et al., 2013; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 289,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 204},\n",
      "                                    {'context': 'Most evaluations have been '\n",
      "                                                'based on word discrimination '\n",
      "                                                '– the task of determining '\n",
      "                                                'whether two speech segments '\n",
      "                                                'correspond to the same word '\n",
      "                                                'or not – which can be seen as '\n",
      "                                                'a proxy for query-by-example '\n",
      "                                                'search (Levin et al., 2013; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 289,\n",
      "                                     'referenceID': 8,\n",
      "                                     'startOffset': 204},\n",
      "                                    {'context': 'For example, the standard '\n",
      "                                                'task of spoken term detection '\n",
      "                                                '(Fiscus et al., 2007) '\n",
      "                                                'involves searching for '\n",
      "                                                'examples of a given text '\n",
      "                                                'query in spoken documents.',\n",
      "                                     'endOffset': 77,\n",
      "                                     'referenceID': 11,\n",
      "                                     'startOffset': 56},\n",
      "                                    {'context': 'This is analogous to the '\n",
      "                                                'evaluation of semantic word '\n",
      "                                                'embeddings via the rank '\n",
      "                                                'correlation between embedding '\n",
      "                                                'distances and human '\n",
      "                                                'similarity judgments '\n",
      "                                                '(Finkelstein et al., 2001; '\n",
      "                                                'Hill et al., 2015).',\n",
      "                                     'endOffset': 193,\n",
      "                                     'referenceID': 10,\n",
      "                                     'startOffset': 148},\n",
      "                                    {'context': 'This is analogous to the '\n",
      "                                                'evaluation of semantic word '\n",
      "                                                'embeddings via the rank '\n",
      "                                                'correlation between embedding '\n",
      "                                                'distances and human '\n",
      "                                                'similarity judgments '\n",
      "                                                '(Finkelstein et al., 2001; '\n",
      "                                                'Hill et al., 2015).',\n",
      "                                     'endOffset': 193,\n",
      "                                     'referenceID': 21,\n",
      "                                     'startOffset': 148},\n",
      "                                    {'context': 'The task and setup were first '\n",
      "                                                'developed by (Carlin et al., '\n",
      "                                                '2011).',\n",
      "                                     'endOffset': 64,\n",
      "                                     'referenceID': 5,\n",
      "                                     'startOffset': 43},\n",
      "                                    {'context': 'The data is drawn from the '\n",
      "                                                'Switchboard English '\n",
      "                                                'conversational speech corpus '\n",
      "                                                '(Godfrey et al., 1992).',\n",
      "                                     'endOffset': 98,\n",
      "                                     'referenceID': 14,\n",
      "                                     'startOffset': 76},\n",
      "                                    {'context': 'Method Test AP Test AP '\n",
      "                                                '(acoustic) (cross-view) MFCCs '\n",
      "                                                '+ DTW (Kamper et al., 2016) '\n",
      "                                                '0.',\n",
      "                                     'endOffset': 80,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 59},\n",
      "                                    {'context': '214 Correspondence '\n",
      "                                                'autoencoder + DTW (Kamper et '\n",
      "                                                'al., 2015) 0.',\n",
      "                                     'endOffset': 58,\n",
      "                                     'referenceID': 25,\n",
      "                                     'startOffset': 37}],\n",
      "              'references': [{'author': ['Xavier Anguera',\n",
      "                                         'Luis Javier Rodriguez-Fuentes',\n",
      "                                         'Igor Szöke',\n",
      "                                         'Andi Buzo',\n",
      "                                         'Florian Metze'],\n",
      "                              'citeRegEx': 'Anguera et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Anguera et al\\\\.',\n",
      "                              'title': 'Query by example search on speech at '\n",
      "                                       'mediaeval',\n",
      "                              'venue': 'In MediaEval,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Kartik Audhkhasi',\n",
      "                                         'Andrew Rosenberg',\n",
      "                                         'Abhinav Sethy',\n",
      "                                         'Bhuvana Ramabhadran',\n",
      "                                         'Brian Kingsbury'],\n",
      "                              'citeRegEx': 'Audhkhasi et al\\\\.,? \\\\Q2017\\\\E',\n",
      "                              'shortCiteRegEx': 'Audhkhasi et al\\\\.',\n",
      "                              'title': 'End-to-end ASR-free keyword search '\n",
      "                                       'from speech',\n",
      "                              'venue': 'arXiv preprint arXiv:1701.04313,',\n",
      "                              'year': 2017},\n",
      "                             {'author': ['Samy Bengio', 'Georg Heigold'],\n",
      "                              'citeRegEx': 'Bengio and Heigold.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Bengio and Heigold.',\n",
      "                              'title': 'Word embeddings for speech recognition',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Yoshua Bengio',\n",
      "                                         'Réjean Ducharme',\n",
      "                                         'Pascal Vincent',\n",
      "                                         'Christian Jauvin'],\n",
      "                              'citeRegEx': 'Bengio et al\\\\.,? \\\\Q2003\\\\E',\n",
      "                              'shortCiteRegEx': 'Bengio et al\\\\.',\n",
      "                              'title': 'A neural probabilistic language model',\n",
      "                              'venue': 'Journal of Machine Learing Research,',\n",
      "                              'year': 2003},\n",
      "                             {'author': ['Jane Bromley',\n",
      "                                         'Isabelle Guyon',\n",
      "                                         'Yann Lecun',\n",
      "                                         'Eduard Säckinger',\n",
      "                                         'Roopak Shah'],\n",
      "                              'citeRegEx': 'Bromley et al\\\\.,? \\\\Q1993\\\\E',\n",
      "                              'shortCiteRegEx': 'Bromley et al\\\\.',\n",
      "                              'title': 'Signature verification using a siamese '\n",
      "                                       'time delay neural network',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 1993},\n",
      "                             {'author': ['Michael A Carlin',\n",
      "                                         'Samuel Thomas',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Hynek Hermansky'],\n",
      "                              'citeRegEx': 'Carlin et al\\\\.,? \\\\Q2011\\\\E',\n",
      "                              'shortCiteRegEx': 'Carlin et al\\\\.',\n",
      "                              'title': 'Rapid evaluation of speech '\n",
      "                                       'representations for spoken term '\n",
      "                                       'discovery',\n",
      "                              'venue': 'In Proc. Interspeech,',\n",
      "                              'year': 2011},\n",
      "                             {'author': ['Guoguo Chen',\n",
      "                                         'Carolina Parada',\n",
      "                                         'Tara N Sainath'],\n",
      "                              'citeRegEx': 'Chen et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Chen et al\\\\.',\n",
      "                              'title': 'Query-by-example keyword spotting '\n",
      "                                       'using long short-term memory networks',\n",
      "                              'venue': 'In Proc. ICASSP,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Sumit Chopra',\n",
      "                                         'Raia Hadsell',\n",
      "                                         'Yann LeCun'],\n",
      "                              'citeRegEx': 'Chopra et al\\\\.,? \\\\Q2005\\\\E',\n",
      "                              'shortCiteRegEx': 'Chopra et al\\\\.',\n",
      "                              'title': 'Learning a similarity metric '\n",
      "                                       'discriminatively, with application to '\n",
      "                                       'face verification',\n",
      "                              'venue': 'In IEEE Computer Society Conf. '\n",
      "                                       'Computer Vision and Pattern '\n",
      "                                       'Recognition,',\n",
      "                              'year': 2005},\n",
      "                             {'author': ['Yu-An Chung',\n",
      "                                         'Chao-Chung Wu',\n",
      "                                         'Chia-Hao Shen',\n",
      "                                         'Hung-Yi Lee'],\n",
      "                              'citeRegEx': 'Chung et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Chung et al\\\\.',\n",
      "                              'title': 'Unsupervised learning of audio segment '\n",
      "                                       'representations using '\n",
      "                                       'sequence-to-sequence recurrent neural '\n",
      "                                       'networks',\n",
      "                              'venue': 'In Proc. Interspeech,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Scott Deerwester',\n",
      "                                         'Susan T Dumais',\n",
      "                                         'George W Furnas',\n",
      "                                         'Thomas K Landauer',\n",
      "                                         'Richard Harshman'],\n",
      "                              'citeRegEx': 'Deerwester et al\\\\.,? \\\\Q1990\\\\E',\n",
      "                              'shortCiteRegEx': 'Deerwester et al\\\\.',\n",
      "                              'title': 'Indexing by latent semantic analysis',\n",
      "                              'venue': 'Journal of the American society for '\n",
      "                                       'information science,',\n",
      "                              'year': 1990},\n",
      "                             {'author': ['Lev Finkelstein',\n",
      "                                         'Evgeniy Gabrilovich',\n",
      "                                         'Yossi Matias',\n",
      "                                         'Ehud Rivlin',\n",
      "                                         'Zach Solan',\n",
      "                                         'Gadi Wolfman',\n",
      "                                         'Eytan Ruppin'],\n",
      "                              'citeRegEx': 'Finkelstein et al\\\\.,? \\\\Q2001\\\\E',\n",
      "                              'shortCiteRegEx': 'Finkelstein et al\\\\.',\n",
      "                              'title': 'Placing search in context: The concept '\n",
      "                                       'revisited',\n",
      "                              'venue': 'In Proceedings of the 10th '\n",
      "                                       'international conference on World Wide '\n",
      "                                       'Web,',\n",
      "                              'year': 2001},\n",
      "                             {'author': ['Jonathan G Fiscus',\n",
      "                                         'Jerome Ajot',\n",
      "                                         'John S Garofolo',\n",
      "                                         'George Doddingtion'],\n",
      "                              'citeRegEx': 'Fiscus et al\\\\.,? \\\\Q2007\\\\E',\n",
      "                              'shortCiteRegEx': 'Fiscus et al\\\\.',\n",
      "                              'title': 'Results of the 2006 spoken term '\n",
      "                                       'detection evaluation',\n",
      "                              'venue': 'In Proc. SIGIR,',\n",
      "                              'year': 2007},\n",
      "                             {'author': ['Jort F Gemmeke',\n",
      "                                         'Tuomas Virtanen',\n",
      "                                         'Antti Hurmalainen'],\n",
      "                              'citeRegEx': 'Gemmeke et al\\\\.,? \\\\Q2011\\\\E',\n",
      "                              'shortCiteRegEx': 'Gemmeke et al\\\\.',\n",
      "                              'title': 'Exemplar-based sparse representations '\n",
      "                                       'for noise robust automatic speech '\n",
      "                                       'recognition',\n",
      "                              'venue': 'IEEE Transactions on Acoustics, '\n",
      "                                       'Speech, and Language Processing,',\n",
      "                              'year': 2011},\n",
      "                             {'author': ['Sahar Ghannay',\n",
      "                                         'Yannick Esteve',\n",
      "                                         'Nathalie Camelin',\n",
      "                                         'Paul Deleglise'],\n",
      "                              'citeRegEx': 'Ghannay et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Ghannay et al\\\\.',\n",
      "                              'title': 'Evaluation of acoustic word embeddings',\n",
      "                              'venue': 'In Proc. ACL Workshop on Evaluating '\n",
      "                                       'Vector-Space Representations for NLP,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['John J Godfrey',\n",
      "                                         'Edward C Holliman',\n",
      "                                         'Jane McDaniel'],\n",
      "                              'citeRegEx': 'Godfrey et al\\\\.,? \\\\Q1992\\\\E',\n",
      "                              'shortCiteRegEx': 'Godfrey et al\\\\.',\n",
      "                              'title': 'SWITCHBOARD: Telephone speech corpus '\n",
      "                                       'for research and development',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 1992},\n",
      "                             {'author': ['Alex Graves',\n",
      "                                         'Abdel rahman Mohamed',\n",
      "                                         'Geoffrey Hinton'],\n",
      "                              'citeRegEx': 'Graves et al\\\\.,? \\\\Q2013\\\\E',\n",
      "                              'shortCiteRegEx': 'Graves et al\\\\.',\n",
      "                              'title': 'Speech recognition with deep recurrent '\n",
      "                                       'neural networks',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2013},\n",
      "                             {'author': ['Raia Hadsell',\n",
      "                                         'Sumit Chopra',\n",
      "                                         'Yann LeCun'],\n",
      "                              'citeRegEx': 'Hadsell et al\\\\.,? \\\\Q2006\\\\E',\n",
      "                              'shortCiteRegEx': 'Hadsell et al\\\\.',\n",
      "                              'title': 'Dimensionality reduction by learning '\n",
      "                                       'an invariant mapping',\n",
      "                              'venue': 'In IEEE Computer Society Conf. '\n",
      "                                       'Computer Vision and Pattern '\n",
      "                                       'Recognition,',\n",
      "                              'year': 2006},\n",
      "                             {'author': ['David Harwath', 'James Glass'],\n",
      "                              'citeRegEx': 'Harwath and Glass.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Harwath and Glass.',\n",
      "                              'title': 'Deep multimodal semantic embeddings '\n",
      "                                       'for speech and images',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Automatic '\n",
      "                                       'Speech Recognition and Understanding '\n",
      "                                       '(ASRU),',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['David Harwath', 'James R Glass'],\n",
      "                              'citeRegEx': 'Harwath and Glass.,? \\\\Q2017\\\\E',\n",
      "                              'shortCiteRegEx': 'Harwath and Glass.',\n",
      "                              'title': 'Learning word-like units from joint '\n",
      "                                       'audio-visual analysis',\n",
      "                              'venue': 'arXiv preprint arXiv:1701.07481,',\n",
      "                              'year': 2017},\n",
      "                             {'author': ['David Harwath',\n",
      "                                         'Antonio Torralba',\n",
      "                                         'James Glass'],\n",
      "                              'citeRegEx': 'Harwath et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Harwath et al\\\\.',\n",
      "                              'title': 'Unsupervised learning of spoken '\n",
      "                                       'language with visual context',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Karl Moritz Hermann', 'Phil Blunsom'],\n",
      "                              'citeRegEx': 'Hermann and Blunsom.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Hermann and Blunsom.',\n",
      "                              'title': 'Multilingual distributed '\n",
      "                                       'representations without word alignment',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Felix Hill',\n",
      "                                         'Roi Reichart',\n",
      "                                         'Anna Korhonen'],\n",
      "                              'citeRegEx': 'Hill et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Hill et al\\\\.',\n",
      "                              'title': 'SimLex-999: Evaluating semantic models '\n",
      "                                       'with (genuine) similarity estimation',\n",
      "                              'venue': 'Computational Linguistics,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Sepp Hochreiter',\n",
      "                                         'Jürgen Schmidhuber'],\n",
      "                              'citeRegEx': 'Hochreiter and Schmidhuber.,? '\n",
      "                                           '\\\\Q1997\\\\E',\n",
      "                              'shortCiteRegEx': 'Hochreiter and Schmidhuber.',\n",
      "                              'title': 'Long short-term memory',\n",
      "                              'venue': 'Neural Computation,',\n",
      "                              'year': 1997},\n",
      "                             {'author': ['Baotian Hu',\n",
      "                                         'Zhengdong Lu',\n",
      "                                         'Hang Li',\n",
      "                                         'Qingcai Chen'],\n",
      "                              'citeRegEx': 'Hu et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Hu et al\\\\.',\n",
      "                              'title': 'Convolutional neural network '\n",
      "                                       'architectures for matching natural '\n",
      "                                       'language sentences',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Mohit Iyyer',\n",
      "                                         'Varun Manjunatha',\n",
      "                                         'Jordan Boyd-Graber',\n",
      "                                         'Hal Daumé III'],\n",
      "                              'citeRegEx': 'Iyyer et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Iyyer et al\\\\.',\n",
      "                              'title': 'Deep unordered composition rivals '\n",
      "                                       'syntactic methods for text '\n",
      "                                       'classification',\n",
      "                              'venue': 'In Proc. Association for Computational '\n",
      "                                       'Linguistics,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Herman Kamper',\n",
      "                                         'Micah Elsner',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Sharon J. Goldwater'],\n",
      "                              'citeRegEx': 'Kamper et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Kamper et al\\\\.',\n",
      "                              'title': 'Unsupervised neural network based '\n",
      "                                       'feature extraction using weak top-down '\n",
      "                                       'constraints',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Herman Kamper',\n",
      "                                         'Weiran Wang',\n",
      "                                         'Karen Livescu'],\n",
      "                              'citeRegEx': 'Kamper et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Kamper et al\\\\.',\n",
      "                              'title': 'Deep convolutional acoustic word '\n",
      "                                       'embeddings using word-pair side '\n",
      "                                       'information',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Diederik Kingma', 'Jimmy Ba'],\n",
      "                              'citeRegEx': 'Kingma and Ba.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Kingma and Ba.',\n",
      "                              'title': 'ADAM: A method for stochastic '\n",
      "                                       'optimization',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Ryan Kiros',\n",
      "                                         'Yukun Zhu',\n",
      "                                         'Ruslan R Salakhutdinov',\n",
      "                                         'Richard Zemel',\n",
      "                                         'Raquel Urtasun',\n",
      "                                         'Antonio Torralba',\n",
      "                                         'Sanja Fidler'],\n",
      "                              'citeRegEx': 'Kiros et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Kiros et al\\\\.',\n",
      "                              'title': 'Skip-thought vectors',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Keith Levin',\n",
      "                                         'Katharine Henry',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Karen Livescu'],\n",
      "                              'citeRegEx': 'Levin et al\\\\.,? \\\\Q2013\\\\E',\n",
      "                              'shortCiteRegEx': 'Levin et al\\\\.',\n",
      "                              'title': 'Fixed-dimensional acoustic embeddings '\n",
      "                                       'of variable-length segments in '\n",
      "                                       'low-resource settings',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Automatic '\n",
      "                                       'Speech Recognition and Understanding '\n",
      "                                       '(ASRU),',\n",
      "                              'year': 2013},\n",
      "                             {'author': ['Keith Levin',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Benjamin Van Durme'],\n",
      "                              'citeRegEx': 'Levin et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Levin et al\\\\.',\n",
      "                              'title': 'Segmental acoustic indexing for zero '\n",
      "                                       'resource keyword search',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Andrew L Maas',\n",
      "                                         'Stephen D Miller',\n",
      "                                         'Tyler M O’neil',\n",
      "                                         'Andrew Y Ng',\n",
      "                                         'Patrick Nguyen'],\n",
      "                              'citeRegEx': 'Maas et al\\\\.,? \\\\Q2012\\\\E',\n",
      "                              'shortCiteRegEx': 'Maas et al\\\\.',\n",
      "                              'title': 'Word-level acoustic modeling with '\n",
      "                                       'convolutional vector regression',\n",
      "                              'venue': 'In Proc. ICML Workshop on '\n",
      "                                       'Representation Learning,',\n",
      "                              'year': 2012},\n",
      "                             {'author': ['Tomas Mikolov',\n",
      "                                         'Ilya Sutskever',\n",
      "                                         'Kai Chen',\n",
      "                                         'Greg S Corrado',\n",
      "                                         'Jeff Dean'],\n",
      "                              'citeRegEx': 'Mikolov et al\\\\.,? \\\\Q2013\\\\E',\n",
      "                              'shortCiteRegEx': 'Mikolov et al\\\\.',\n",
      "                              'title': 'Distributed representations of words '\n",
      "                                       'and phrases and their compositionality',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2013},\n",
      "                             {'author': ['Andriy Mnih', 'Geoffrey Hinton'],\n",
      "                              'citeRegEx': 'Mnih and Hinton.,? \\\\Q2007\\\\E',\n",
      "                              'shortCiteRegEx': 'Mnih and Hinton.',\n",
      "                              'title': 'Three new graphical models for '\n",
      "                                       'statistical language modelling',\n",
      "                              'venue': 'In ICML,',\n",
      "                              'year': 2007},\n",
      "                             {'author': ['Jiquan Ngiam',\n",
      "                                         'Aditya Khosla',\n",
      "                                         'Mingyu Kim',\n",
      "                                         'Juhan Nam',\n",
      "                                         'Honglak Lee',\n",
      "                                         'Andrew Ng'],\n",
      "                              'citeRegEx': 'Ngiam et al\\\\.,? \\\\Q2011\\\\E',\n",
      "                              'shortCiteRegEx': 'Ngiam et al\\\\.',\n",
      "                              'title': 'Multimodal deep learning',\n",
      "                              'venue': 'In ICML, pp',\n",
      "                              'year': 2011},\n",
      "                             {'author': ['Jeffrey Pennington',\n",
      "                                         'Richard Socher',\n",
      "                                         'Christopher D Manning'],\n",
      "                              'citeRegEx': 'Pennington et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Pennington et al\\\\.',\n",
      "                              'title': 'GloVe: Global vectors for word '\n",
      "                                       'representation',\n",
      "                              'venue': 'In Proc. Conference on Empirical '\n",
      "                                       'Methods in Natural Language '\n",
      "                                       'Processing,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Shane Settle', 'Karen Livescu'],\n",
      "                              'citeRegEx': 'Settle and Livescu.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Settle and Livescu.',\n",
      "                              'title': 'Discriminative acoustic word '\n",
      "                                       'embeddings: Recurrent neural '\n",
      "                                       'network-based approaches',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Spoken '\n",
      "                                       'Language Technology (SLT),',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Richard Socher',\n",
      "                                         'Andrej Karpathy',\n",
      "                                         'Quoc V Le',\n",
      "                                         'Christopher D Manning',\n",
      "                                         'Andrew Y Ng'],\n",
      "                              'citeRegEx': 'Socher et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Socher et al\\\\.',\n",
      "                              'title': 'Grounded compositional semantics for '\n",
      "                                       'finding and describing images with '\n",
      "                                       'sentences',\n",
      "                              'venue': 'Transactions of the Association for '\n",
      "                                       'Computational Linguistics,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Kihyuk Sohn',\n",
      "                                         'Wenling Shang',\n",
      "                                         'Honglak Lee'],\n",
      "                              'citeRegEx': 'Sohn et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Sohn et al\\\\.',\n",
      "                              'title': 'Improved multimodal deep learning with '\n",
      "                                       'variation of information',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Nitish Srivastava',\n",
      "                                         'Ruslan Salakhutdinov'],\n",
      "                              'citeRegEx': 'Srivastava and Salakhutdinov.,? '\n",
      "                                           '\\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Srivastava and Salakhutdinov.',\n",
      "                              'title': 'Multimodal learning with deep '\n",
      "                                       'boltzmann machines',\n",
      "                              'venue': 'Journal of Machine Learing Research,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Gabriel Synnaeve',\n",
      "                                         'Thomas Schatz',\n",
      "                                         'Emmanuel Dupoux'],\n",
      "                              'citeRegEx': 'Synnaeve et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Synnaeve et al\\\\.',\n",
      "                              'title': 'Phonetics embedding learning with side '\n",
      "                                       'information',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Spoken '\n",
      "                                       'Language Technology (SLT),',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Laurens J.P. van der Maaten',\n",
      "                                         'Geoffrey E. Hinton'],\n",
      "                              'citeRegEx': 'Maaten and Hinton.,? \\\\Q2008\\\\E',\n",
      "                              'shortCiteRegEx': 'Maaten and Hinton.',\n",
      "                              'title': 'Visualizing data using t-SNE',\n",
      "                              'venue': 'Journal of Machine Learing Research,',\n",
      "                              'year': 2008},\n",
      "                             {'author': ['Ivan Vendrov',\n",
      "                                         'Ryan Kiros',\n",
      "                                         'Sanja Fidler',\n",
      "                                         'Raquel Urtasun'],\n",
      "                              'citeRegEx': 'Vendrov et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Vendrov et al\\\\.',\n",
      "                              'title': 'Order-embeddings of images and '\n",
      "                                       'language',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Weiran Wang',\n",
      "                                         'Raman Arora',\n",
      "                                         'Karen Livescu',\n",
      "                                         'Jeff Bilmes'],\n",
      "                              'citeRegEx': 'Wang et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Wang et al\\\\.',\n",
      "                              'title': 'On deep multi-view representation '\n",
      "                                       'learning',\n",
      "                              'venue': 'In ICML, pp',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['John Wieting',\n",
      "                                         'Mohit Bansal',\n",
      "                                         'Kevin Gimpel',\n",
      "                                         'Karen Livescu'],\n",
      "                              'citeRegEx': 'Wieting et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Wieting et al\\\\.',\n",
      "                              'title': 'Towards universal paraphrastic '\n",
      "                                       'sentence embeddings',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2016}],\n",
      "              'sections': [{'heading': '1 INTRODUCTION',\n",
      "                            'text': 'Word embeddings—continuous-valued vector '\n",
      "                                    'representations of words—are an almost '\n",
      "                                    'ubiquitous component of recent natural '\n",
      "                                    'language processing (NLP) research. Word '\n",
      "                                    'embeddings can be learned using spectral '\n",
      "                                    'methods (Deerwester et al., 1990) or, '\n",
      "                                    'more commonly in recent work, via neural '\n",
      "                                    'networks (Bengio et al., 2003; Mnih & '\n",
      "                                    'Hinton, 2007; Mikolov et al., 2013; '\n",
      "                                    'Pennington et al., 2014). Word embeddings '\n",
      "                                    'can also be composed to form embeddings '\n",
      "                                    'of phrases, sentences, or documents '\n",
      "                                    '(Socher et al., 2014; Kiros et al., 2015; '\n",
      "                                    'Wieting et al., 2016; Iyyer et al., '\n",
      "                                    '2015).\\n'\n",
      "                                    'In typical NLP applications, such '\n",
      "                                    'embeddings are intended to represent the '\n",
      "                                    'semantics of the corresponding '\n",
      "                                    'words/sequences. In contrast, embeddings '\n",
      "                                    'that represent the way a word or sequence '\n",
      "                                    'sounds are rarely considered. In this '\n",
      "                                    'work we address this problem, starting '\n",
      "                                    'with embeddings of individual words. Such '\n",
      "                                    'embeddings could be useful for tasks like '\n",
      "                                    'spoken term detection (Fiscus et al., '\n",
      "                                    '2007), spoken query-by-example search '\n",
      "                                    '(Anguera et al., 2014), or even speech '\n",
      "                                    'recognition using a whole-word approach '\n",
      "                                    '(Gemmeke et al., 2011; Bengio & Heigold, '\n",
      "                                    '2014). In tasks that involve comparing '\n",
      "                                    'speech segments to each other, vector '\n",
      "                                    'embeddings can allow more efficient and '\n",
      "                                    'more accurate distance computation than '\n",
      "                                    'sequence-based approaches such as dynamic '\n",
      "                                    'time warping (Levin et al., 2013, 2015; '\n",
      "                                    'Kamper et al., 2016; Settle & Livescu, '\n",
      "                                    '2016; Chung et al., 2016).\\n'\n",
      "                                    'We consider the problem of learning '\n",
      "                                    'vector representations of acoustic '\n",
      "                                    'sequences and orthographic (character) '\n",
      "                                    'sequences corresponding to single words, '\n",
      "                                    'such that the learned embeddings '\n",
      "                                    'represent the way the word sounds. We '\n",
      "                                    'take a multi-view approach, where we '\n",
      "                                    'jointly learn the embeddings for '\n",
      "                                    'character and acoustic sequences. We '\n",
      "                                    'consider several contrastive losses, '\n",
      "                                    'based on learning from pairs of matched '\n",
      "                                    'acoustic-orthographic examples and '\n",
      "                                    'randomly drawn mismatched pairs. The '\n",
      "                                    'losses correspond to different goals for '\n",
      "                                    'learning such embeddings; for example, we '\n",
      "                                    'might want the embeddings of two '\n",
      "                                    'waveforms to be close when they '\n",
      "                                    'correspond to the same word and far when '\n",
      "                                    'they correspond to different ones, or we '\n",
      "                                    'might want the distances between '\n",
      "                                    'embeddings to correspond to some '\n",
      "                                    'ground-truth orthographic edit distance.\\n'\n",
      "                                    'One of the useful properties of this '\n",
      "                                    'multi-view approach is that, unlike '\n",
      "                                    'earlier work on acoustic word embeddings, '\n",
      "                                    'it produces both acoustic and '\n",
      "                                    'orthographic embeddings that can be '\n",
      "                                    'directly compared. This makes it possible '\n",
      "                                    'to use the same learned embeddings for '\n",
      "                                    'multiple single-view and cross-view '\n",
      "                                    'tasks. Our multi-view embeddings produce '\n",
      "                                    'improved results over earlier work on '\n",
      "                                    'acoustic word discrimination, as well as '\n",
      "                                    'encouraging results on cross-view '\n",
      "                                    'discrimination and word similarity.1'},\n",
      "                           {'heading': '2 OUR APPROACH',\n",
      "                            'text': 'In this section, we first introduce our '\n",
      "                                    'approach for learning acoustic word '\n",
      "                                    'embeddings in a multiview setting, after '\n",
      "                                    'briefly reviewing related approaches to '\n",
      "                                    'put ours in context. We then discuss the '\n",
      "                                    'particular neural network architecture we '\n",
      "                                    'use, based on bidirectional long '\n",
      "                                    'short-term memory (LSTM) networks '\n",
      "                                    '(Hochreiter & Schmidhuber, 1997).'},\n",
      "                           {'heading': '2.1 MULTI-VIEW LEARNING OF ACOUSTIC '\n",
      "                                       'WORD EMBEDDINGS',\n",
      "                            'text': 'Previous approaches have focused on '\n",
      "                                    'learning acoustic word embeddings in a '\n",
      "                                    '“single-view” setting. In the simplest '\n",
      "                                    'approach, one uses supervision of the '\n",
      "                                    'form “acoustic segment x is an instance '\n",
      "                                    'of the word y”, and trains the embedding '\n",
      "                                    'to be discriminative of the word '\n",
      "                                    'identity. Formally, given a dataset of '\n",
      "                                    'paired acoustic segments and word labels '\n",
      "                                    '{(xi, yi)}Ni=1, this approach solves the '\n",
      "                                    'following optimization:\\n'\n",
      "                                    'min f,h\\n'\n",
      "                                    'objclassify := 1\\n'\n",
      "                                    'N N∑ i ` (h(f(xi)),yi) , (1)\\n'\n",
      "                                    'where network f maps an acoustic segment '\n",
      "                                    'into a fixed-dimensional feature '\n",
      "                                    'vector/embedding, h is a classifier that '\n",
      "                                    'predicts the corresponding word label '\n",
      "                                    'from the label set of the training data, '\n",
      "                                    'and the loss ` measures the discrepancy '\n",
      "                                    'between the prediction and ground-truth '\n",
      "                                    'word label (one can use any multi-class '\n",
      "                                    'classification loss here, and a typical '\n",
      "                                    'choice is the cross-entropy loss where h '\n",
      "                                    'has a softmax top layer). The two '\n",
      "                                    'networks f and h are trained jointly. '\n",
      "                                    'Equivalently, one could consider the '\n",
      "                                    'composition h(f(x)) as a classifier '\n",
      "                                    'network, and use any intermediate layer’s '\n",
      "                                    'activations as the features. We refer to '\n",
      "                                    'the objective in (1) as the “classifier '\n",
      "                                    'network” objective, which has been used '\n",
      "                                    'in several prior studies on acoustic word '\n",
      "                                    'embeddings (Bengio & Heigold, 2014; '\n",
      "                                    'Kamper et al., 2016; Settle & Livescu, '\n",
      "                                    '2016).\\n'\n",
      "                                    'This objective, however, is not ideal for '\n",
      "                                    'learning acoustic word embeddings. This '\n",
      "                                    'is because the set of possible word '\n",
      "                                    'labels is huge, and we may not have '\n",
      "                                    'enough instances of each label to train a '\n",
      "                                    'good classifier. In downstream tasks, we '\n",
      "                                    'may encounter acoustic segments of words '\n",
      "                                    'that did not appear in the embedding '\n",
      "                                    'training set, and it is not clear that '\n",
      "                                    'the classifier-based embeddings will have '\n",
      "                                    'reasonable behavior on previously unseen '\n",
      "                                    'words.\\n'\n",
      "                                    'An alternative approach, based on Siamese '\n",
      "                                    'networks (Bromley et al., 1993), uses '\n",
      "                                    'supervision of the form “segment x1 is '\n",
      "                                    'similar to segment x2, and is not similar '\n",
      "                                    'to segment x3”, where two segments are '\n",
      "                                    'considered similar if they have the same '\n",
      "                                    'word label and dissimilar otherwise. '\n",
      "                                    'Models based on Siamese networks have '\n",
      "                                    'been used for a variety of representation '\n",
      "                                    'learning problems in NLP (Hu et al., '\n",
      "                                    '2014; Wieting et al., 2016), vision '\n",
      "                                    '(Hadsell et al., 2006), and speech '\n",
      "                                    '(Synnaeve et al., 2014; Kamper et al., '\n",
      "                                    '2015) including acoustic word embeddings '\n",
      "                                    '(Kamper et al., 2016; Settle & Livescu, '\n",
      "                                    '2016). A typical objective in this '\n",
      "                                    'category enforces that the distance '\n",
      "                                    'between (x1, x3) is larger than the '\n",
      "                                    'distance between (x1, x2) by some '\n",
      "                                    'margin:\\n'\n",
      "                                    'min f\\n'\n",
      "                                    'objsiamese := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x1i ), f(x 2 i '\n",
      "                                    ') ) − dis ( f(x1i ), f(x 3 i ) )) , (2)\\n'\n",
      "                                    'where the network f extracts the '\n",
      "                                    'fixed-dimensional embedding, the distance '\n",
      "                                    'function dis (·, ·) measures the distance '\n",
      "                                    'between the two embedding vectors, andm > '\n",
      "                                    '0 is the margin parameter. The term '\n",
      "                                    '“Siamese” (Bromley et al., 1993; Chopra '\n",
      "                                    'et al., 2005) refers to the fact that the '\n",
      "                                    'triplet (x1, x2, x3) share the same '\n",
      "                                    'embedding network f .\\n'\n",
      "                                    'Unlike the classification-based loss, the '\n",
      "                                    'Siamese network loss does not enforce '\n",
      "                                    'hard decisions on the label of each '\n",
      "                                    'segment. Instead it tries to learn '\n",
      "                                    'embeddings that respect distances between '\n",
      "                                    'word\\n'\n",
      "                                    '1Our tensorflow implementation is '\n",
      "                                    'available at '\n",
      "                                    'https://github.com/opheadacheh/Multi-view-neural-acoustic-words-embeddings\\n'\n",
      "                                    'pairs, which can be helpful for dealing '\n",
      "                                    'with unseen words. The Siamese network '\n",
      "                                    'approach also uses more examples in '\n",
      "                                    'training, as one can easily generate many '\n",
      "                                    'more triplets than (segment, label) '\n",
      "                                    'pairs, and it is not limited to those '\n",
      "                                    'labels that occur a sufficient number of '\n",
      "                                    'times in the training set.\\n'\n",
      "                                    'The above approaches treat the word '\n",
      "                                    'labels as discrete classes, which ignores '\n",
      "                                    'the similarity between different words, '\n",
      "                                    'and does not take advantage of the more '\n",
      "                                    'complex information contained in the '\n",
      "                                    'character sequences corresponding to word '\n",
      "                                    'labels. The orthography naturally '\n",
      "                                    'reflects some aspects of similarity '\n",
      "                                    'between the words’ pronunciations, which '\n",
      "                                    'should also be reflected in the acoustic '\n",
      "                                    'embeddings. One way to learn features '\n",
      "                                    'from multiple sources of complementary '\n",
      "                                    'information is using a multi-view '\n",
      "                                    'representation learning setting. We take '\n",
      "                                    'this approach, and consider the acoustic '\n",
      "                                    'segment and the character sequence to be '\n",
      "                                    'two different views of the pronunciation '\n",
      "                                    'of the word.\\n'\n",
      "                                    'While many deep multi-view learning '\n",
      "                                    'objectives are applicable (Ngiam et al., '\n",
      "                                    '2011; Srivastava & Salakhutdinov, 2014; '\n",
      "                                    'Sohn et al., 2014; Wang et al., 2015), we '\n",
      "                                    'consider the multi-view contrastive loss '\n",
      "                                    'objective of (Hermann & Blunsom, 2014), '\n",
      "                                    'which is simple to optimize and implement '\n",
      "                                    'and performs well in practice. In this '\n",
      "                                    'algorithm, we embed acoustic segments x '\n",
      "                                    'by a network f and character label '\n",
      "                                    'sequences c by another network g into a '\n",
      "                                    'common space, and use weak supervision of '\n",
      "                                    'the form “for paired segment x+ and its '\n",
      "                                    'character label sequence c+, the distance '\n",
      "                                    'between their embedding is much smaller '\n",
      "                                    'than the distance between embeddings of '\n",
      "                                    'x+ and an unmatched character label '\n",
      "                                    'sequence c−”. Formally, we optimize the '\n",
      "                                    'following objective with such '\n",
      "                                    'supervision:\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj0 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( f(x+i ), g(c − i ) )) , (3)\\n'\n",
      "                                    'where c−i is a negative character label '\n",
      "                                    'sequence of x + i to be contrasted with '\n",
      "                                    'the positive/correct character sequence '\n",
      "                                    'c+i , and m is the margin parameter. In '\n",
      "                                    'this paper we use the cosine distance,\\n'\n",
      "                                    'dis (a,b) = 1− 〈\\n'\n",
      "                                    'a ‖a‖ , b ‖b‖\\n'\n",
      "                                    '〉 .2\\n'\n",
      "                                    'Note that in the multi-view setting, we '\n",
      "                                    'have multiple ways of generating triplets '\n",
      "                                    'that contain one positive pair and one '\n",
      "                                    'negative pair each. Below are the other '\n",
      "                                    'three objectives we explore in this '\n",
      "                                    'paper:\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj1 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( g(c+i ), g(c − i ) )) , (4)\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj2 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( f(x−i ), g(c + i ) )) , (5)\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj3 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( f(x+i ), f(x − i ) )) . (6)\\n'\n",
      "                                    'x−i in (5) and (6) refers to a negative '\n",
      "                                    'acoustic feature sequence, that is one '\n",
      "                                    'with a different label from x+i . We note '\n",
      "                                    'that obj\\n'\n",
      "                                    '1 and obj3 contain distances between '\n",
      "                                    'same-view embeddings, and are less '\n",
      "                                    'thoroughly explored in the literature. We '\n",
      "                                    'will also consider combinations of obj0 '\n",
      "                                    'through obj3.\\n'\n",
      "                                    'Finally, thus far we have considered '\n",
      "                                    'losses that do not explicitly take into '\n",
      "                                    'account the degree of difference between '\n",
      "                                    'the positive and negative pairs (although '\n",
      "                                    'the learned embeddings may implicitly '\n",
      "                                    'learn this through the relationship '\n",
      "                                    'between sequences in the two views). We '\n",
      "                                    'also consider a costsensitive objective '\n",
      "                                    'designed to explicitly arrange the '\n",
      "                                    'embedding space such that word similarity '\n",
      "                                    'is respected. In (3), instead of a fixed '\n",
      "                                    'margin m, we use:\\n'\n",
      "                                    'm(c+, c−) := mmax · min (tmax, editdis(c\\n'\n",
      "                                    '+, c−))\\n'\n",
      "                                    'tmax , (7)\\n'\n",
      "                                    'where tmax > 0 is a threshold for edit '\n",
      "                                    'distances (all edit distances above tmax '\n",
      "                                    'are considered equally bad), and mmax is '\n",
      "                                    'the maximum margin we impose. The margin '\n",
      "                                    'is set to mmax if the edit distance '\n",
      "                                    'between two character sequences is above '\n",
      "                                    'tmax; otherwise it scales linearly with '\n",
      "                                    'the edit distance editdis(c+, c−)). We '\n",
      "                                    'use the Levenshtein distance as the edit '\n",
      "                                    'distance. Here we explore the '\n",
      "                                    'costsensitive margin with obj0, but it '\n",
      "                                    'could in principle be used with other '\n",
      "                                    'objectives as well.\\n'\n",
      "                                    '2In experiments, we use the unit-length '\n",
      "                                    'vector a‖a‖ as the embedding. It tends to '\n",
      "                                    'perform better than f(x) and more '\n",
      "                                    'directly reflects the cosine similarity. '\n",
      "                                    'This is equivalent to adding a nonlinear '\n",
      "                                    'normalization layer on top of f .'},\n",
      "                           {'heading': '2.2 RECURRENT NEURAL NETWORK '\n",
      "                                       'ARCHITECTURE',\n",
      "                            'text': 'Since the inputs of both views have a '\n",
      "                                    'sequential structure, we implement both f '\n",
      "                                    'and g with recurrent neural networks and '\n",
      "                                    'in particular long-short term memory '\n",
      "                                    'networks (LSTMs). Recurrent neural '\n",
      "                                    'networks are the state-of-the-art models '\n",
      "                                    'for a number of speech tasks including '\n",
      "                                    'speech recognition Graves et al. (2013), '\n",
      "                                    'and LSTM-based acoustic word embeddings '\n",
      "                                    'have produced the best results on one of '\n",
      "                                    'the tasks in our experiments (Settle & '\n",
      "                                    'Livescu, 2016).\\n'\n",
      "                                    'As shown in Figure 1, our f and g are '\n",
      "                                    'produced by multi-layer (stacked) '\n",
      "                                    'bidirectional LSTMs. The inputs can be '\n",
      "                                    'any frame-level acoustic feature '\n",
      "                                    'representation and vector representation '\n",
      "                                    'of the characters in the orthographic '\n",
      "                                    'input. At each layer, two LSTM cells '\n",
      "                                    'process the input sequence from left to '\n",
      "                                    'right and from right to left '\n",
      "                                    'respectively. At intermediate layers, the '\n",
      "                                    'outputs of the two LSTMs at each time '\n",
      "                                    'step are concatenated to form the input '\n",
      "                                    'sequence to the next layer. At the top '\n",
      "                                    'layer, the last time step outputs of the '\n",
      "                                    'two LSTMs are concatenated to form a '\n",
      "                                    'fixed-dimensional embedding of the view, '\n",
      "                                    'and the embeddings are then used to '\n",
      "                                    'calculate the cosine distances in our '\n",
      "                                    'objectives.'},\n",
      "                           {'heading': '3 RELATED WORK',\n",
      "                            'text': 'We are aware of no prior work on '\n",
      "                                    'multi-view learning of acoustic and '\n",
      "                                    'character-based word embeddings. However, '\n",
      "                                    'acoustic word embeddings learned in other '\n",
      "                                    'ways have recently begun to be studied. '\n",
      "                                    'Levin et al. (2013) proposed an approach '\n",
      "                                    'for embedding an arbitrary-length segment '\n",
      "                                    'of speech as a fixed-dimensional vector, '\n",
      "                                    'based on representing each word as a '\n",
      "                                    'vector of dynamic time warping (DTW) '\n",
      "                                    'distances to a set of template words. '\n",
      "                                    'This approach produced improved '\n",
      "                                    'performance on a word discrimination task '\n",
      "                                    'compared to using raw DTW distances, and '\n",
      "                                    'was later also applied successfully for a '\n",
      "                                    'query-by-example task (Levin et al., '\n",
      "                                    '2015). One disadvantage of this approach '\n",
      "                                    'is that, while DTW handles the issue of '\n",
      "                                    'variable sequence lengths, it is '\n",
      "                                    'computationally costly and involves a '\n",
      "                                    'number of DTW parameters that are not '\n",
      "                                    'learned.\\n'\n",
      "                                    'Kamper et al. (2016) and Settle & Livescu '\n",
      "                                    '(2016) later improved on Levin et al.’s '\n",
      "                                    'word discrimination results using '\n",
      "                                    'convolutional neural networks (CNNs) and '\n",
      "                                    'recurrent neural networks (RNNs) trained '\n",
      "                                    'with either a classification or '\n",
      "                                    'contrastive loss. Bengio & Heigold (2014) '\n",
      "                                    'trained convolutional neural network '\n",
      "                                    '(CNN)-based acoustic word embeddings for '\n",
      "                                    'rescoring the outputs of a speech '\n",
      "                                    'recognizer, using a loss combining '\n",
      "                                    'classification and ranking criteria. Maas '\n",
      "                                    'et al. (2012) trained a CNN to predict a '\n",
      "                                    'semantic word embedding from an acoustic '\n",
      "                                    'segment, and used the resulting '\n",
      "                                    'embeddings as features in a segmental '\n",
      "                                    'word-level speech recognizer. Harwath and '\n",
      "                                    'Glass Harwath & Glass (2015); Harwath et '\n",
      "                                    'al. (2016); Harwath & Glass (2017) '\n",
      "                                    'jointly trained CNN embeddings of images '\n",
      "                                    'and spoken captions, and showed that '\n",
      "                                    'word-like unit embeddings can be '\n",
      "                                    'extracted from the speech model. CNNs '\n",
      "                                    'require normalizing the duration of the '\n",
      "                                    'input sequences, which has typically been '\n",
      "                                    'done via padding. RNNs, on the other '\n",
      "                                    'hand, are more flexible in dealing with '\n",
      "                                    'very different-length sequences. Chen et '\n",
      "                                    'al. (2015) used long short-term memory '\n",
      "                                    '(LSTM) networks with a classification '\n",
      "                                    'loss to embed acoustic words for a simple '\n",
      "                                    '(single-query) query-by-example search '\n",
      "                                    'task. Chung et al. (2016) learned '\n",
      "                                    'acoustic word embeddings based on '\n",
      "                                    'recurrent neural network (RNN) '\n",
      "                                    'autoencoders, and found that they improve '\n",
      "                                    'over DTW for a word discrimination task '\n",
      "                                    'similar to that of Levin et al. (2013). '\n",
      "                                    'Audhkhasi et al. (2017) learned '\n",
      "                                    'autoencoders for acoustic and written '\n",
      "                                    'words, as well as a model for comparing '\n",
      "                                    'the two, and applied these to a keyword '\n",
      "                                    'search task.\\n'\n",
      "                                    'Evaluation of acoustic word embeddings in '\n",
      "                                    'downstream tasks such as speech '\n",
      "                                    'recognition and search can be costly, and '\n",
      "                                    'can obscure details of embedding models '\n",
      "                                    'and training approaches. Most evaluations '\n",
      "                                    'have been based on word discrimination – '\n",
      "                                    'the task of determining whether two '\n",
      "                                    'speech segments correspond to the same '\n",
      "                                    'word or not – which can be seen as a '\n",
      "                                    'proxy for query-by-example search (Levin '\n",
      "                                    'et al., 2013; Kamper et al., 2016; Settle '\n",
      "                                    '& Livescu, 2016; Chung et al., 2016). One '\n",
      "                                    'difference between word discrimination '\n",
      "                                    'and search/recognition tasks is that in '\n",
      "                                    'word discrimination the word boundaries '\n",
      "                                    'are given. However, prior work has been '\n",
      "                                    'able to apply results from word '\n",
      "                                    'discrimination Levin et al. (2013) to '\n",
      "                                    'improve a query-by-example system without '\n",
      "                                    'known word boundaries Levin et al. '\n",
      "                                    '(2015), by simply applying their '\n",
      "                                    'embeddings to non-word segments as well.\\n'\n",
      "                                    'The only prior work focused on vector '\n",
      "                                    'embeddings of character sequences '\n",
      "                                    'explicitly aimed at representing their '\n",
      "                                    'acoustic similarity is that of Ghannay et '\n",
      "                                    'al. (2016), who proposed evaluations '\n",
      "                                    'based on nearest-neighbor retrieval, '\n",
      "                                    'phonetic/orthographic similarity '\n",
      "                                    'measures, and homophone disambiguation. '\n",
      "                                    'We use related tasks here, as well as '\n",
      "                                    'acoustic word discrimination for '\n",
      "                                    'comparison with prior work on acoustic '\n",
      "                                    'embeddings.'},\n",
      "                           {'heading': '4 EXPERIMENTS AND RESULTS',\n",
      "                            'text': 'The ultimate goal is to gain improvements '\n",
      "                                    'in speech systems where word-level '\n",
      "                                    'discrimination is needed, such as speech '\n",
      "                                    'recognition and query-by-example search. '\n",
      "                                    'However, in order to focus on the content '\n",
      "                                    'of the embeddings themselves and to more '\n",
      "                                    'quickly compare a variety of models, it '\n",
      "                                    'is desirable to have surrogate tasks that '\n",
      "                                    'serve as intrinsic measures of '\n",
      "                                    'performance. Here we consider three forms '\n",
      "                                    'of evaluation, all based on measuring '\n",
      "                                    'whether cosine distances between learned '\n",
      "                                    'embeddings correspond well to desired '\n",
      "                                    'properties.\\n'\n",
      "                                    'In the first task, acoustic word '\n",
      "                                    'discrimination, we are given a pair of '\n",
      "                                    'acoustic sequences and must decide '\n",
      "                                    'whether they correspond to the same word '\n",
      "                                    'or to different words. This task has been '\n",
      "                                    'used in several prior papers on acoustic '\n",
      "                                    'word embeddings Kamper et al. (2015, '\n",
      "                                    '2016); Chung et al. (2016); Settle & '\n",
      "                                    'Livescu (2016) and is a proxy for '\n",
      "                                    'query-by-example search. For each given '\n",
      "                                    'spoken word pair, we calculate the cosine '\n",
      "                                    'distance between their embeddings. If the '\n",
      "                                    'cosine distance is below a threshold, we '\n",
      "                                    'output “yes” (same word), otherwise we '\n",
      "                                    'output “no” (different words). The '\n",
      "                                    'performance measure is the average '\n",
      "                                    'precision (AP), which is the area under '\n",
      "                                    'the precision-recall curve generated by '\n",
      "                                    'varying the threshold and has a maximum '\n",
      "                                    'value of 1.\\n'\n",
      "                                    'In our multi-view setup, we embed not '\n",
      "                                    'only the acoustic words but also the '\n",
      "                                    'character sequences. This allows us to '\n",
      "                                    'use our embeddings also for tasks '\n",
      "                                    'involving comparisons between written and '\n",
      "                                    'spoken words. For example, the standard '\n",
      "                                    'task of spoken term detection (Fiscus et '\n",
      "                                    'al., 2007) involves searching for '\n",
      "                                    'examples of a given text query in spoken '\n",
      "                                    'documents. This task is identical to '\n",
      "                                    'queryby-example except that the query is '\n",
      "                                    'given as text. In order to explore the '\n",
      "                                    'potential of multi-view embeddings for '\n",
      "                                    'such tasks, we design another proxy task, '\n",
      "                                    'cross-view word discrimination. Here we '\n",
      "                                    'are given a pair of inputs, one a written '\n",
      "                                    'word and one an acoustic word segment, '\n",
      "                                    'and our task is to determine if the '\n",
      "                                    'acoustic signal is an example of the '\n",
      "                                    'written word. The evalution proceeds '\n",
      "                                    'analogously to the acoustic word '\n",
      "                                    'discrimination task: We output “yes” if '\n",
      "                                    'the cosine distance between the '\n",
      "                                    'embeddings of the written and spoken '\n",
      "                                    'sequences are below some threshold, and '\n",
      "                                    'measure performance as the average '\n",
      "                                    'precision (AP) over all thresholds.\\n'\n",
      "                                    'Finally, we also would like to obtain a '\n",
      "                                    'more fine-grained measure of whether the '\n",
      "                                    'learned embeddings capture our intuitive '\n",
      "                                    'sense of similarity between words. Being '\n",
      "                                    'able to capture word similarity may also '\n",
      "                                    'be useful in building query or '\n",
      "                                    'recognition systems that fail gracefully '\n",
      "                                    'and produce humanlike errors. For this '\n",
      "                                    'purpose we measure the rank correlation '\n",
      "                                    'between embedding distances and character '\n",
      "                                    'edit distances. This is analogous to the '\n",
      "                                    'evaluation of semantic word embeddings '\n",
      "                                    'via the rank correlation between '\n",
      "                                    'embedding distances and human similarity '\n",
      "                                    'judgments (Finkelstein et al., 2001; Hill '\n",
      "                                    'et al., 2015). In our case, however, we '\n",
      "                                    'do not use human judgments since the '\n",
      "                                    'ground-truth edit distances themselves '\n",
      "                                    'provide a good measure. We refer to this '\n",
      "                                    'as the word similarity task, and we apply '\n",
      "                                    'this measure to both pairs of acoustic '\n",
      "                                    'embeddings and pairs of character '\n",
      "                                    'sequence embeddings. Similar measures '\n",
      "                                    'have been proposed by Ghannay et al. '\n",
      "                                    '(2016) to evaluate acoustic word '\n",
      "                                    'embeddings, although they considered only '\n",
      "                                    'near neighbors of each word whereas we '\n",
      "                                    'consider the correlation across the full '\n",
      "                                    'range of word pairs.\\n'\n",
      "                                    'In the experiments described below, we '\n",
      "                                    'first focus on the acoustic word '\n",
      "                                    'discrimination task for purposes of '\n",
      "                                    'initial exploration and hyperparameter '\n",
      "                                    'search, and then largely fix the models '\n",
      "                                    'for evaluation using the cross-view word '\n",
      "                                    'discrimination and word similarity '\n",
      "                                    'measures.'},\n",
      "                           {'heading': '4.1 DATA',\n",
      "                            'text': 'We use the same experimental setup and '\n",
      "                                    'data as in Kamper et al. (2015, 2016); '\n",
      "                                    'Settle & Livescu (2016). The task and '\n",
      "                                    'setup were first developed by (Carlin et '\n",
      "                                    'al., 2011). The data is drawn from the '\n",
      "                                    'Switchboard English conversational speech '\n",
      "                                    'corpus (Godfrey et al., 1992). The spoken '\n",
      "                                    'word segments range in duration from 50 '\n",
      "                                    'to 200 frames (0.5 - 2 seconds). The '\n",
      "                                    'train/dev/test splits contain '\n",
      "                                    '9971/10966/11024 pairs of acoustic '\n",
      "                                    'segments and character sequences, '\n",
      "                                    'corresponding to 1687/3918/3390 unique '\n",
      "                                    'words. In computing the AP for the dev or '\n",
      "                                    'test set, all pairs in the set are used, '\n",
      "                                    'yielding approximately 60 million word '\n",
      "                                    'pairs.\\n'\n",
      "                                    'The input to the embedding model in the '\n",
      "                                    'acoustic view is a sequence of '\n",
      "                                    '39-dimensional vectors (one per frame) of '\n",
      "                                    'standard mel frequency cepstral '\n",
      "                                    'coefficients (MFCCs) and their first and '\n",
      "                                    'second derivatives. The input to the '\n",
      "                                    'character sequence embedding model is a '\n",
      "                                    'sequence of 26-dimensional one-hot '\n",
      "                                    'vectors indicating each character of the '\n",
      "                                    'word’s orthography.'},\n",
      "                           {'heading': '4.2 MODEL DETAILS AND HYPERPARAMETER '\n",
      "                                       'TUNING',\n",
      "                            'text': 'We experiment with different neural '\n",
      "                                    'network architectures for each view, '\n",
      "                                    'varying the number of stacked LSTM '\n",
      "                                    'layers, the number of hidden units for '\n",
      "                                    'each layer, and the use of single- or '\n",
      "                                    'bidirectional LSTM cells. A coarse grid '\n",
      "                                    'search shows that 2-layer bidirectional '\n",
      "                                    'LSTMs with 512 hidden units per direction '\n",
      "                                    'per layer perform well on the acoustic '\n",
      "                                    'word discrimination task, and we keep '\n",
      "                                    'this structure fixed for subsequent '\n",
      "                                    'experiments (see Appendix A for more '\n",
      "                                    'details). We use the outputs of the '\n",
      "                                    'top-layer LSTMs as the learned embedding '\n",
      "                                    'for each view, which is 1024-dimensional '\n",
      "                                    'if bidirectional LSTMs are used.\\n'\n",
      "                                    'In training, we use dropout on the inputs '\n",
      "                                    'of the acoustic view and between stacked '\n",
      "                                    'layers for both views. The architecture '\n",
      "                                    'is illustrated in Figure 1. For each '\n",
      "                                    'training example, our contrastive losses '\n",
      "                                    'require a corresponding negative example. '\n",
      "                                    'We generate a negative character label '\n",
      "                                    'sequence by uniformly sampling a word '\n",
      "                                    'label from the training set that is '\n",
      "                                    'different from the positive label. We '\n",
      "                                    'perform a new negative label sampling at '\n",
      "                                    'the beginning of each epoch. Similarly, '\n",
      "                                    'negative acoustic feature sequences are '\n",
      "                                    'uniformly sampled from all of the '\n",
      "                                    'differently labeled acoustic feature '\n",
      "                                    'sequences in the training set.\\n'\n",
      "                                    'The network weights are initialized with '\n",
      "                                    'values sampled uniformly from the range '\n",
      "                                    '[−0.05, 0.05]. We use the Adam optimizer '\n",
      "                                    '(Kingma & Ba, 2015) for updating the '\n",
      "                                    'weights using mini-batches of 20 acoustic '\n",
      "                                    'segments, with an initial learning rate '\n",
      "                                    'tuned over {0.0001, 0.001}. Dropout is '\n",
      "                                    'used at each layer, with the rate tuned '\n",
      "                                    'over {0, 0.2, 0.4, 0.5}, in which 0.4 '\n",
      "                                    'usually outperformed others. The margin '\n",
      "                                    'in our basic contrastive objectives 0-3 '\n",
      "                                    'is tuned over {0.3, 0.4, 0.5, 0.6, 0.7}, '\n",
      "                                    'out of which 0.4 and 0.5 typically yield '\n",
      "                                    'best results. For obj0 with the '\n",
      "                                    'cost-sensitive margin, we tune the '\n",
      "                                    'maximum margin mmax over {0.5, 0.6, 0.7} '\n",
      "                                    'and the threshold tmax over {9, 11, 13}. '\n",
      "                                    'We train each model for up to 1000 '\n",
      "                                    'epochs. The model that gives the best AP '\n",
      "                                    'on the development set is used for '\n",
      "                                    'evaluation on the test set.'},\n",
      "                           {'heading': '4.3 EFFECTS OF DIFFERENT OBJECTIVES',\n",
      "                            'text': 'We presented four contrastive losses '\n",
      "                                    '(3)–(6) and potential combinations in '\n",
      "                                    'Section 2.1. We now explore the effects '\n",
      "                                    'of these different objectives on the word '\n",
      "                                    'discrimination tasks.\\n'\n",
      "                                    'Table 1 shows the development set AP for '\n",
      "                                    'acoustic and cross-view word '\n",
      "                                    'discrimination achieved using the various '\n",
      "                                    'objectives. We tuned the objectives for '\n",
      "                                    'the acoustic discrimination task, and '\n",
      "                                    'then used the corresponding converged '\n",
      "                                    'models for the cross-view task. Of the '\n",
      "                                    'simple contrastive objectives, obj0 and '\n",
      "                                    'obj2 (which involve only cross-view '\n",
      "                                    'distances) slightly outperform the other '\n",
      "                                    'two on the acoustic word discrimination '\n",
      "                                    'task. The best-performing objective is '\n",
      "                                    'the “symmetrized” objective obj0 + obj2, '\n",
      "                                    'which significantly outperforms all '\n",
      "                                    'individual objectives (and the '\n",
      "                                    'combination of the four). Finally, the '\n",
      "                                    'cost-sensitive objective is very '\n",
      "                                    'competitive as well, while falling '\n",
      "                                    'slightly short of the best performance. '\n",
      "                                    'We note that a similar objective to our '\n",
      "                                    'obj0 + obj2 was used by Vendrov et al. '\n",
      "                                    '(2016) for the task of caption-image '\n",
      "                                    'retrieval, where the authors essentially '\n",
      "                                    'use all non-paired\\n'\n",
      "                                    'Objective Dev AP Dev AP (acoustic) '\n",
      "                                    '(cross-view)\\n'\n",
      "                                    'obj0 0.659 0.791 obj1 0.654 0.807 obj2 '\n",
      "                                    '0.675 0.788 obj3 0.640 0.782 obj0 + obj2 '\n",
      "                                    '0.702 0.814∑3\\n'\n",
      "                                    'i=0 obj i 0.672 0.804 cost-sensitive '\n",
      "                                    '0.671 0.802\\n'\n",
      "                                    'Table 1: Word discrimination performance '\n",
      "                                    'with different objectives.\\n'\n",
      "                                    'examples from the other view in the '\n",
      "                                    'minibatch as negative examples (instead '\n",
      "                                    'of random sampling one negative example '\n",
      "                                    'as we do) to be contrasted with one '\n",
      "                                    'paired example.\\n'\n",
      "                                    'Figure 2 shows the progression of the '\n",
      "                                    'development set AP for acoustic word '\n",
      "                                    'discrimination over 1000 training epochs, '\n",
      "                                    'using several of the objectives, where AP '\n",
      "                                    'is evaluated every 5 epochs. We observe '\n",
      "                                    'that even after 1000 epochs, the '\n",
      "                                    'development set AP has not quite '\n",
      "                                    'saturated, indicating that it may be '\n",
      "                                    'possible to further improve performance.\\n'\n",
      "                                    'Overall, our best-performing objective is '\n",
      "                                    'the combined obj0+obj2, and we use it for '\n",
      "                                    'reporting final test-set results. Table 2 '\n",
      "                                    'shows the test set AP for both the '\n",
      "                                    'acoustic and cross-view tasks using our '\n",
      "                                    'final model (“multi-view LSTM”). For '\n",
      "                                    'comparison, we also include acoustic word '\n",
      "                                    'discrimination results reported '\n",
      "                                    'previously by Kamper et al. (2016); '\n",
      "                                    'Settle & Livescu (2016). Previous '\n",
      "                                    'approaches have not addressed the problem '\n",
      "                                    'of learning embeddings jointly with the '\n",
      "                                    'text view, so they can not be evaluated '\n",
      "                                    'on the cross-view task.'},\n",
      "                           {'heading': '4.4 WORD SIMILARITY TASKS',\n",
      "                            'text': 'Table 3 gives our results on the word '\n",
      "                                    'similarity tasks, that is the rank '\n",
      "                                    'correlation (Spearman’s ρ) between '\n",
      "                                    'embedding distances and orthographic edit '\n",
      "                                    'distance (Levenshtein distance between '\n",
      "                                    'character sequences). We measure this '\n",
      "                                    'correlation for both our acoustic word '\n",
      "                                    'embeddings and for our text embeddings. '\n",
      "                                    'In the case of the text embeddings, we '\n",
      "                                    'could of course directly measure the '\n",
      "                                    'Levenshtein distance between the inputs; '\n",
      "                                    'here we are simply measuring how much of '\n",
      "                                    'this information the text embeddings are '\n",
      "                                    'able to retain.\\n'\n",
      "                                    'Interestingly, while the cost-sensitive '\n",
      "                                    'objective did not produce substantial '\n",
      "                                    'gains on the word discrimination tasks '\n",
      "                                    'above, it does greatly improve the '\n",
      "                                    'performance on this word similarity '\n",
      "                                    'measure. This is a satisfying '\n",
      "                                    'observation, since the cost-sensitive '\n",
      "                                    'loss is trying to improve precisely this '\n",
      "                                    'relationship between distances in the '\n",
      "                                    'embedding space and the orthographic edit '\n",
      "                                    'distance.\\n'\n",
      "                                    'Although we have trained our embeddings '\n",
      "                                    'using orthographic labels, it is also '\n",
      "                                    'interesting to consider how closely '\n",
      "                                    'aligned the embeddings are with the '\n",
      "                                    'corresponding phonetic pronunciations. '\n",
      "                                    'For comparison, the rank correlation '\n",
      "                                    'between our acoustic embeddings and '\n",
      "                                    'phonetic edit distances is 0.226, and for '\n",
      "                                    'our text embeddings it is 0.241, which '\n",
      "                                    'are relatively close to the rank '\n",
      "                                    'correlations with orthographic edit '\n",
      "                                    'distance. A future direction is to '\n",
      "                                    'directly train embeddings with phonetic '\n",
      "                                    'sequence supervision rather than '\n",
      "                                    'orthography; this setting involves '\n",
      "                                    'somewhat stronger supervision, but it is '\n",
      "                                    'easy to obtain in many cases.\\n'\n",
      "                                    'Another interesting point is that the '\n",
      "                                    'performance is not a great deal better '\n",
      "                                    'for the text embeddings than for the '\n",
      "                                    'acoustic embeddings, even though the text '\n",
      "                                    'embeddings have at their disposal the '\n",
      "                                    'text input itself. We believe this has to '\n",
      "                                    'do with the distribution of words in our '\n",
      "                                    'data: While the data includes a large '\n",
      "                                    'variety of words, it does not include '\n",
      "                                    'many very similar pairs. In fact, of all '\n",
      "                                    'possible pairs of unique training set '\n",
      "                                    'words, fewer than 2% have an edit '\n",
      "                                    'distance below 5 characters. Therefore, '\n",
      "                                    'there may not be sufficient information '\n",
      "                                    'to learn to distinguish detailed '\n",
      "                                    'differences among character sequences, '\n",
      "                                    'and the cost-sensitive loss ultimately '\n",
      "                                    'does not learn much more than to separate '\n",
      "                                    'different words. In future work it would '\n",
      "                                    'be interesting to experiment with data '\n",
      "                                    'sets that have a larger variety of '\n",
      "                                    'similar words.'},\n",
      "                           {'heading': '4.5 VISUALIZATION OF LEARNED '\n",
      "                                       'EMBEDDINGS',\n",
      "                            'text': 'Figure 3 gives a 2-dimensional t-SNE (van '\n",
      "                                    'der Maaten & Hinton, 2008) visualization '\n",
      "                                    'of selected acoustic and character '\n",
      "                                    'sequences from the development set, '\n",
      "                                    'including some that were seen in the '\n",
      "                                    'training set and some previously unseen '\n",
      "                                    'words. The previously seen words in this '\n",
      "                                    'figure were selected uniformly at random '\n",
      "                                    'among those that appear at least 15 times '\n",
      "                                    'in the development set (the unseen words '\n",
      "                                    'are the only six that appear at least 15 '\n",
      "                                    'times in the development set). This '\n",
      "                                    'visualization demonstrates that the '\n",
      "                                    'acoustic embeddings cluster very tightly '\n",
      "                                    'and are very close to the text '\n",
      "                                    'embeddings, and that unseen words cluster '\n",
      "                                    'nearly as well as previously seen ones.\\n'\n",
      "                                    'While Figure 3 shows the relationship '\n",
      "                                    'among the multiple acoustic embeddings '\n",
      "                                    'and the text embeddings, the words are '\n",
      "                                    'all very different so we cannot draw '\n",
      "                                    'conclusions about the relationships '\n",
      "                                    'between words. Figure 4 provides another '\n",
      "                                    'visualization, this time exploring the '\n",
      "                                    'relationship among the text embeddings of '\n",
      "                                    'a number of closely related words, namely '\n",
      "                                    'all development set words ending in '\n",
      "                                    '“-ly”, “-ing”, and “-tion”. This '\n",
      "                                    'visualization confirms that related words '\n",
      "                                    'are embedded close together, with the '\n",
      "                                    'words sharing a suffix forming fairly '\n",
      "                                    'well-defined clusters.'},\n",
      "                           {'heading': '5 CONCLUSION',\n",
      "                            'text': 'We have presented an approach for jointly '\n",
      "                                    'learning acoustic word embeddings and '\n",
      "                                    'their orthographic counterparts. This '\n",
      "                                    'multi-view approach produces improved '\n",
      "                                    'acoustic word embedding performance over '\n",
      "                                    'previous approaches, and also has the '\n",
      "                                    'benefit that the same embeddings can be '\n",
      "                                    'applied for both spoken and written query '\n",
      "                                    'tasks. We have explored a variety of '\n",
      "                                    'contrastive objectives: ones with a fixed '\n",
      "                                    'margin that aim to separate same and '\n",
      "                                    'different word pairs, as well as a '\n",
      "                                    'cost-sensitive loss that aims to capture '\n",
      "                                    'orthographic edit distances. While the '\n",
      "                                    'losses generally perform similarly for '\n",
      "                                    'word discrimination tasks, the '\n",
      "                                    'cost-sensitive loss improves the '\n",
      "                                    'correlation between embedding distances '\n",
      "                                    'and orthographic distances. One '\n",
      "                                    'interesting direction for future work is '\n",
      "                                    'to directly use knowledge about phonetic '\n",
      "                                    'pronunciations, in both evaluation and '\n",
      "                                    'training. Another direction is to extend '\n",
      "                                    'our approach to directly train on both '\n",
      "                                    'word and non-word segments.'},\n",
      "                           {'heading': 'ACKNOWLEDGMENTS',\n",
      "                            'text': 'This research was supported by a Google '\n",
      "                                    'Faculty Award and by NSF grant '\n",
      "                                    'IIS-1321015. The opinions expressed in '\n",
      "                                    'this work are those of the authors and do '\n",
      "                                    'not necessarily reflect the views of the '\n",
      "                                    'funding agency. This research used GPUs '\n",
      "                                    'donated by NVIDIA Corporation. We thank '\n",
      "                                    'Herman Kamper and Shane Settle for their '\n",
      "                                    'assistance with the data and experimental '\n",
      "                                    'setup.'},\n",
      "                           {'heading': 'A ADDITIONAL ANALYSIS',\n",
      "                            'text': 'We first explore the effect of network '\n",
      "                                    'architectures for our embedding models. '\n",
      "                                    'We learn embeddings using objective obj0 '\n",
      "                                    'and evaluate them on the acoustic and '\n",
      "                                    'cross-view word discrimination tasks. The '\n",
      "                                    'resulting average precisions on the '\n",
      "                                    'development set are given in Table 4. All '\n",
      "                                    'of the models were trained for 1000 '\n",
      "                                    'epochs, except for the 1-layer '\n",
      "                                    'unidirectional models which converged '\n",
      "                                    'after 500 epochs. It is clear that '\n",
      "                                    'bidirectional LSTMs are more successful '\n",
      "                                    'than unidirectional LSTMs for these '\n",
      "                                    'tasks, and two layers of LSTMs are much '\n",
      "                                    'better than a single layer of LSTMs. We '\n",
      "                                    'did not observe significant further '\n",
      "                                    'improvement by using more than two layers '\n",
      "                                    'of LSTMs. For all other experiments, we '\n",
      "                                    'fix the architecture to 2-layer '\n",
      "                                    'bidirectional LSTMs for each view.\\n'\n",
      "                                    'In Figure 5 we also give the '\n",
      "                                    'precision-recall curve for our best '\n",
      "                                    'models, as well as the scatter plot of '\n",
      "                                    'cosine distances between acoustic '\n",
      "                                    'embeddings vs. orthographic edit '\n",
      "                                    'distances.'}],\n",
      "              'source': 'CRF',\n",
      "              'title': 'MULTI-VIEW RECURRENT NEURAL ACOUSTIC WORD EMBEDDINGS',\n",
      "              'year': 2017},\n",
      " 'name': 'rJxDkvqee.pdf'}\n"
     ]
    }
   ],
   "source": [
    "pprint(paper_data[2017][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['metadata', 'name'])\n",
      "dict_keys(['year', 'source', 'referenceMentions', 'abstractText', 'references', 'creator', 'title', 'authors', 'emails', 'sections'])\n"
     ]
    }
   ],
   "source": [
    "print(paper_data[0].keys())\n",
    "print(paper_data[0]['metadata'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run for all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_data = read_papers_json([2017, 2018, 2019, 2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([2017, 2018, 2019, 2020])\n",
      "\n",
      "Number of papers each year:\n",
      "490\n",
      "909\n",
      "1418\n",
      "2212\n"
     ]
    }
   ],
   "source": [
    "type(paper_data)\n",
    "print(paper_data.keys())\n",
    "\n",
    "print(\"\\nNumber of papers each year:\")\n",
    "for y in [2017, 2018, 2019, 2020]:\n",
    "    print(len(paper_data[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_text_features(sections_list):\n",
    "    full_text = \"\"\n",
    "    text_features = {\"contains_appendix\": 0, \"table_ref_count\": 0, \"fig_ref_count\": 0, \"eqn_ref_count\": 0}\n",
    "    \n",
    "    for section in sections_list:\n",
    "        if \"heading\" in section and section[\"heading\"]:\n",
    "            full_text += \" \" + section[\"heading\"].lower()\n",
    "        if \"text\" in section and section[\"text\"]:\n",
    "            full_text += \" \" + section[\"text\"].lower()\n",
    "    \n",
    "    # Appendix\n",
    "    if full_text.find(\"appendix\") > -1:\n",
    "        text_features[\"contains_appendix\"] = 1\n",
    "    \n",
    "    # Table\n",
    "    p = re.compile('table [0-9][0-9]?|tables [0-9][0-9]?')\n",
    "    table_refs = p.findall(full_text)\n",
    "    if table_refs:\n",
    "#         table_refs_count = len(table_refs)\n",
    "        text_features[\"table_ref_count\"] = len(table_refs)\n",
    "    \n",
    "    # Figure\n",
    "    fig_refs = re.findall(r'fig[s]?\\. [0-9][0-9]*|figure [0-9][0-9]*|figures [0-9][0-9]*', full_text)\n",
    "    if fig_refs:\n",
    "        text_features[\"fig_ref_count\"] = len(fig_refs)\n",
    "    \n",
    "    # Equations\n",
    "    eqn_refs = re.findall(r'eq[s]?\\.[ \\(\\[ ][A-Z0-9]|equation[s]?[ \\:\\.\\(\\[\\{]?[0-9]?', full_text)\n",
    "    if eqn_refs:\n",
    "        text_features[\"eqn_ref_count\"] = len(eqn_refs)\n",
    "    \n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table 0', 'table 1', 'table 2']\n"
     ]
    }
   ],
   "source": [
    "# SCRATCH FOR REGEX\n",
    "p = re.compile('table [0-9][0-9]?')\n",
    "ft = \"As a matter of fact, sk plays a similarly table 0 role as the attention score in various attention models such\\nas Vinyals et al. (2015). The impact of proceeding elements to the current output can be adjusted (either increase or decrease) by sk. The memory capability of ELSTM-II can be proven in a similarly fashion, so even ELSTM-II does not have forget gate, it is capable in attending to or forgetting a particular position of a sequence as ELSTM-I through the scaling factor.\\nThe major difference between the ELSTM-I and the ELSTM-II is that fewer parameters are used in the ELSTM-II than those in the ELSTM-I. The numbers of parameters used by different RNN cells are compared in Table 1, where Xt ∈ RM , ht ∈ RN and t = 1, · · · , T .\\nAlthough the number of parameters of ELSTM depends on the maximum length of a sequence in practice, the memory overhead required is limited. ELSTM-II requires less number of parameters than LSTM for typical lengthed sequence. From Table. 1, to double the number of parameters as compare to an ordinary LSTM, the length of a sentence needs to be 4 times the size of the word embedding size and number of cells put together. That is, in the case of Sutskever et al. (2014) with 1000 word embedding and 1000 cells, the sentence length needs to be 4 × (1000 + 1000) = 8000! In practice, most NLP problems whose input involves sentences, the length will be typically less than 100. In our experiment, sequence to sequence with attention (Vinyals et al., 2015) for maximum sentence length 100 (other model settings please refer to Table 2), ELSTM-I parameters uses 75M of memory, ELSTM-II uses 69.1M, LSTM uses 71.5M\"\n",
    "pt = p.findall(ft.lower())\n",
    "print(pt)\n",
    "\n",
    "\n",
    "s = \"d and accelerated. Our method is outlined in Figure 1. The main idea is to RNN model is shown in Fig. 3. It consists of a lower and an upper BRNN branches. At each time step\"\n",
    "print(re.findall(r'fig\\. [0-9][0-9]*|figs\\. [0-9][0-9]*|figure [0-9][0-9]*|figures [0-9][0-9]*', s.lower()))\n",
    "if re.findall(r'fig\\. [0-9][0-9]*|figs\\. [0-9][0-9]*|figure [0-9][0-9]*|figures [0-9][0-9]*', s):\n",
    "    print(\"farw\")\n",
    "else:\n",
    "    print(\"yheh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def construct_features(paper_data):\n",
    "    features = {}\n",
    "    \n",
    "    metadata_nf = 0\n",
    "    sections_nf = 0\n",
    "    ref_mentions_nf = 0\n",
    "    \n",
    "    for year_key in paper_data.keys():\n",
    "        for p in paper_data[year_key]:\n",
    "            try:\n",
    "                if \"name\" in p:\n",
    "                    pid = str(year_key) + \"_\" + p[\"name\"].split(\".\")[0]\n",
    "                    if \"metadata\" in p:\n",
    "                        features[pid] = {}\n",
    "\n",
    "                        # count sections list. Note: This also counts the subsections separately, but works for now as it is consistent for all.\n",
    "                        if \"sections\" in p[\"metadata\"] and p[\"metadata\"][\"sections\"]:\n",
    "                            features[pid][\"num_sections\"] = len(p[\"metadata\"][\"sections\"])\n",
    "\n",
    "                            # Other features such as presence of tables. equations, figures, and appendix.\n",
    "                            features[pid].update(paper_text_features(p[\"metadata\"][\"sections\"]))\n",
    "                            #features[pid][\"contains_appendix\"] = paper_text_features(p[\"metadata\"][\"sections\"])\n",
    "\n",
    "                        else:\n",
    "                            features[pid][\"num_sections\"] = 4\n",
    "                            sections_nf += 1\n",
    "\n",
    "\n",
    "                        # count avg reference mention length\n",
    "                        if \"referenceMentions\" in p[\"metadata\"] and p[\"metadata\"][\"referenceMentions\"]:\n",
    "                            ref_lengths = 0\n",
    "                            ref_counts = 0\n",
    "                            for ref_info in p[\"metadata\"][\"referenceMentions\"]:\n",
    "                                if \"startOffset\" in ref_info and \"endOffset\" in ref_info:\n",
    "                                    ref_lengths += (ref_info[\"endOffset\"] - ref_info[\"startOffset\"] + 1)\n",
    "                                    ref_counts += 1\n",
    "\n",
    "                            features[pid][\"avg_ref_mention\"] = ref_lengths//ref_counts\n",
    "                        else:\n",
    "                            features[pid][\"avg_ref_mention\"] = 0\n",
    "                            ref_mentions_nf += 1\n",
    "                    else:\n",
    "                        print(\"Metadata not present for id: \", pid)\n",
    "                        metadata_nf += 1\n",
    "                else:\n",
    "                    print(\"Paper id not present for: \", end=\"\")\n",
    "                    pprint(p)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(\"Exception occurred for: \" + p[\"name\"], end=\"\")\n",
    "                print(\"============================================================================================\")\n",
    "        print(\"NOT FOUNDS: \", metadata_nf, sections_nf, ref_mentions_nf)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT FOUNDS:  0 1 3\n"
     ]
    }
   ],
   "source": [
    "features_dict = construct_features(paper_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2017_S1_pAu9xl',\n",
       "  {'avg_ref_mention': 86,\n",
       "   'contains_appendix': 0,\n",
       "   'eqn_ref_count': 7,\n",
       "   'fig_ref_count': 8,\n",
       "   'num_sections': 19,\n",
       "   'table_ref_count': 5}),\n",
       " ('2017_SyEiHNKxx',\n",
       "  {'avg_ref_mention': 30,\n",
       "   'contains_appendix': 0,\n",
       "   'eqn_ref_count': 8,\n",
       "   'fig_ref_count': 8,\n",
       "   'num_sections': 14,\n",
       "   'table_ref_count': 1}),\n",
       " ('2017_r1LXit5ee',\n",
       "  {'avg_ref_mention': 35,\n",
       "   'contains_appendix': 1,\n",
       "   'eqn_ref_count': 2,\n",
       "   'fig_ref_count': 3,\n",
       "   'num_sections': 17,\n",
       "   'table_ref_count': 5}),\n",
       " ('2017_Hk3mPK5gg',\n",
       "  {'avg_ref_mention': 308,\n",
       "   'contains_appendix': 0,\n",
       "   'eqn_ref_count': 0,\n",
       "   'fig_ref_count': 7,\n",
       "   'num_sections': 12,\n",
       "   'table_ref_count': 0}),\n",
       " ('2017_S1AG8zYeg',\n",
       "  {'avg_ref_mention': 746,\n",
       "   'contains_appendix': 0,\n",
       "   'eqn_ref_count': 8,\n",
       "   'fig_ref_count': 9,\n",
       "   'num_sections': 22,\n",
       "   'table_ref_count': 13})]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features_dict.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkYbF1slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'heading': None,\n",
      "  'text': 'A framework is presented for unsupervised learning of '\n",
      "          'representations based on infomax principle for large-scale neural '\n",
      "          'populations. We use an asymptotic approximation to the Shannon’s '\n",
      "          'mutual information for a large neural population to demonstrate '\n",
      "          'that a good initial approximation to the global '\n",
      "          'information-theoretic optimum can be obtained by a hierarchical '\n",
      "          'infomax method. Starting from the initial solution, an efficient '\n",
      "          'algorithm based on gradient descent of the final objective function '\n",
      "          'is proposed to learn representations from the input datasets, and '\n",
      "          'the method works for complete, overcomplete, and undercomplete '\n",
      "          'bases. As confirmed by numerical experiments, our method is robust '\n",
      "          'and highly efficient for extracting salient features from input '\n",
      "          'datasets. Compared with the main existing methods, our algorithm '\n",
      "          'has a distinct advantage in both the training speed and the '\n",
      "          'robustness of unsupervised representation learning. Furthermore, '\n",
      "          'the proposed method is easily extended to the supervised or '\n",
      "          'unsupervised model for training deep structure networks.'},\n",
      " {'heading': '1 INTRODUCTION',\n",
      "  'text': 'How to discover the unknown structures in data is a key task for '\n",
      "          'machine learning. Learning good representations from observed data '\n",
      "          'is important because a clearer description may help reveal the '\n",
      "          'underlying structures. Representation learning has drawn '\n",
      "          'considerable attention in recent years (Bengio et al., 2013). One '\n",
      "          'category of algorithms for unsupervised learning of representations '\n",
      "          'is based on probabilistic models (Lewicki & Sejnowski, 2000; Hinton '\n",
      "          '& Salakhutdinov, 2006; Lee et al., 2008), such as maximum '\n",
      "          'likelihood (ML) estimation, maximum a posteriori (MAP) probability '\n",
      "          'estimation, and related methods. Another category of algorithms is '\n",
      "          'based on reconstruction error or generative criterion (Olshausen & '\n",
      "          'Field, 1996; Aharon et al., 2006; Vincent et al., 2010; Mairal et '\n",
      "          'al., 2010; Goodfellow et al., 2014), and the objective functions '\n",
      "          'usually involve squared errors with additional constraints. '\n",
      "          'Sometimes the reconstruction error or generative criterion may also '\n",
      "          'have a probabilistic interpretation (Olshausen & Field, 1997; '\n",
      "          'Vincent et al., 2010).\\n'\n",
      "          'Shannon’s information theory is a powerful tool for description of '\n",
      "          'stochastic systems and could be utilized to provide a '\n",
      "          'characterization for good representations (Vincent et al., 2010). '\n",
      "          'However, computational difficulties associated with Shannon’s '\n",
      "          'mutual information (MI) (Shannon, 1948) have hindered its wider '\n",
      "          'applications. The Monte Carlo (MC) sampling (Yarrow et al., 2012) '\n",
      "          'is a convergent method for estimating MI with arbitrary accuracy, '\n",
      "          'but its computational inefficiency makes it unsuitable for '\n",
      "          'difficult optimization problems especially in the cases of '\n",
      "          'high-dimensional input stimuli and large population networks. Bell '\n",
      "          'and Sejnowski (Bell & Sejnowski, 1995; 1997) have directly applied '\n",
      "          'the infomax approach (Linsker, 1988) to independent component '\n",
      "          'analysis (ICA) of data with independent non-Gaussian components '\n",
      "          'assuming additive noise, but their method requires that the number '\n",
      "          'of outputs be equal to the number of inputs. The extensions of ICA '\n",
      "          'to overcomplete or undercomplete bases incur increased algorithm '\n",
      "          'complexity and difficulty in learning of parameters (Lewicki & '\n",
      "          'Sejnowski, 2000; Kreutz-Delgado et al., 2003; Karklin & Simoncelli, '\n",
      "          '2011).\\n'\n",
      "          'Since Shannon MI is closely related to ML and MAP (Huang & Zhang, '\n",
      "          '2017), the algorithms of representation learning based on '\n",
      "          'probabilistic models should be amenable to information-theoretic '\n",
      "          'treatment. Representation learning based on reconstruction error '\n",
      "          'could be accommodated also by information theory, because the '\n",
      "          'inverse of Fisher information (FI) is the Cramér-Rao lower bound '\n",
      "          'on the mean square decoding error of any unbiased decoder (Rao, '\n",
      "          '1945). Hence minimizing the reconstruction error potentially '\n",
      "          'maximizes a lower bound on the MI (Vincent et al., 2010).\\n'\n",
      "          'Related problems arise also in neuroscience. It has long been '\n",
      "          'suggested that the real nervous systems might approach an '\n",
      "          'information-theoretic optimum for neural coding and computation '\n",
      "          '(Barlow, 1961; Atick, 1992; Borst & Theunissen, 1999). However, in '\n",
      "          'the cerebral cortex, the number of neurons is huge, with about 105 '\n",
      "          'neurons under a square millimeter of cortical surface (Carlo & '\n",
      "          'Stevens, 2013). It has often been computationally intractable to '\n",
      "          'precisely characterize information coding and processing in large '\n",
      "          'neural populations.\\n'\n",
      "          'To address all these issues, we present a framework for '\n",
      "          'unsupervised learning of representations in a large-scale nonlinear '\n",
      "          'feedforward model based on infomax principle with realistic '\n",
      "          'biological constraints such as neuron models with Poisson spikes. '\n",
      "          'First we adopt an objective function based on an asymptotic formula '\n",
      "          'in the large population limit for the MI between the stimuli and '\n",
      "          'the neural population responses (Huang & Zhang, 2017). Since the '\n",
      "          'objective function is usually nonconvex, choosing a good initial '\n",
      "          'value is very important for its optimization. Starting from an '\n",
      "          'initial value, we use a hierarchical infomax approach to quickly '\n",
      "          'find a tentative global optimal solution for each layer by analytic '\n",
      "          'methods. Finally, a fast convergence learning rule is used for '\n",
      "          'optimizing the final objective function based on the tentative '\n",
      "          'optimal solution. Our algorithm is robust and can learn complete, '\n",
      "          'overcomplete or undercomplete basis vectors quickly from different '\n",
      "          'datasets. Experimental results showed that the convergence rate of '\n",
      "          'our method was significantly faster than other existing methods, '\n",
      "          'often by an order of magnitude. More importantly, the number of '\n",
      "          'output units processed by our method can be very large, much larger '\n",
      "          'than the number of inputs. As far as we know, no existing model can '\n",
      "          'easily deal with this situation.'},\n",
      " {'heading': '2 METHODS', 'text': ''},\n",
      " {'heading': '2.1 APPROXIMATION OF MUTUAL INFORMATION FOR NEURAL POPULATIONS',\n",
      "  'text': 'Suppose the input x is a K-dimensional vector, x = (x1, · · · , '\n",
      "          'xK)T , the outputs of N neurons are denoted by a vector, r = (r1, · '\n",
      "          '· · , rN )T , where we assume N is large, generally N K. We denote '\n",
      "          'random variables by upper case letters, e.g., random variables X '\n",
      "          'and R, in contrast to their vector values x and r. The MI between X '\n",
      "          'and R is defined by I(X;R) = 〈 ln p(x|r)p(x) 〉 r,x , where 〈·〉r,x '\n",
      "          'denotes the expectation with respect to the probability density '\n",
      "          'function (PDF) p(r,x).\\n'\n",
      "          'Our goal is to maxmize MI I(X;R) by finding the optimal PDF p(r|x) '\n",
      "          'under some constraint conditions, assuming that p(r|x) is '\n",
      "          'characterized by a noise model and activation functions f(x;θn) '\n",
      "          'with parameters θn for the n-th neuron (n = 1, · · · , N ). In '\n",
      "          'other words, we optimize p(r|x) by solving for the optimal '\n",
      "          'parameters θn. Unfortunately, it is intractable in most cases to '\n",
      "          'solve for the optimal parameters that maximizes I(X;R). However, if '\n",
      "          'p(x) and p(r|x) are twice continuously differentiable for almost '\n",
      "          'every x ∈ RK , then for large N we can use an asymptotic formula to '\n",
      "          'approximate the true value of I(X;R) with high accuracy (Huang & '\n",
      "          'Zhang, 2017):\\n'\n",
      "          \"I(X;R) ' IG = 1\\n\"\n",
      "          '2\\n'\n",
      "          '〈 ln ( det ( G(x)\\n'\n",
      "          '2πe ))〉 x +H(X), (1)\\n'\n",
      "          'where det (·) denotes the matrix determinant and H(X) = −〈ln p(x)〉x '\n",
      "          'is the stimulus entropy, G(x) = J(x) + P (x) , (2)\\n'\n",
      "          'J(x) = − 〈 ∂2 ln p (r|x) ∂x∂xT 〉 r|x , (3)\\n'\n",
      "          'P(x) = −∂ 2 ln p (x)\\n'\n",
      "          '∂x∂xT . (4) Assuming independent noises in neuronal responses, we '\n",
      "          'have p(r|x) = ∏N n=1 p(rn|x;θn),\\n'\n",
      "          'and the Fisher information matrix becomes J(x) ≈ N ∑K1 k=1 '\n",
      "          'αkS(x;θk), where S(x;θk) =\\n'\n",
      "          '〈 ∂ ln p(r|x;θk)\\n'\n",
      "          '∂x ∂ ln p(r|x;θk) ∂xT 〉 r|x and αk > 0 (k = 1, · · · ,K1) is the '\n",
      "          'population density of param-\\n'\n",
      "          'eter θk, with ∑K1 k=1 αk = 1, and 1 ≤ K1 ≤ N (see Appendix A.1 for '\n",
      "          'details). Since the cerebral cortex usually forms functional column '\n",
      "          'structures and each column is composed of neurons with the same '\n",
      "          'properties (Hubel & Wiesel, 1962), the positive integer K1 can be '\n",
      "          'regarded as the number of distinct classes in the neural '\n",
      "          'population.\\n'\n",
      "          'Therefore, given the activation function f(x;θk), our goal becomes '\n",
      "          'to find the optimal population distribution density αk of parameter '\n",
      "          'vector θk so that the MI between the stimulus x and the response r '\n",
      "          'is maximized. By Eq. (1), our optimization problem can be stated as '\n",
      "          'follows:\\n'\n",
      "          'minimize QG[{αk}] = − 1\\n'\n",
      "          '2 〈ln (det (G(x)))〉x , (5)\\n'\n",
      "          'subject to K1∑ k=1 αk = 1, αk > 0, ∀k = 1, · · · ,K1. (6)\\n'\n",
      "          'Since QG[{αk}] is a convex function of {αk} (Huang & Zhang, 2017), '\n",
      "          'we can readily find the optimal solution for small K by efficient '\n",
      "          'numerical methods. For large K, however, finding an optimal '\n",
      "          'solution by numerical methods becomes intractable. In the following '\n",
      "          'we will propose an alternative approach to this problem. Instead of '\n",
      "          'directly solving for the density distribution {αk}, we optimize the '\n",
      "          'parameters {αk} and {θk} simultaneously under a hierarchical '\n",
      "          'infomax framework.'},\n",
      " {'heading': '2.2 HIERARCHICAL INFOMAX',\n",
      "  'text': 'For clarity, we consider neuron model with Poisson spikes although '\n",
      "          'our method is easily applicable to other noise models. The '\n",
      "          'activation function f(x;θn) is generally a nonlinear function, such '\n",
      "          'as sigmoid and rectified linear unit (ReLU) (Nair & Hinton, 2010). '\n",
      "          'We assume that the nonlinear function for the n-th neuron has the '\n",
      "          'following form: f(x;θn) = f̃(yn; θ̃n), where\\n'\n",
      "          'yn = w T nx. (7)\\n'\n",
      "          'with wn being aK-dimensional weights vector, f̃(yn; θ̃n) is a '\n",
      "          'nonlinear function, θn = (wTn , θ̃ T n ) T and θ̃n are the '\n",
      "          'parameter vectors (n = 1, · · · , N ). In general, it is very '\n",
      "          'difficult to find the optimal parameters, θn, n = 1, · · · , N , '\n",
      "          'for the following reasons. First, the number of output neuronsN is '\n",
      "          'very large, usuallyN K. Second, the activation function f(x;θn) is '\n",
      "          'a nonlinear function, which usually leads to a nonconvex '\n",
      "          'optimization problem. For nonconvex optimization problems, the '\n",
      "          'selection of initial values often has a great influence on the '\n",
      "          'final optimization results. Our approach meets these challenges by '\n",
      "          'making better use of the large number of neurons and by finding '\n",
      "          'good initial values by a hierarchical infomax method.\\n'\n",
      "          'We divide the nonlinear transformation into two stages, mapping '\n",
      "          'first from x to yn (n = 1, · · · , N ), and then from yn to f̃(yn; '\n",
      "          'θ̃n), where yn can be regarded as the membrane potential of the '\n",
      "          'n-th neuron, and f̃(yn; θ̃n) as its firing rate. As with the real '\n",
      "          'neurons, we assume that the membrane potential is corrupted by '\n",
      "          'noise:\\n'\n",
      "          'Y̆n = Yn + Zn, (8) where Zn ∼ N ( 0,σ2 ) is a normal distribution '\n",
      "          'with mean 0 and variance σ2. Then the mean membrane potential of '\n",
      "          'the k-th class subpopulation with Nk = Nαk neurons is given by\\n'\n",
      "          'Ȳk = 1\\n'\n",
      "          'Nk Nk∑ n=1 Y̆kn = Yk + Z̄k, k = 1, · · · ,K1, (9)\\n'\n",
      "          'Z̄k ∼ N (0, N−1k σ 2). (10)\\n'\n",
      "          'Define vectors y̆ = (y̆1, · · · , y̆N )T , ȳ = (ȳ1, · · · , '\n",
      "          'ȳK1)T and y = (y1, · · · , yK1)T , where yk = wTk x (k = 1, · · · '\n",
      "          ',K1). Notice that y̆n (n = 1, · · · , N ) is also divided into K1 '\n",
      "          'classes, the same as for rn. If we assume f(x;θk) = f̃(ȳk; θ̃k), '\n",
      "          'i.e. assuming an additive Gaussian noise for yn (see Eq. 9), then '\n",
      "          'the random variables X , Y , Y̆ , Ȳ and R form a Markov chain, '\n",
      "          'denoted by X → Y → Y̆ → Ȳ → R (see Figure 1), and we have the '\n",
      "          'following proposition (see Appendix A.2).\\n'\n",
      "          'Proposition 1. With the random variables X , Y , Y̆ , Ȳ , R and '\n",
      "          'Markov chain X → Y → Y̆ → Ȳ → R, the following equations hold,\\n'\n",
      "          'I(X;R) = I(Y ;R) ≤ I(Y̆ ;R) ≤ I(Ȳ ;R), (11) I(X;R) ≤ I(X; Ȳ ) = '\n",
      "          'I(X; Y̆ ) ≤ I(X;Y ), (12)\\n'\n",
      "          'and for large Nk (k = 1, · · · ,K1),\\n'\n",
      "          \"I(Y̆ ;R) ' I(Ȳ ;R) ' I(Y ;R) = I(X;R), (13) I(X;Y ) ' I(X; Ȳ ) = \"\n",
      "          'I(X; Y̆ ). (14)\\n'\n",
      "          'A major advantage of incorporating membrane noise is that it '\n",
      "          'facilitates finding the optimal solution by using the infomax '\n",
      "          'principle. Moreover, the optimal solution obtained this way is more '\n",
      "          'robust; that is, it discourages overfitting and has a strong '\n",
      "          'ability to resist distortion. With vanishing noise σ2 → 0, we have '\n",
      "          \"Ȳk → Yk, f̃(ȳk; θ̃k) ' f̃(yk; θ̃k) = f(x;θk), so that Eqs. (13) \"\n",
      "          'and (14) hold as in the case of large Nk.\\n'\n",
      "          'To optimize MI I(Y ;R), the probability distribution of random '\n",
      "          'variable Y , p(y), needs to be determined, i.e. maximizing I(Y ;R) '\n",
      "          'about p(y) under some constraints should yield an optimal '\n",
      "          'distribution: p∗(y) = arg maxp(y) I(Y ;R). Let C = maxp(y) I (Y ;R) '\n",
      "          'be the channel capacity of neural population coding, and we always '\n",
      "          'have I(X;R) ≤ C (Huang & Zhang, 2017). To find a suitable linear '\n",
      "          'transformation from X to Y that is compatible with this '\n",
      "          'distribution p∗(y), a reasonable choice is to maximize I(X; Y̆ ) (≤ '\n",
      "          'I(X;Y )), where Y̆ is a noise-corrupted version of Y . This implies '\n",
      "          'minimum information loss in the first transformation step. However, '\n",
      "          'there may exist many transformations from X to Y̆ that maximize '\n",
      "          'I(X; Y̆ ) (see Appendix A.3.1). Ideally, if we can find a '\n",
      "          'transformation that maximizes both I(X; Y̆ ) and I(Y ;R) '\n",
      "          'simultaneously, then I(X;R) reaches its maximum value: I(X;R) = '\n",
      "          'maxp(y) I (Y ;R) = C.\\n'\n",
      "          'From the discussion above we see that maximizing I(X;R) can be '\n",
      "          'divided into two steps, namely, maximizing I(X; Y̆ ) and maximizing '\n",
      "          'I(Y ;R). The optimal solutions of max I(X; Y̆ ) and max I(Y ;R) '\n",
      "          'will provide a good initial approximation that tend to be very '\n",
      "          'close to the optimal solution of max I(X;R).\\n'\n",
      "          'Similarly, we can extend this method to multilayer neural '\n",
      "          'population networks. For example, a twolayer network with outputs '\n",
      "          'R(1) and R(2) form a Markov chain, X → R̃(1) → R(1) → R̄(1) →\\n'\n",
      "          'R(2), where random variable R̃(1) is similar to Y , random variable '\n",
      "          'R(1) is similar to Y̆ , and R̄(1) is similar to Ȳ in the above. '\n",
      "          'Then we can show that the optimal solution of max I(X;R(2)) can be '\n",
      "          'approximated by the solutions of max I(X;R(1)) and max '\n",
      "          \"I(R̃(1);R(2)), with I(R̃(1);R(2)) ' I(R̄(1);R(2)).\\n\"\n",
      "          'More generally, consider a highly nonlinear feedforward neural '\n",
      "          'network that maps the input x to output z, with z = F (x;θ) = hL ◦ '\n",
      "          '· · · ◦ h1 (x), where hl (l = 1, · · · , L) is a linear or '\n",
      "          'nonlinear function (Montufar et al., 2014). We aim to find the '\n",
      "          'optimal parameter θ by maximizing I (X;Z). It is usually difficult '\n",
      "          'to solve the optimization problem when there are many local extrema '\n",
      "          'for F (x;θ). However, if each function hl is easy to optimize, then '\n",
      "          'we can use the hierarchical infomax method described above to get a '\n",
      "          'good initial approximation to its global optimization solution, and '\n",
      "          'go from there to find the final optimal solution. This '\n",
      "          'information-theoretic consideration from the neural population '\n",
      "          'coding point of view may help explain why deep structure networks '\n",
      "          'with unsupervised pre-training have a powerful ability for learning '\n",
      "          'representations.'},\n",
      " {'heading': '2.3 THE OBJECTIVE FUNCTION',\n",
      "  'text': 'The optimization processes for maximizing I(X; Y̆ ) and maximizing '\n",
      "          'I(Y ;R) are discussed in detail in Appendix A.3. First, by '\n",
      "          'maximizing I(X; Y̆ ) (see Appendix A.3.1 for details), we can get '\n",
      "          'the optimal weight parameter wk (k = 1, · · · ,K1, see Eq. 7) and '\n",
      "          'its population density αk (see Eq. 6) which satisfy\\n'\n",
      "          'W = [w1, · · · ,wK1 ] = aU0Σ −1/2 0 C, (15) α1 = · · · = αK1 = K−11 '\n",
      "          ', (16)\\n'\n",
      "          'where a = √ K1K −1 0 , C = [c1, · · · , cK1 ] ∈ RK0×K1 , CC\\n'\n",
      "          'T = IK0 , IK0 is a K0 × K0 identity matrix with integer K0 ∈ [1,K], '\n",
      "          'the diagonal matrix Σ0 ∈ RK0×K0 and matrix U0 ∈ RK×K0 are given in '\n",
      "          '(A.44) and (A.45), with K0 given by Eq. (A.52). Matrices Σ0 and U0 '\n",
      "          'can be obtained by Σ and U with UT0 U0 = IK0 and U0Σ0U T 0 ≈ UΣU T '\n",
      "          '≈ 〈 xxT 〉 x\\n'\n",
      "          '(see Eq. A.23). The optimal weight parameter wk (15) means that the '\n",
      "          'input variable x must first undergo a whiteninglike transformation '\n",
      "          'x̂ = Σ−1/20 U T 0 x, and then goes through the transformation y = '\n",
      "          'aC\\n'\n",
      "          'T x̂, with matrix C to be optimized below. Note that weight matrix '\n",
      "          'W satisfies rank(W) = min(K0,K1), which is a low rank matrix, and '\n",
      "          'its low dimensionality helps reduce overfitting during training '\n",
      "          '(see Appendix A.3.1).\\n'\n",
      "          'By maximizing I (Y ;R) (see Appendix A.3.2), we further solve the '\n",
      "          'the optimal parameters θ̃k for the nonlinear functions f̃(yk; θ̃k), '\n",
      "          'k = 1, · · · ,K1. Finally, the objective function for our '\n",
      "          'optimization problem (Eqs. 5 and 6) turns into (see Appendix A.3.3 '\n",
      "          'for details):\\n'\n",
      "          'minimize Q [C] =− 1 2\\n'\n",
      "          '〈 ln ( det ( CΦ̂C T ))〉\\n'\n",
      "          'x̂ , (17)\\n'\n",
      "          'subject to CCT = IK0 , (18) where Φ̂ = diag ( φ(ŷ1) 2, · · · , '\n",
      "          'φ(ŷK1)2 ) , φ(ŷk) = a−1 |∂gk(ŷk)/∂ŷk| (k = 1, · · · ,K1), '\n",
      "          'gk(ŷk) =\\n'\n",
      "          '2 √ f̃(ŷk; θ̃k), ŷk = a−1yk = cTk x̂, and x̂ = Σ −1/2 0 U T 0 x. '\n",
      "          'We apply the gradient descent method to optimize the objective '\n",
      "          'function, with the gradient of Q[C] given by:\\n'\n",
      "          'dQ[C]\\n'\n",
      "          'dC = −\\n'\n",
      "          '〈( CΦ̂C T )−1 CΦ̂ + x̂ωT 〉\\n'\n",
      "          'x̂\\n'\n",
      "          ', (19)\\n'\n",
      "          'where ω = (ω1, · · · , ωK1) T , ωk = φ(ŷk)φ′(ŷk)cTk\\n'\n",
      "          '( CΦ̂C T )−1 ck, k = 1, · · · ,K1.\\n'\n",
      "          'When K0 = K1 (or K0 > K1), the objective function Q[C] can be '\n",
      "          'reduced to a simpler form, and its gradient is also easy to compute '\n",
      "          '(see Appendix A.4.1). However, when K0 < K1, it is computationally '\n",
      "          'expensive to update C by applying the gradient of Q[C] directly, '\n",
      "          'since it requires matrix inversion for every x̂. We use another '\n",
      "          'objective function Q̂[C] (see Eq. A.118) which is an approximation '\n",
      "          'to Q[C], but its gradient is easier to compute (see Appendix '\n",
      "          'A.4.2). The function\\n'\n",
      "          'Q̂[C] is the approximation of Q[C], ideally they have the same '\n",
      "          'optimal solution for the parameter C.\\n'\n",
      "          'Usually, for optimizing the objective in Eq. 17, the orthogonality '\n",
      "          'constraint (Eq. 18) is unnecessary. However, this orthogonality '\n",
      "          'constraint can accelerate the convergence rate if we employ it for '\n",
      "          'the initial iteration to update C (see Appendix A.5).'},\n",
      " {'heading': '3 EXPERIMENTAL RESULTS',\n",
      "  'text': 'We have applied our methods to the natural images from Olshausen’s '\n",
      "          'image dataset (Olshausen & Field, 1996) and the images of '\n",
      "          'handwritten digits from MNIST dataset (LeCun et al., 1998) using '\n",
      "          'Matlab 2016a on a computer with 12 Intel CPU cores (2.4 GHz). The '\n",
      "          'gray level of each raw image was normalized to the range of 0 to 1. '\n",
      "          'M image patches with size w × w = K for training were randomly '\n",
      "          'sampled from the images. We used the Poisson neuron model with a '\n",
      "          'modified sigmoidal\\n'\n",
      "          'tuning function f̃(y; θ̃) = 1 4(1+exp(−βy−b))2 , with g(y) = 2 √ '\n",
      "          'f̃(y; θ̃) = 11+exp(−βy−b) , where\\n'\n",
      "          'θ̃ = (β, b) T . We obtained the initial values (see Appendix '\n",
      "          'A.3.2): b0 = 0 and β0 ≈ 1.81 √ K1K −1 0 . For our experiments, we '\n",
      "          'set β = 0.5β0 for iteration epoch t = 1, · · · , t0 and β = β0 for '\n",
      "          't = t0 + 1, · · · , tmax, where t0 = 50. Firstly, we tested the '\n",
      "          'case of K = K0 = K1 = 144 and randomly sampled M = 105 image '\n",
      "          'patches with size 12×12 from the Olshausen’s natural images, '\n",
      "          'assuming thatN = 106 neurons were divided into K1 = 144 classes and '\n",
      "          '= 1 (see Eq. A.52 in Appendix). The input patches were preprocessed '\n",
      "          'by the ZCA whitening filters (see Eq. A.68). To test our '\n",
      "          'algorithms, we chose the batch size to be equal to the number of '\n",
      "          'training samples M , although we could also choose a smaller batch '\n",
      "          'size. We updated the matrix C from a random start, and set '\n",
      "          'parameters tmax = 300, v1 = 0.4, and τ = 0.8 for all experiments.\\n'\n",
      "          'In this case, the optimal solution C looked similar to the optimal '\n",
      "          'solution of IICA (Bell & Sejnowski, 1997). We also compared with '\n",
      "          'the fast ICA algorithm (FICA) (Hyvärinen, 1999), which is faster '\n",
      "          'than IICA. We also tested the restricted Boltzmann machine (RBM) '\n",
      "          '(Hinton et al., 2006) for a unsupervised learning of '\n",
      "          'representations, and found that it could not easily learn '\n",
      "          'Gabor-like filters from Olshausen’s image dataset as trained by '\n",
      "          'contrastive divergence. However, an improved method by adding a '\n",
      "          'sparsity constraint on the output units, e.g., sparse RBM (SRBM) '\n",
      "          '(Lee et al., 2008) or sparse autoencoder (Hinton, 2010), could '\n",
      "          'attain Gabor-like filters from this dataset. Similar results with '\n",
      "          'Gabor-like filters were also reproduced by the denoising '\n",
      "          'autoencoders (Vincent et al., 2010), which method requires a '\n",
      "          'careful choice of parameters, such as noise level, learning rate, '\n",
      "          'and batch size.\\n'\n",
      "          'In order to compare our methods, i.e. Algorithm 1 (Alg.1, see '\n",
      "          'Appendix A.4.1) and Algorithm 2 (Alg.2, see Appendix A.4.2), with '\n",
      "          'other methods, i.e. IICA, FICA and SRBM, we implemented these '\n",
      "          'algorithms using the same initial weights and the same training '\n",
      "          'data set (i.e. 105 image patches preprocessed by the ZCA whitening '\n",
      "          'filters). To get a good result by IICA, we must carefully select '\n",
      "          'the parameters; we set the batch size as 50, the initial learning '\n",
      "          'rate as 0.01, and final learning rate as 0.0001, with an '\n",
      "          'exponential decay with the epoch of iterations. IICA tends to have '\n",
      "          'a faster convergence rate for a bigger batch size but it may become '\n",
      "          'harder to escape local minima. For FICA, we chose the nonlinearity '\n",
      "          'function f(u) = log cosh(u) as contrast function, and for SRBM, we '\n",
      "          'set the sparseness control constant p as 0.01 and 0.03. The number '\n",
      "          'of epoches for iterations was set to 300 for all algorithms. Figure '\n",
      "          '2 shows the filters learned by our methods and other methods. Each '\n",
      "          'filter in Figure 2(a) corresponds to a column vector of matrix Č '\n",
      "          '(see Eq. A.69), where each vector for display is normalized by čk '\n",
      "          '← čk/max(|č1,k|, · · · , |čK,k|), k = 1, · · · ,K1. The results '\n",
      "          'in Figures 2(a), 2(b) and 2(c) look very similar to one another, '\n",
      "          'and slightly different from the results in Figure 2(d) and 2(e). '\n",
      "          'There are no Gabor-like filters in Figure 2(f), which corresponds '\n",
      "          'to SRBM with p = 0.03.\\n'\n",
      "          'Figure 3 shows how the coefficient entropy (CFE) (see Eq. A.122) '\n",
      "          'and the conditional entropy (CDE) (see Eq. A.125) varied with '\n",
      "          'training time. We calculated CFE and CDE by sampling once every 10 '\n",
      "          'epoches from a total of 300 epoches. These results show that our '\n",
      "          'algorithms had a fast convergence rate towards stable solutions '\n",
      "          'while having CFE and CDE values similar to the algorithm of IICA, '\n",
      "          'which converged much more slowly. Here the values of CFE and CDE '\n",
      "          'should be as small\\n'\n",
      "          '100 101 102\\n'\n",
      "          '1.8\\n'\n",
      "          '1.85\\n'\n",
      "          '1.9\\n'\n",
      "          '1.95\\n'\n",
      "          '2\\n'\n",
      "          'co ef\\n'\n",
      "          'fic ie\\n'\n",
      "          'nt e\\n'\n",
      "          'nt ro\\n'\n",
      "          'py (\\n'\n",
      "          'bi ts ) Alg.1 Alg.2 IICA FICA SRBM (p = 0.01) SRBM (p = 0.03)\\n'\n",
      "          '100 101 102\\n'\n",
      "          '-400\\n'\n",
      "          '-350\\n'\n",
      "          '-300\\n'\n",
      "          '-250\\n'\n",
      "          '-200\\n'\n",
      "          '-150\\n'\n",
      "          'co nd\\n'\n",
      "          'iti on\\n'\n",
      "          'al e\\n'\n",
      "          'nt ro\\n'\n",
      "          'py (\\n'\n",
      "          'bi ts ) Alg.1 Alg.2 IICA\\n'\n",
      "          '100 101 102\\n'\n",
      "          '-200\\n'\n",
      "          '-100\\n'\n",
      "          '0\\n'\n",
      "          '100\\n'\n",
      "          '200\\n'\n",
      "          '300\\n'\n",
      "          'co nd\\n'\n",
      "          'iti on\\n'\n",
      "          'al e\\n'\n",
      "          'nt ro\\n'\n",
      "          'py (\\n'\n",
      "          'bi ts\\n'\n",
      "          ')\\n'\n",
      "          'SRBM (p = 0.01) SRBM (p = 0.03) SRBM (p = 0.05) SRBM (p = 0.10)\\n'\n",
      "          'as possible for a good representation learned from the same data '\n",
      "          'set. Here we set epoch number t0 = 50 in our algorithms (see Alg.1 '\n",
      "          'and Alg.2), and the start time was set to 1 second. This explains '\n",
      "          'the step seen in Figure 3 (b) for Alg.1 and Alg.2 since the '\n",
      "          'parameter β was updated when epoch number t = t0. FICA had a '\n",
      "          'convergence rate close to our algorithms but had a big CFE, which '\n",
      "          'is reflected by the quality of the filter results in Figure 2. The '\n",
      "          'convergence rate and CFE for SRBM were close to IICA, but SRBM had '\n",
      "          'a much bigger CDE than IICA, which implies that the information had '\n",
      "          'a greater loss when passing through the system optimized by SRBM '\n",
      "          'than by IICA or our methods.\\n'\n",
      "          'From Figure 3(c) we see that the CDE (or MI I(X;R), see Eq. A.124 '\n",
      "          'and A.125) decreases (or increases) with the increase of the value '\n",
      "          'of the sparseness control constant p. Note that a smaller p means '\n",
      "          'sparser outputs. Hence, in this sense, increasing sparsity may '\n",
      "          'result in sacrificing some information. On the other hand, a weak '\n",
      "          'sparsity constraint may lead to failure of learning Gaborlike '\n",
      "          'filters (see Figure 2(f)), and increasing sparsity has an advantage '\n",
      "          'in reducing the impact of noise in many practical cases. Similar '\n",
      "          'situation also occurs in sparse coding (Olshausen & Field, 1997), '\n",
      "          'which provides a class of algorithms for learning overcomplete '\n",
      "          'dictionary representations of the input signals. However, its '\n",
      "          'training is time consuming due to its expensive computational cost, '\n",
      "          'although many new training algorithms have emerged (e.g. Aharon et '\n",
      "          'al., 2006; Elad & Aharon, 2006; Lee et al., 2006; Mairal et al., '\n",
      "          '2010). See Appendix A.5 for additional experimental results.'},\n",
      " {'heading': '4 CONCLUSIONS',\n",
      "  'text': 'In this paper, we have presented a framework for unsupervised '\n",
      "          'learning of representations via information maximization for neural '\n",
      "          'populations. Information theory is a powerful tool for machine '\n",
      "          'learning and it also provides a benchmark of optimization principle '\n",
      "          'for neural information processing in nervous systems. Our framework '\n",
      "          'is based on an asymptotic approximation to MI for a large-scale '\n",
      "          'neural population. To optimize the infomax objective, we first use '\n",
      "          'hierarchical infomax to obtain a good approximation to the global '\n",
      "          'optimal solution. Analytical solutions of the hierarchical infomax '\n",
      "          'are further improved by a fast convergence algorithm based on '\n",
      "          'gradient descent. This method allows us to optimize highly '\n",
      "          'nonlinear neural networks via hierarchical optimization using '\n",
      "          'infomax principle.\\n'\n",
      "          'From the viewpoint of information theory, the unsupervised '\n",
      "          'pre-training for deep learning (Hinton & Salakhutdinov, 2006; '\n",
      "          'Bengio et al., 2007) may be reinterpreted as a process of '\n",
      "          'hierarchical infomax, which might help explain why unsupervised '\n",
      "          'pre-training helps deep learning (Erhan et al., 2010). In our '\n",
      "          'framework, a pre-whitening step can emerge naturally by the '\n",
      "          'hierarchical infomax, which might also explain why a pre-whitening '\n",
      "          'step is useful for training in many learning algorithms (Coates et '\n",
      "          'al., 2011; Bengio, 2012).\\n'\n",
      "          'Our model naturally incorporates a considerable degree of '\n",
      "          'biological realism. It allows the optimization of a large-scale '\n",
      "          'neural population with noisy spiking neurons while taking into '\n",
      "          'account of multiple biological constraints, such as membrane noise, '\n",
      "          'limited energy, and bounded connection weights. We employ a '\n",
      "          'technique to attain a low-rank weight matrix for optimization, so '\n",
      "          'as to reduce the influence of noise and discourage overfitting '\n",
      "          'during training. In our model, many parameters are optimized, '\n",
      "          'including the population density of parameters, filter weight '\n",
      "          'vectors, and parameters for nonlinear tuning functions. Optimizing '\n",
      "          'all these model parameters could not be easily done by many other '\n",
      "          'methods.\\n'\n",
      "          'Our experimental results suggest that our method for unsupervised '\n",
      "          'learning of representations has obvious advantages in its training '\n",
      "          'speed and robustness over the main existing methods. Our model has '\n",
      "          'a nonlinear feedforward structure and is convenient for fast '\n",
      "          'learning and inference. This simple and flexible framework for '\n",
      "          'unsupervised learning of presentations should be readily extended '\n",
      "          'to training deep structure networks. In future work, it would '\n",
      "          'interesting to use our method to train deep structure networks with '\n",
      "          'either unsupervised or supervised learning.'},\n",
      " {'heading': 'ACKNOWLEDGMENTS',\n",
      "  'text': 'We thank Prof. Honglak Lee for sharing Matlab code for algorithm '\n",
      "          'comparison, Prof. Shan Tan for discussions and comments and Kai Liu '\n",
      "          'for helping draw Figure 1. Supported by grant NIH-NIDCD R01 '\n",
      "          'DC013698.'},\n",
      " {'heading': 'APPENDIX', 'text': ''},\n",
      " {'heading': 'A.1 FORMULAS FOR APPROXIMATION OF MUTUAL INFORMATION',\n",
      "  'text': 'It follows from I(X;R) = 〈\\n'\n",
      "          'ln p(x|r)p(x) 〉 r,x and Eq. (1) that the conditional entropy should '\n",
      "          'read:\\n'\n",
      "          \"H(X|R) = −〈ln p(x|r)〉r,x ' − 1\\n\"\n",
      "          '2\\n'\n",
      "          '〈 ln ( det ( G(x)\\n'\n",
      "          '2πe ))〉 x . (A.1)\\n'\n",
      "          'The Fisher information matrix J(x) (see Eq. 3), which is symmetric '\n",
      "          'and positive semidefinite, can be written also as\\n'\n",
      "          'J(x) =\\n'\n",
      "          '〈 ∂ ln p(r|x)\\n'\n",
      "          '∂x\\n'\n",
      "          '∂ ln p(r|x) ∂xT 〉 r|x . (A.2)\\n'\n",
      "          'If we suppose p(r|x) is conditional independent, namely, p(r|x) = '\n",
      "          '∏N n=1 p(rn|x;θn), then we have (see Huang & Zhang, 2017)\\n'\n",
      "          'J(x) = N ∫ Θ p(θ)S(x;θ)dθ, (A.3)\\n'\n",
      "          'S(x;θ) =\\n'\n",
      "          '〈 ∂ ln p(r|x;θ)\\n'\n",
      "          '∂x\\n'\n",
      "          '∂ ln p(r|x;θ) ∂xT 〉 r|x , (A.4)\\n'\n",
      "          'where p(θ) is the population density function of parameter θ,\\n'\n",
      "          'p(θ) = 1\\n'\n",
      "          'N N∑ n=1 δ(θ − θn), (A.5)\\n'\n",
      "          'and δ(·) denotes the Dirac delta function. It can be proved that '\n",
      "          'the approximation function of MI IG[p(θ)] (Eq. 1) is concave about '\n",
      "          'p(θ) (Huang & Zhang, 2017). In Eq. (A.3), we can approximate the '\n",
      "          'continuous integral by a discrete summation for numerical '\n",
      "          'computation,\\n'\n",
      "          'J(x) ≈ N K1∑ k=1 αkS(x;θk), (A.6)\\n'\n",
      "          'where ∑K1 k=1 αk = 1, αk > 0, k = 1, · · · ,K1, 1 ≤ K1 ≤ N .\\n'\n",
      "          'For Poisson neuron model, by Eq. (A.4) we have (see Huang & Zhang, '\n",
      "          '2017)\\n'\n",
      "          'p(r|x;θ) = f(x;θ) r\\n'\n",
      "          'r! exp (−f(x;θ)), (A.7)\\n'\n",
      "          'S(x;θ) = 1\\n'\n",
      "          'f(x;θ)\\n'\n",
      "          '∂f(x;θ)\\n'\n",
      "          '∂x\\n'\n",
      "          '∂f(x;θ)\\n'\n",
      "          '∂xT\\n'\n",
      "          '= ∂g(x;θ)\\n'\n",
      "          '∂x\\n'\n",
      "          '∂g(x;θ)\\n'\n",
      "          '∂xT , (A.8)\\n'\n",
      "          'where f(x;θ) ≥ 0 is the activation function (mean response) of '\n",
      "          'neuron and g(x;θ) = 2 √ f(x;θ). (A.9)\\n'\n",
      "          'Similarly, for Gaussian noise model, we have\\n'\n",
      "          'p(r|x;θ) = 1 σ √ 2π exp\\n'\n",
      "          '( − (r − f(x;θ)) 2\\n'\n",
      "          '2σ2\\n'\n",
      "          ') , (A.10)\\n'\n",
      "          'S(x;θ) = 1 σ2 ∂f(x;θ) ∂x ∂f(x;θ) ∂xT , (A.11)\\n'\n",
      "          'where σ > 0 denotes the standard deviation of noise.\\n'\n",
      "          'Sometimes we do not know the specific form of p(x) and only know M '\n",
      "          'samples, x1, · · · , xM , which are independent and identically '\n",
      "          'distributed (i.i.d.) samples drawn from the distribution p(x). Then '\n",
      "          'we can use the empirical average to approximate the integral in Eq. '\n",
      "          '(1):\\n'\n",
      "          'IG ≈ 1\\n'\n",
      "          '2 M∑ m=1 ln (det (G(xm))) +H (X) . (A.12)'},\n",
      " {'heading': 'A.2 PROOF OF PROPOSITION 1',\n",
      "  'text': 'Proof. It follows from the data-processing inequality (Cover & '\n",
      "          'Thomas, 2006) that\\n'\n",
      "          'I(X;R) ≤ I(Y ;R) ≤ I(Y̆ ;R) ≤ I(Ȳ ;R), (A.13) I(X;R) ≤ I(X; Ȳ ) ≤ '\n",
      "          'I(X; Y̆ ) ≤ I(X;Y ). (A.14)\\n'\n",
      "          'Since p(ȳk|x) = p(y̆k1 , · · · , y̆kNk |x) = N (w T k x, N −1 k σ '\n",
      "          '2), k = 1, · · · ,K1, (A.15)\\n'\n",
      "          'we have\\n'\n",
      "          'p(ȳ|x) = p(y̆|x), (A.16) p(ȳ) = p(y̆), (A.17)\\n'\n",
      "          'I(X; Ȳ ) = I(X; Y̆ ). (A.18)\\n'\n",
      "          'Hence, by (A.14) and (A.18), expression (12) holds. On the other '\n",
      "          'hand, when Nk is large, from Eq. (10) we know that the distribution '\n",
      "          'of Z̄k, namely, N ( 0,N−1k σ 2 ) , approaches a Dirac delta '\n",
      "          \"function δ(z̄k). Then by (7) and (9) we have p (r|ȳ) ' p(r|y) = p \"\n",
      "          '(r|x) and\\n'\n",
      "          'I(X;R) = I (Y ;R)− 〈\\n'\n",
      "          'ln p (r|y) p (r|x) 〉 r,x = I (Y ;R) , (A.19)\\n'\n",
      "          'I (Y ;R) = I(Ȳ ;R)− 〈\\n'\n",
      "          \"ln p (r|ȳ) p (r|y) 〉 r,y,ȳ ' I(Ȳ ;R), (A.20)\\n\"\n",
      "          'I (Y ;R) = I(Y̆ ;R)− 〈\\n'\n",
      "          \"ln p (r|y̆) p (r|y) 〉 r,y,y̆ ' I(Y̆ ;R), (A.21)\\n\"\n",
      "          'I(X;Y ) = I(X; Ȳ )− 〈\\n'\n",
      "          \"ln p (x|ȳ) p (x|y) 〉 x,y,ȳ ' I(X; Ȳ ). (A.22)\\n\"\n",
      "          'It follows from (A.13) and (A.19) that (11) holds. Combining (11), '\n",
      "          '(12) and (A.20)–(A.22), we immediately get (13) and (14). This '\n",
      "          'completes the proof of Proposition 1.\\n'\n",
      "          'A.3 HIERARCHICAL OPTIMIZATION FOR MAXIMIZING I(X;R)\\n'\n",
      "          'In the following, we will discuss the optimization procedure for '\n",
      "          'maximizing I(X;R) in two stages: maximizing I(X; Y̆ ) and '\n",
      "          'maximizing I(Y ;R).'},\n",
      " {'heading': 'A.3.1 THE 1ST STAGE',\n",
      "  'text': 'In the first stage, our goal is to maximize the MI I(X; Y̆ ) and '\n",
      "          'get the optimal parameters wk (k = 1, · · · ,K1). Assume that the '\n",
      "          'stimulus x has zero mean (if not, let x ← x − 〈x〉x) and covariance '\n",
      "          'matrix Σx. It follows from eigendecomposition that\\n'\n",
      "          'Σx = 〈 xxT 〉 x ≈ 1 M − 1 XXT = UΣUT , (A.23)\\n'\n",
      "          'where X = [x1, · · · , xM ], U = [u1, · · · ,uK ] ∈ RK×K is an '\n",
      "          'unitary orthogonal matrix and Σ = diag ( σ21 , · · · , σ2K ) is a '\n",
      "          'positive diagonal matrix with σ1 ≥ · · · ≥ σK > 0. Define\\n'\n",
      "          'x̃ = Σ−1/2UTx, (A.24)\\n'\n",
      "          'w̃k = Σ 1/2UTwk, (A.25)\\n'\n",
      "          'yk = w̃ T k x̃, (A.26)\\n'\n",
      "          'where k = 1, · · · ,K1. The covariance matrix of x̃ is given by Σx̃ '\n",
      "          '= 〈 x̃x̃T 〉 x̃ ≈ IK , (A.27)\\n'\n",
      "          'and IK is a K ×K identity matrix. From (1) and (A.11) we have I(X; '\n",
      "          'Y̆ ) = I(X̃; Y̆ ) and\\n'\n",
      "          \"I(X̃; Y̆ ) ' I ′G = 1\\n\"\n",
      "          '2 ln\\n'\n",
      "          '( det ( G̃\\n'\n",
      "          '2πe\\n'\n",
      "          ')) +H(X̃), (A.28)\\n'\n",
      "          'G̃ ≈ Nσ−2 K1∑ k=1 αkw̃kw̃ T k + IK . (A.29)\\n'\n",
      "          'The following approximations are useful (see Huang & Zhang, 2017):\\n'\n",
      "          'p(x̃) ≈ N (0, IK), (A.30)\\n'\n",
      "          'P(x̃) = −∂ 2 ln p (x̃)\\n'\n",
      "          '∂x̃∂x̃T ≈ IK . (A.31)\\n'\n",
      "          'By the central limit theorem, the distribution of random variable '\n",
      "          'X̃ is closer to a normal distribution than the distribution of the '\n",
      "          'original random variable X . On the other hand, the PCA models '\n",
      "          'assume multivariate gaussian data whereas the ICA models assume '\n",
      "          'multivariate non-gaussian data. Hence by a PCA-like whitening '\n",
      "          'transformation (A.24) we can use the approximation (A.31) with the '\n",
      "          'Laplace’s method of asymptotic expansion, which only requires that '\n",
      "          'the peak be close to its mean while random variable X̃ needs not be '\n",
      "          'exactly Gaussian.\\n'\n",
      "          'Without any constraints on the Gaussian channel of neural '\n",
      "          'populations, especially the peak firing rates, the capacity of this '\n",
      "          'channel may grow indefinitely: I(X̃; Y̆ ) → ∞. The most common '\n",
      "          'constraint on the neural populations is an energy or power '\n",
      "          'constraint which can also be regarded as a signal-to-noise ratio '\n",
      "          '(SNR) constraint. The SNR for the output y̆n of the n-th neuron is '\n",
      "          'given by\\n'\n",
      "          'SNRn = 1\\n'\n",
      "          'σ2\\n'\n",
      "          '〈( wTnx )2〉 x ≈ 1 σ2 w̃Tn w̃n, n = 1, · · · , N . (A.32)\\n'\n",
      "          'We require that 1\\n'\n",
      "          'N N∑ n=1 SNRn ≈ 1 σ2 K1∑ k=1 αkw̃ T k w̃k ≤ ρ, (A.33)\\n'\n",
      "          'where ρ is a positive constant. Then by Eq. (A.28), (A.29) and '\n",
      "          '(A.33), we have the following optimization problem:\\n'\n",
      "          'minimize Q′G[Ŵ] = − 1 2 ln ( det ( Nσ−2ŴŴ T + IK )) , (A.34)\\n'\n",
      "          'subject to h = Tr ( ŴŴ T ) − E ≤ 0, (A.35)\\n'\n",
      "          'where Tr (·) denotes matrix trace and\\n'\n",
      "          'Ŵ = W̃A 1/2\\n'\n",
      "          '= Σ1/2UTWA1/2 = [ŵ1, · · · , ŵK1 ], (A.36) A = diag (α1, · · · , '\n",
      "          'αK1), (A.37) W = [w1, · · · ,wK1 ], (A.38) W̃ = [w̃1, · · · , w̃K1 '\n",
      "          '], (A.39) E = ρσ2. (A.40)\\n'\n",
      "          'Here E is a constant that does not affect the final optimal '\n",
      "          'solution so we set E = 1. Then we obtain an optimal solution as '\n",
      "          'follows:\\n'\n",
      "          'W = aU0Σ −1/2 0 V T 0 , (A.41)\\n'\n",
      "          'A = K−11 IK1 , (A.42)\\n'\n",
      "          'a = √ EK1K −1 0 = √ K1K −1 0 , (A.43)\\n'\n",
      "          'Σ0 = diag ( σ21 , · · · , σ2K0 ) , (A.44) U0 = U (:, 1:K0) ∈ RK×K0 '\n",
      "          ', (A.45) V0 = V (:, 1:K0) ∈ RK1×K0 , (A.46)\\n'\n",
      "          'where V = [v1, · · · ,vK1 ] is an K1 ×K1 unitary orthogonal matrix, '\n",
      "          'parameter K0 represents the size of the reduced dimension (1 ≤ K0 ≤ '\n",
      "          'K), and its value will be determined below. Now the optimal '\n",
      "          'parameters wn (n = 1, · · · , N ) are clustered into K1 classes '\n",
      "          '(see Eq. A.6) and obey an uniform discrete distribution (see also '\n",
      "          'Eq. A.60 in Appendix A.3.2).\\n'\n",
      "          'When K = K0 = K1, the optimal solution of W in Eq. (A.41) is a '\n",
      "          'whitening-like filter. When V = IK , the optimal matrix W is the '\n",
      "          'principal component analysis (PCA) whitening filters. In the '\n",
      "          'symmetrical case with V = U, the optimal matrix W becomes a zero '\n",
      "          'component analysis (ZCA) whitening filter. IfK < K1, this case '\n",
      "          'leads to an overcomplete solution, whereas whenK0 ≤ K1 < K, the '\n",
      "          'undercomplete solution arises. Since K0 ≤ K1 and K0 ≤ K, Q′G '\n",
      "          'achieves its minimum when K0 = K. However, in practice other '\n",
      "          'factors may prevent it from reaching this minimum. For example, '\n",
      "          'consider the average of squared weights,\\n'\n",
      "          'ς = K1∑ k=1 αk ‖wk‖2 = Tr ( WAWT ) = E K0 K0∑ k=1 σ−2k , (A.47)\\n'\n",
      "          'where ‖·‖ denotes the Frobenius norm. The value of ς is extremely '\n",
      "          'large when any σk becomes vanishingly small. For real neurons these '\n",
      "          'weights of connection are not allowed to be too large. Hence we '\n",
      "          'impose a limitation on the weights: ς ≤ E1, where E1 is a positive '\n",
      "          'constant. This yields another constraint on the objective '\n",
      "          'function,\\n'\n",
      "          'h̃ = E\\n'\n",
      "          'K0 K0∑ k=1 σ−2k − E1 ≤ 0. (A.48)\\n'\n",
      "          'From (A.35) and (A.48) we get the optimal K0 = arg maxK̃0 ( EK̃−10 '\n",
      "          '∑K̃0 k=1 σ −2 k ) . By this constraint, small values of σ2k will '\n",
      "          'often result in K0 < K and a low-rank matrix W (Eq. A.41).\\n'\n",
      "          'On the other hand, the low-rank matrix W can filter out the noise '\n",
      "          'of stimulus x. Consider the transformation Y = WTX with X = [x1, · '\n",
      "          '· · , xM ] and Y = [y1, · · · , yM ] for M samples. It follows from '\n",
      "          'the singular value decomposition (SVD) of X that\\n'\n",
      "          'X = USṼ T , (A.49)\\n'\n",
      "          'where U is given in (A.23), Ṽ is aM×M unitary orthogonal matrix, S '\n",
      "          'is aK×M diagonal matrix with non-negative real numbers on the '\n",
      "          'diagonal, Sk,k = √ M − 1σk (k = 1, · · · ,K, K ≤M ), and SST = (M − '\n",
      "          '1)Σ. Let X̆ = √ M − 1U0Σ1/20 ṼT0 ≈ X, (A.50)\\n'\n",
      "          'where Ṽ0 = Ṽ (:, 1:K0) ∈ RM×K0 , Σ0 and U0 are given in (A.44) '\n",
      "          'and (A.45), respectively. Then\\n'\n",
      "          'Y = WTX = aV0Σ −1/2 0 U T 0 USṼ T = WT X̆ = a √ M − 1V0ṼT0 , '\n",
      "          '(A.51)\\n'\n",
      "          'where X̆ can be regarded as a denoised version of X. The '\n",
      "          'determination of the effective rank K0 ≤ K of the matrix X̆ by '\n",
      "          'using singular values is based on various criteria (Konstantinides '\n",
      "          '& Yao, 1988). Here we choose K0 as follows:\\n'\n",
      "          'K0 = arg min K′0\\n'\n",
      "          ' √√√√∑K′0k=1 σ2k∑K\\n'\n",
      "          'k=1 σ 2 k\\n'\n",
      "          '≥  , (A.52) where is a positive constant (0 < ≤ 1). Another '\n",
      "          'advantage of a low-rank matrix W is that it can significantly '\n",
      "          'reduce overfitting for learning neural population parameters. In '\n",
      "          'practice, the constraint (A.47) is equivalent to a weight-decay '\n",
      "          'regularization term used in many other optimization problems '\n",
      "          '(Cortes & Vapnik, 1995; Hinton, 2010), which can reduce overfitting '\n",
      "          'to the training data. To prevent the neural networks from '\n",
      "          'overfitting, Srivastava et al. (2014) presented a technique to '\n",
      "          'randomly drop units from the neural network during training, which '\n",
      "          'may in fact be regarded as an attempt to reduce the rank of the '\n",
      "          'weight matrix because the dropout can result in a sparser weights '\n",
      "          '(lower rank matrix). This means that the update is only concerned '\n",
      "          'with keeping the more important components, which is similar to '\n",
      "          'first performing a denoising process by the SVD low rank '\n",
      "          'approximation.\\n'\n",
      "          'In this stage, we have obtained the optimal parameter W (see A.41). '\n",
      "          'The optimal value of matrix V0 can also be determined, as shown in '\n",
      "          'Appendix A.3.3.'},\n",
      " {'heading': 'A.3.2 THE 2ND STAGE',\n",
      "  'text': 'For this stage, our goal is to maximize the MI I(Y ;R) and get the '\n",
      "          'optimal parameters θ̃k, k = 1, · · · ,K1. Here the input is y = '\n",
      "          '(y1, · · · , yK1)T and the output r = (r1, · · · , rN )T is also '\n",
      "          'clustered into K1 classes. The responses of Nk neurons in the k-th '\n",
      "          'subpopulation obey a Poisson distribution with mean f̃(eTk y; θ̃k), '\n",
      "          'where ek is a unit vector with 1 in the k-th element and yk = e T k '\n",
      "          'y. By (A.24) and (A.26), we have\\n'\n",
      "          '〈yk〉yk = 0, (A.53) σ2yk = 〈 y2k 〉 yk = ‖w̃k‖2 . (A.54)\\n'\n",
      "          'Then for large N , by (1)–(4) and (A.30) we can use the following '\n",
      "          'approximation,\\n'\n",
      "          \"I(Y ;R) ' ĬF = 1\\n\"\n",
      "          '2\\n'\n",
      "          '〈 ln ( det ( J̆(y)\\n'\n",
      "          '2πe ))〉 y +H(Y ), (A.55)\\n'\n",
      "          'where\\n'\n",
      "          'J̆(y) = diag ( Nα1 |g′1(y1)| 2 , · · · , NαK1 ∣∣g′K1(yK1)∣∣2) , '\n",
      "          '(A.56) g′k(yk) = ∂gk(yk)\\n'\n",
      "          '∂yk , k = 1, · · · ,K1, (A.57)\\n'\n",
      "          'gk(yk) = 2 √ f̃(yk; θ̃k), k = 1, · · · ,K1. (A.58)\\n'\n",
      "          'It is easy to get that\\n'\n",
      "          'ĬF = 1\\n'\n",
      "          '2 K1∑ k=1\\n'\n",
      "          '〈 ln ( Nαk |g′k(yk)| 2\\n'\n",
      "          '2πe\\n'\n",
      "          ')〉 y +H(Y )\\n'\n",
      "          '≤ 1 2 K1∑ k=1\\n'\n",
      "          '〈 ln ( |g′k(yk)| 2\\n'\n",
      "          '2πe )〉 y − K1 2 ln ( K1 N ) +H(Y ), (A.59)\\n'\n",
      "          'where the equality holds if and only if\\n'\n",
      "          'αk = 1\\n'\n",
      "          'K1 , k = 1, · · · ,K1, (A.60)\\n'\n",
      "          'which is consistent with Eq. (A.42).\\n'\n",
      "          'On the other hand, it follows from the Jensen’s inequality that\\n'\n",
      "          'ĬF =\\n'\n",
      "          '〈 ln p (y)−1 det( J̆(y) 2πe )1/2〉 y\\n'\n",
      "          '≤ ln ∫ det ( J̆(y)\\n'\n",
      "          '2πe\\n'\n",
      "          ')1/2 dy, (A.61)\\n'\n",
      "          'where the equality holds if and only if p (y)−1 det ( J̆(y) )1/2 is '\n",
      "          'a constant, which implies that\\n'\n",
      "          'p (y) = det ( J̆(y) )1/2 ∫\\n'\n",
      "          'det ( J̆(y) )1/2 dy = ∏K1 k=1 |g′k(yk)|∫ ∏K1 k=1 |g′k(yk)| dy . '\n",
      "          '(A.62)\\n'\n",
      "          'From (A.61) and (A.62), maximizing ĨF yields\\n'\n",
      "          'p (yk) = |g′k(yk)|∫ |g′k(yk)| dyk , k = 1, · · · ,K1. (A.63)\\n'\n",
      "          'We assume that (A.63) holds, at least approximately. Hence we can '\n",
      "          'let the peak of g′k(yk) be at yk = 〈yk〉yk = 0 and 〈 y2k 〉 yk = σ2yk '\n",
      "          '= ‖w̃k‖ 2. Then combining (A.57), (A.61) and (A.63) we find the '\n",
      "          'optimal parameters θ̃k for the nonlinear functions f̃(yk; θ̃k), k = '\n",
      "          '1, · · · ,K1.'},\n",
      " {'heading': 'A.3.3 THE FINAL OBJECTIVE FUNCTION',\n",
      "  'text': 'In the preceding sections we have obtained the initial optimal '\n",
      "          'solutions by maximizing I ( X; Y̆ ) and I(Y ;R). In this section, '\n",
      "          'we will discuss how to find the final optimal V0 and other '\n",
      "          'parameters by maximizing I(X;R) from the initial optimal '\n",
      "          'solutions.\\n'\n",
      "          'First, we have y = W̃T x̃ = aŷ, (A.64)\\n'\n",
      "          'where a is given in (A.43) and\\n'\n",
      "          'ŷ = (ŷ1, · · · , ŷK1)T = CT x̂ = ČT x̌, (A.65) x̂ = Σ −1/2 0 U '\n",
      "          'T 0 x, (A.66) C = VT0 ∈ RK0×K1 , (A.67) x̌ = U0Σ −1/2 0 U T 0 x = '\n",
      "          'U0x̂, (A.68)\\n'\n",
      "          'Č = U0C = [č1, · · · , čK1 ]. (A.69)\\n'\n",
      "          \"It follows that I(X;R) = I ( X̃;R ) ' ĨG = 1\\n\"\n",
      "          '2\\n'\n",
      "          '〈 ln ( det ( G(x̂)\\n'\n",
      "          '2πe ))〉 x̂ +H(X̃), (A.70)\\n'\n",
      "          'G(x̂) = NŴΦ̂Ŵ T + IK , (A.71)\\n'\n",
      "          'Ŵ = Σ1/2UTWA1/2 = a √ K−11 I K K0C = √ K−10 I K K0C, (A.72)\\n'\n",
      "          'where IKK0 is a K ×K0 diagonal matrix with value 1 on the diagonal '\n",
      "          'and\\n'\n",
      "          'Φ̂ = Φ2, (A.73) Φ = diag (φ(ŷ1), · · · , φ(ŷK1)) , (A.74)\\n'\n",
      "          'φ(ŷk) = a −1 ∣∣∣∣∂gk(ŷk)∂ŷk ∣∣∣∣ , (A.75) gk(ŷk) = 2 √ f̃(ŷk; '\n",
      "          'θ̃k), (A.76)\\n'\n",
      "          'ŷk = a −1yk = c T k x̂, k = 1, · · · ,K1. (A.77)\\n'\n",
      "          'Then we have det (G(x̂)) = det ( NK−10 CΦ̂C T + IK0 ) . (A.78)\\n'\n",
      "          'For large N and K0/N → 0, we have det (G(x̂)) ≈ det (J(x̂)) = det ( '\n",
      "          'NK−10 CΦ̂C T ) , (A.79)\\n'\n",
      "          'ĨG ≈ ĨF = −Q− K 2 ln (2πe)− K0 2 ln (ε) +H(X̃), (A.80) Q = −1 2 〈 '\n",
      "          'ln ( det ( CΦ̂C T ))〉 x̂ , (A.81)\\n'\n",
      "          'ε = K0 N . (A.82)\\n'\n",
      "          'Hence we can state the optimization problem as:\\n'\n",
      "          'minimize Q [C] =− 1 2\\n'\n",
      "          '〈 ln ( det ( CΦ̂C T ))〉\\n'\n",
      "          'x̂ , (A.83)\\n'\n",
      "          'subject to CCT = IK0 . (A.84) The gradient from (A.83) is given '\n",
      "          'by:\\n'\n",
      "          'dQ[C]\\n'\n",
      "          'dC = −\\n'\n",
      "          '〈( CΦ̂C T )−1 CΦ̂ + x̂ωT 〉\\n'\n",
      "          'x̂\\n'\n",
      "          ', (A.85)\\n'\n",
      "          'where C = [c1, · · · , cK1 ], ω = (ω1, · · · , ωK1) T , and\\n'\n",
      "          'ωk = φ(ŷk)φ ′(ŷk)c T k\\n'\n",
      "          '( CΦ̂C T )−1 ck, k = 1, · · · ,K1. (A.86)\\n'\n",
      "          'In the following we will discuss how to get the optimal solution of '\n",
      "          'C for two specific cases.'},\n",
      " {'heading': 'A.4 ALGORITHMS FOR OPTIMIZATION OBJECTIVE FUNCTION',\n",
      "  'text': 'A.4.1 ALGORITHM 1: K0 = K1\\n'\n",
      "          'Now CCT = CTC = IK1 , then by Eq. (A.83) we have\\n'\n",
      "          'Q1[C] = − 〈 K1∑ k=1 ln (φ(ŷk)) 〉 x̂ , (A.87)\\n'\n",
      "          'dQ1[C]\\n'\n",
      "          'dC = −\\n'\n",
      "          '〈 x̂ωT 〉 x̂ , (A.88)\\n'\n",
      "          'ωk = φ′(ŷk)\\n'\n",
      "          'φ(ŷk) , k = 1, · · · ,K1. (A.89)\\n'\n",
      "          'Under the orthogonality constraints (Eq. A.84), we can use the '\n",
      "          'following update rule for learning C (Edelman et al., 1998; Amari, '\n",
      "          '1999):\\n'\n",
      "          'Ct+1 = Ct + µt dCt\\n'\n",
      "          'dt , (A.90)\\n'\n",
      "          'dCt\\n'\n",
      "          'dt = −dQ1[C\\n'\n",
      "          't]\\n'\n",
      "          'dCt + Ct\\n'\n",
      "          '( dQ1[C t]\\n'\n",
      "          'dCt\\n'\n",
      "          ')T Ct, (A.91)\\n'\n",
      "          'where the learning rate parameter µt changes with the iteration '\n",
      "          'count t, t = 1, · · · , tmax. Here we can use the empirical average '\n",
      "          'to approximate the integral in (A.88) (see Eq. A.12). We can also '\n",
      "          'apply stochastic gradient descent (SGD) method for online updating '\n",
      "          'of Ct+1 in (A.90).\\n'\n",
      "          'The orthogonality constraint (Eq. A.84) can accelerate the '\n",
      "          'convergence rate. In practice, the orthogonal constraint (A.84) for '\n",
      "          'objective function (A.83) is not strictly necessary in this case. '\n",
      "          'We can completely discard this constraint condition and consider\\n'\n",
      "          'minimize Q2 [C] =− 〈 K1∑ k=1 ln (φ (ŷk)) 〉 x̂ − 1 2 ln ( det ( CTC '\n",
      "          ')) , (A.92)\\n'\n",
      "          'where we assume rank (C) = K1 = K0. If we let\\n'\n",
      "          'dC dt = −CCT dQ2 [C] dC , (A.93)\\n'\n",
      "          'then\\n'\n",
      "          'Tr\\n'\n",
      "          '( dQ2 [C]\\n'\n",
      "          'dC\\n'\n",
      "          'dCT\\n'\n",
      "          'dt\\n'\n",
      "          ') = −Tr ( CT dQ2 [C]\\n'\n",
      "          'dC\\n'\n",
      "          'dQ2 [C]\\n'\n",
      "          'dCT C\\n'\n",
      "          ') ≤ 0. (A.94)\\n'\n",
      "          'Therefore we can use an update rule similar to Eq. A.90 for '\n",
      "          'learning C. In fact, the method can also be extended to the case K0 '\n",
      "          '> K1 by using the same objective function (A.92).\\n'\n",
      "          'The learning rate parameter µt (see A.90) is updated adaptively, as '\n",
      "          'follows. First, calculate\\n'\n",
      "          'µt = vt κt , t = 1, · · · , tmax, (A.95)\\n'\n",
      "          'κt = 1\\n'\n",
      "          'K1 K1∑ k=1 ‖∇Ct(:, k)‖ ‖Ct(:, k)‖ , (A.96)\\n'\n",
      "          'and Ct+1 by (A.90) and (A.91), then calculate the value Q1 [ Ct+1 ] '\n",
      "          '. If Q1 [ Ct+1 ] < Q1[C\\n'\n",
      "          't], then let vt+1 ← vt, continue for the next iteration; otherwise, '\n",
      "          'let vt ← τvt, µt ← vt/κt and recalculate Ct+1 and Q1 [ Ct+1 ] . '\n",
      "          'Here 0 < v1 < 1 and 0 < τ < 1 are set as constants. After getting '\n",
      "          'Ct+1 for each update, we employ a Gram–Schmidt orthonormalization '\n",
      "          'process for matrix Ct+1, where the orthonormalization process can '\n",
      "          'accelerate the convergence. However, we can discard the Gram– '\n",
      "          'Schmidt orthonormalization process after iterative t0 (> 1) epochs '\n",
      "          'for more accurate optimization solution C. In this case, the '\n",
      "          'objective function is given by the Eq. (A.92). We can also further '\n",
      "          'optimize parameter b by gradient descent.\\n'\n",
      "          'When K0 = K1, the objective function Q2 [C] in Eq. (A.92) without '\n",
      "          'constraint is the same as the objective function of infomax ICA '\n",
      "          '(IICA) (Bell & Sejnowski, 1995; 1997), and as a consequence we '\n",
      "          'should get the same optimal solution C. Hence, in this sense, the '\n",
      "          'IICA may be regarded as a special case of our method. Our method '\n",
      "          'has a wider range of applications and can handle more generic '\n",
      "          'situations. Our model is derived by neural populations with a huge '\n",
      "          'number of neurons and it is not restricted to additive noise model. '\n",
      "          'Moreover, our method has a faster convergence rate during training '\n",
      "          'than IICA (see Section 3).\\n'\n",
      "          'A.4.2 ALGORITHM 2: K0 ≤ K1\\n'\n",
      "          'In this case, it is computationally expensive to update C by using '\n",
      "          'the gradient of Q (see Eq. A.85), since it needs to compute the '\n",
      "          'inverse matrix for every x̂. Here we provide an alternative method '\n",
      "          'for learning the optimal C. First, we consider the following '\n",
      "          'inequalities.\\n'\n",
      "          'Proposition 2. The following inequations hold, 1\\n'\n",
      "          '2\\n'\n",
      "          '〈 ln ( det ( CΦ̂CT ))〉 x̂ ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 x̂ CT ))\\n'\n",
      "          ', (A.97)〈 ln ( det ( CΦCT ))〉 x̂ ≤ ln ( det ( C 〈Φ〉x̂ C T )) '\n",
      "          '(A.98)\\n'\n",
      "          '≤ 1 2\\n'\n",
      "          'ln ( det ( C 〈Φ〉2x̂ C T ))\\n'\n",
      "          '(A.99)\\n'\n",
      "          '≤ 1 2\\n'\n",
      "          'ln ( det ( C 〈 Φ̂ 〉 x̂ CT )) , (A.100)\\n'\n",
      "          'ln ( det ( CΦCT )) ≤ 1 2 ln ( det ( CΦ̂CT )) , (A.101)\\n'\n",
      "          'where C ∈ RK0×K1 , K0 ≤ K1, and CCT = IK0 .\\n'\n",
      "          'Proof. Functions ln ( det ( C 〈 Φ̂ 〉 x̂ CT )) and ln ( det ( C '\n",
      "          '〈Φ〉x̂ CT )) are concave functions about p (x̂) (see the proof of '\n",
      "          'Proposition 5.2. in Huang & Zhang, 2017), which fact establishes '\n",
      "          'inequalities (A.97) and (A.98).\\n'\n",
      "          'Next we will prove the inequality (A.101). By SVD, we have\\n'\n",
      "          'CΦ = ÜD̈V̈ T , (A.102)\\n'\n",
      "          'where Ü is a K0 ×K0 unitary orthogonal matrix, V̈ = [v̈1, v̈2, · · '\n",
      "          '· , v̈K1 ] is an K1 ×K1 unitary orthogonal matrix, and D̈ is an '\n",
      "          'K0×K1 rectangular diagonal matrix with K0 positive real numbers on '\n",
      "          'the diagonal. By the matrix Hadamard’s inequality and '\n",
      "          'Cauchy–Schwarz inequality we have\\n'\n",
      "          'det ( CΦCTCΦCT ) det ( CΦ̂C T )−1\\n'\n",
      "          '= det ( D̈V̈ T CTCV̈D̈ T ( D̈D̈ T )−1)\\n'\n",
      "          '= det ( V̈T1 C TCV̈1 ) = det ( CV̈1\\n'\n",
      "          ')2 ≤\\n'\n",
      "          'K0∏ k=1 ( CV̈1 )2 k,k\\n'\n",
      "          '≤ K0∏ k=1 ( CCT )2 k,k ( V̈T1 V̈1 )2 k,k\\n'\n",
      "          '= 1, (A.103)\\n'\n",
      "          'where V̈1 = [v̈1, v̈2, · · · , v̈K0 ] ∈ RK1×K0 . The last equality '\n",
      "          'holds because of CC T = IK0 and V̈T1 V̈1 = IK0 . This establishes '\n",
      "          'inequality (A.101) and the equality holds if and only if K0 = K1 or '\n",
      "          'CV̈1 = IK0 .\\n'\n",
      "          'Similarly, we get inequality (A.99):\\n'\n",
      "          'ln ( det ( C 〈Φ〉x̂ C T )) ≤ 1 2 ln ( det ( C 〈Φ〉2x̂ C T )) . '\n",
      "          '(A.104)\\n'\n",
      "          'By Jensen’s inequality, we have 〈φ (ŷk)〉2x̂ ≤ 〈 φ (ŷk) 2 〉\\n'\n",
      "          'x̂ , ∀k = 1, · · · ,K1. (A.105)\\n'\n",
      "          'Then it follows from (A.105) that inequality (A.100) holds:\\n'\n",
      "          '1 2 ln ( det ( C 〈Φ〉2x̂ C T )) ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 x̂ CT )) . '\n",
      "          '(A.106)\\n'\n",
      "          'This completes the proof of Proposition 2.\\n'\n",
      "          'By Proposition 2, if K0 = K1 then we get 1\\n'\n",
      "          '2\\n'\n",
      "          '〈 ln ( det ( Φ̂ ))〉\\n'\n",
      "          'x̂ ≤ 1 2 ln ( det (〈 Φ̂ 〉 x̂ )) , (A.107)\\n'\n",
      "          '〈ln (det (Φ))〉x̂ ≤ ln (det (〈Φ〉x̂)) (A.108)\\n'\n",
      "          '= 1 2 ln ( det ( 〈Φ〉2x̂ )) (A.109) ≤ 1 2 ln ( det (〈 Φ̂ 〉 x̂ )) , '\n",
      "          '(A.110)\\n'\n",
      "          'ln (det (Φ)) = 1 2 ln ( det ( Φ̂ )) . (A.111)\\n'\n",
      "          'On the other hand, it follows from (A.81) and Proposition 2 that〈 '\n",
      "          'ln ( det ( CΦCT ))〉 x̂ ≤ −Q ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 x̂ CT ))\\n'\n",
      "          ', (A.112)〈 ln ( det ( CΦCT ))〉 x̂ ≤ −Q̂ ≤ 1 2 ln ( det ( C 〈 Φ̂ 〉 '\n",
      "          'x̂ CT )) . (A.113)\\n'\n",
      "          'Hence we can see that Q̂ is close to Q (see A.81). Moreover, it '\n",
      "          'follows from the Cauchy–Schwarz inequality that 〈\\n'\n",
      "          '(Φ)k,k 〉 x̂ = 〈φ (ŷk)〉ŷk ≤ (∫ φ (ŷk) 2 dŷk ∫ p (ŷk) 2 dŷk '\n",
      "          ')1/2 , (A.114)\\n'\n",
      "          'where k = 1, · · · ,K1, the equality holds if and only if the '\n",
      "          'following holds:\\n'\n",
      "          'p (ŷk) = φ (ŷk)∫ φ (ŷk) dŷk , k = 1, · · · ,K1, (A.115)\\n'\n",
      "          'which is the similar to Eq. (A.63).\\n'\n",
      "          'Since I(X;R) = I(Y ;R) (see Proposition 1), by maximizing I(X;R) we '\n",
      "          'hope the equality in inequality (A.61) and equality (A.63) hold, at '\n",
      "          'least approximatively. On the other hand, let\\n'\n",
      "          'Copt = arg min C Q[C] = arg max C\\n'\n",
      "          '(〈 ln ( det(CΦ̂CT ) )〉\\n'\n",
      "          'x̂\\n'\n",
      "          ') , (A.116)\\n'\n",
      "          'Ĉopt = arg min C Q̂[C] = arg max C\\n'\n",
      "          '( ln ( det ( C 〈Φ〉2x̂ C T ))) , (A.117)\\n'\n",
      "          'Copt and Ĉopt make (A.63) and (A.115) to hold true, which implies '\n",
      "          'that they are the same optimal solution: Copt = Ĉopt.\\n'\n",
      "          'Therefore, we can use the following objective function Q̂[C] as a '\n",
      "          'substitute for Q[C] and write the optimization problem as:\\n'\n",
      "          'minimize Q̂[C] =− 1 2\\n'\n",
      "          'ln ( det ( C 〈Φ〉2x̂ C T )) , (A.118)\\n'\n",
      "          'subject to CCT = IK0 . (A.119) The update rule (A.90) may also '\n",
      "          'apply here and a modified algorithm similar to Algorithm 1 may be '\n",
      "          'used for parameter learning.'},\n",
      " {'heading': 'A.5 SUPPLEMENTARY EXPERIMENTS', 'text': ''},\n",
      " {'heading': 'A.5.1 QUANTITATIVE METHODS FOR COMPARISON',\n",
      "  'text': 'To quantify the efficiency of learning representations by the above '\n",
      "          'algorithms, we calculate the coefficient entropy (CFE) for '\n",
      "          'estimating coding cost as follows (Lewicki & Olshausen, 1999; '\n",
      "          'Lewicki & Sejnowski, 2000):\\n'\n",
      "          'y̌k = ζw̌ T k x̌, k = 1, · · · ,K1, (A.120)\\n'\n",
      "          'ζ = K1∑K1\\n'\n",
      "          'k=1 ‖w̌k‖ , (A.121)\\n'\n",
      "          'where x̌ is defined by Eq. (A.68), and w̌k is the corresponding '\n",
      "          'optimal filter. To estimate the probability density of coefficients '\n",
      "          'qk(y̌k) (k = 1, · · · ,K1) from the M training samples, we apply '\n",
      "          'the kernel density estimation for qk(y̌k) and use a normal kernel '\n",
      "          'with an adaptive optimal window width. Then we define the CFE h as\\n'\n",
      "          'h = 1\\n'\n",
      "          'K1 K1∑ k=1 Hk(Y̌k), (A.122)\\n'\n",
      "          'Hk(Y̌k) = −∆ ∑ nqk(n∆) log2 qk(n∆), (A.123)\\n'\n",
      "          'where qk(y̌k) is quantized as discrete qk(n∆) and ∆ is the step '\n",
      "          'size.\\n'\n",
      "          'Methods such as IICA and SRBM as well as our methods have '\n",
      "          'feedforward structures in which information is transferred directly '\n",
      "          'through a nonlinear function, e.g., the sigmoid function. We can '\n",
      "          'use the amount of transmitted information to measure the results '\n",
      "          'learned by these methods. Consider a neural population with N '\n",
      "          'neurons, which is a stochastic system with nonlinear transfer '\n",
      "          'functions. We chose a sigmoidal transfer function and Gaussian '\n",
      "          'noise with standard deviation set to 1 as the system noise. In this '\n",
      "          'case, from (1), (A.8) and (A.11), we see that the approximate MI IG '\n",
      "          'is equivalent to the case of the Poisson neuron model. It follows '\n",
      "          'from (A.70)–(A.82) that\\n'\n",
      "          \"I(X;R) = I ( X̃;R ) = H(X̃)−H ( X̃|R ) ' ĨG = H(X̃)− h1, (A.124)\\n\"\n",
      "          \"H ( X̃|R ) ' h1 = − 1\\n\"\n",
      "          '2\\n'\n",
      "          '〈 ln ( det ( 1\\n'\n",
      "          '2πe\\n'\n",
      "          '( NK−10 CΦ̂C T + IK0 )))〉 x̂ , (A.125)\\n'\n",
      "          'where we set N = 106. A good representation should make the MI '\n",
      "          'I(X;R) as big as possible. Equivalently, for the same inputs, a '\n",
      "          'good representation should make the conditional entropy (CDE) H ( '\n",
      "          'X̃|R ) (or h1) as small as possible.'},\n",
      " {'heading': 'A.5.2 COMPARISON OF BASIS VECTORS',\n",
      "  'text': 'We compared our algorithm with an up-to-date sparse coding '\n",
      "          'algorithm, the mini-batch dictionary learning (MBDL) as given in '\n",
      "          '(Mairal et al., 2009; 2010) and integrated in Python library, i.e. '\n",
      "          'scikitlearn. The input data was the same as the above, i.e. 105 '\n",
      "          'nature image patches preprocessed by the ZCA whitening filters.\\n'\n",
      "          'We denotes the optimal dictionary learned by MBDL as B̌ ∈ RK×K1 for '\n",
      "          'which each column represents a basis vector. Now we have\\n'\n",
      "          'x ≈ UΣ1/2UT B̌y = B̃y, (A.126) B̃ = UΣ1/2UT B̌, (A.127)\\n'\n",
      "          'where y = (y1, · · · , yK1) T is the coefficient vector.\\n'\n",
      "          'Similarly, we can obtain a dictionary from the filter matrix C. '\n",
      "          'Suppose rank (C) = K0 ≤ K1, then it follows from (A.64) that\\n'\n",
      "          'x̂ = ( aCCT )−1 Cy. (A.128)\\n'\n",
      "          'By (A.66) and (A.128), we get\\n'\n",
      "          'x ≈ By = aBCTΣ−1/20 UT0 x, (A.129)\\n'\n",
      "          'B = a−1U0Σ 1/2 0\\n'\n",
      "          '( CCT )−1 C = [b1, · · · ,bK1 ] , (A.130)\\n'\n",
      "          'where y = WTx = aCTΣ−1/20 U T 0 x, the vectors b1, · · · ,bK1 can '\n",
      "          'be regarded as the basis vectors and the strict equality holds when '\n",
      "          'K0 = K1 = K. Recall that X = [x1, · · · , xM ] = USṼ T (see Eq. '\n",
      "          'A.49) and Y = [y1, · · · , yM ] = WTX = a √ M − 1CT ṼT0 , then we '\n",
      "          'get X̆ = BY =√\\n'\n",
      "          'M − 1U0Σ1/20 ṼT0 ≈ X. Hence, Eq. (A.129) holds. The basis vectors '\n",
      "          'shown in Figure 4(a)–4(e) correspond to filters in Figure '\n",
      "          '2(a)–2(e). And Figure 4(f) illustrates the optimal dictionary B̃ '\n",
      "          'learned by MBDL, where we set the regularization parameter as λ = '\n",
      "          '1.2/ √ K, the batch size as 50 and the total number of iterations '\n",
      "          'to perform as 20000, which took about 3 hours for training. From '\n",
      "          'Figure 4 we see that these basis vectors obtained by the above '\n",
      "          'algorithms have local Gabor-like shapes except for those by SRBM. '\n",
      "          'If rank(B̌) = K = K1, then the matrix B̌−T can be regarded as a '\n",
      "          'filter matrix like matrix Č (see Eq. A.69). However, from the '\n",
      "          'column vector of matrix B̌−T we cannot find any local Gabor-like '\n",
      "          'filter that resembles the filters shown in Figure 2. Our algorithm '\n",
      "          'has less computational cost and a much faster convergence rate than '\n",
      "          'the sparse coding algorithm. Moreover, the sparse coding method '\n",
      "          'involves a dynamic generative model that requires relaxation and is '\n",
      "          'therefore unsuitable for fast inference, whereas the feedforward '\n",
      "          'framework of our model is easy for inference because it only '\n",
      "          'requires evaluating the nonlinear tuning functions.'},\n",
      " {'heading': 'A.5.3 LEARNING OVERCOMPLETE BASES',\n",
      "  'text': 'We have trained our model on the Olshausen’s nature image patches '\n",
      "          'with a highly overcomplete setup by optimizing the objective '\n",
      "          '(A.118) by Alg.2 and got Gabor-like filters. The results of 400 '\n",
      "          'typical filters chosen from 1024 output filters are displayed in '\n",
      "          'Figure 5(a) and corresponding base (see Eq. A.130) are shown in '\n",
      "          'Figure 5(b). Here the parameters are K1 = 1024, tmax = 100, v1 = '\n",
      "          '0.4, τ = 0.8, and = 0.98 (see A.52), from which we got rank (B) = '\n",
      "          'K0 = 82. Compared to the ICA-like results in Figure 2(a)–2(c), the '\n",
      "          'average size of Gabor-like filters in Figure 5(a) is bigger, '\n",
      "          'indicating that the small noise-like local structures in the images '\n",
      "          'have been filtered out.\\n'\n",
      "          'We have also trained our model on 60,000 images of handwritten '\n",
      "          'digits from MNIST dataset (LeCun et al., 1998) and the resultant '\n",
      "          '400 typical optimal filters and bases are shown in Figure 5(c) and '\n",
      "          'Figure 5(d), respectively. All parameters were the same as Figure '\n",
      "          '5(a) and Figure 5(b): K1 = 1024, tmax = 100, v1 = 0.4, τ = 0.8 and '\n",
      "          '= 0.98, from which we got rank (B) = K0 = 183. From these figures '\n",
      "          'we can see that the salient features of the input images are '\n",
      "          'reflected in these filters and bases. We could also get the similar '\n",
      "          'overcomplete filters and bases by SRBM and MBDL. However, the '\n",
      "          'results depended sensitively on the choice of parameters and the '\n",
      "          'training took a long time.\\n'\n",
      "          'Figure 6 shows that CFE as a function of training time for Alg.2, '\n",
      "          'where Figure 6(a) corresponds to Figure 5(a)-5(b) for learning '\n",
      "          'nature image patches and Figure 6(b) corresponds to Figure '\n",
      "          '5(c)-5(d) for learning MNIST dataset. We set parameters tmax = 100 '\n",
      "          'and τ = 0.8 for all experiments and varied parameter v1 for each '\n",
      "          'experiment, with v1 = 0.2, 0.4, 0.6 or 0.8. These results indicate '\n",
      "          'a fast convergence rate for training on different datasets. '\n",
      "          'Generally, the convergence is insensitive to the change of '\n",
      "          'parameter v1.\\n'\n",
      "          'We have also performed additional tests on other image datasets and '\n",
      "          'got similar results, confirming the speed and robustness of our '\n",
      "          'learning method. Compared with other methods, e.g., IICA, FICA, '\n",
      "          'MBDL, SRBM or sparse autoencoders etc., our method appeared to be '\n",
      "          'more efficient and robust for unsupervised learning of '\n",
      "          'representations. We also found that complete and overovercomplete '\n",
      "          'filters and bases learned by our methods had local Gabor-like '\n",
      "          'shapes while the results by SRBM or MBDL did not have this '\n",
      "          'property.\\n'\n",
      "          'A.5.4 IMAGE DENOISING\\n'\n",
      "          'Similar to the sparse coding method applied to image denoising '\n",
      "          '(Elad & Aharon, 2006), our method (see Eq. A.130) can also be '\n",
      "          'applied to image denoising, as shown by an example in Figure 7. The '\n",
      "          'filters or bases were learned by using 7×7 image patches sampled '\n",
      "          'from the left half of the image, and subsequently used to '\n",
      "          'reconstruct the right half of the image which was distorted by '\n",
      "          'Gaussian noise. A common practice for evaluating the results of '\n",
      "          'image denoising is by looking at the difference between the '\n",
      "          'reconstruction and the original image. If the reconstruction is '\n",
      "          'perfect the difference should look like Gaussian noise. In Figure '\n",
      "          '7(c) and 7(d) a dictionary (100 bases) was learned by MBDL and '\n",
      "          'orthogonal matching pursuit was used to estimate the sparse '\n",
      "          'solution. 1 For our method (shown in Figure 7(b)), we first get the '\n",
      "          'optimal filters parameter W, a low rank matrix (K0 < K), then from '\n",
      "          'the distorted image patches xm (m = 1, · · · ,M ) we get filter '\n",
      "          'outputs ym = WTxm and the reconstruction x̆m = Bym (parameters: = '\n",
      "          '0.975 and K0 = K1 = 14). As can be seen from Figure 7, our method '\n",
      "          'worked better than dictionary learning, although we only used 14 '\n",
      "          'bases compared with 100 bases used by dictionary learning. Our '\n",
      "          'method is also more efficient. We can get better optimal bases B by '\n",
      "          'a generative model using our infomax approach (details not shown).\\n'\n",
      "          '1Python source code is available at http://scikit-learn.org/stable/ '\n",
      "          'downloads/plot image denoising.py'}]\n"
     ]
    }
   ],
   "source": [
    "for p in paper_data[2017]:\n",
    "    if p[\"name\"] == \"SkYbF1slg.pdf\":\n",
    "        pprint(p[\"metadata\"][\"sections\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save 2017 new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./2017_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT FOUNDS:  0 1 3\n",
      "NOT FOUNDS:  0 6 9\n",
      "NOT FOUNDS:  0 9 17\n",
      "NOT FOUNDS:  0 16 35\n"
     ]
    }
   ],
   "source": [
    "features_dict = construct_features(paper_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./17_20_new_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2018_rJGY8GbR-', {'avg_ref_mention': 395, 'num_sections': 4}),\n",
       " ('2019_SyehMhC9Y7',\n",
       "  {'avg_ref_mention': 50,\n",
       "   'contains_appendix': 0,\n",
       "   'eqn_ref_count': 2,\n",
       "   'fig_ref_count': 10,\n",
       "   'num_sections': 11,\n",
       "   'table_ref_count': 5}),\n",
       " ('2017_SygvTcYee',\n",
       "  {'avg_ref_mention': 71,\n",
       "   'contains_appendix': 0,\n",
       "   'eqn_ref_count': 0,\n",
       "   'fig_ref_count': 7,\n",
       "   'num_sections': 8,\n",
       "   'table_ref_count': 0}),\n",
       " ('2019_SkguE30ct7',\n",
       "  {'avg_ref_mention': 38,\n",
       "   'contains_appendix': 1,\n",
       "   'eqn_ref_count': 0,\n",
       "   'fig_ref_count': 10,\n",
       "   'num_sections': 20,\n",
       "   'table_ref_count': 6}),\n",
       " ('2020_H1ggKyrYwB',\n",
       "  {'avg_ref_mention': 51,\n",
       "   'contains_appendix': 1,\n",
       "   'eqn_ref_count': 0,\n",
       "   'fig_ref_count': 14,\n",
       "   'num_sections': 12,\n",
       "   'table_ref_count': 9})]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features_dict.items())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_from_text = None\n",
    "with open('./feature_dict/2017_features.pkl', \"rb\") as f:\n",
    "    new_features_from_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_ref_mention': 27,\n",
       " 'contains_appendix': 1,\n",
       " 'eqn_ref_count': 6,\n",
       " 'fig_ref_count': 9,\n",
       " 'num_sections': 26,\n",
       " 'table_ref_count': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features_from_text[\"rJeKjwvclx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_features_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_ref_mention': 56, 'num_sections': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features_from_text[\"r1Usiwcex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
