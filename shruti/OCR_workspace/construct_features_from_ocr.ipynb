{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the year wise paper json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_papers_json(year):\n",
    "    paper_data = []\n",
    "    \n",
    "    file_path = \"./science-parse/output/{}/{}_ICLR\".format(year, year)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            paper_data.append(json.loads(line))\n",
    "            \n",
    "    return paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_data = read_papers_json(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'abstractText': 'Recent work has begun exploring neural acoustic '\n",
      "                              'word embeddings—fixeddimensional vector '\n",
      "                              'representations of arbitrary-length speech '\n",
      "                              'segments corresponding to words. Such '\n",
      "                              'embeddings are applicable to speech retrieval '\n",
      "                              'and recognition tasks, where reasoning about '\n",
      "                              'whole words may make it possible to avoid '\n",
      "                              'ambiguous sub-word representations. The main '\n",
      "                              'idea is to map acoustic sequences to '\n",
      "                              'fixed-dimensional vectors such that examples of '\n",
      "                              'the same word are mapped to similar vectors, '\n",
      "                              'while different-word examples are mapped to '\n",
      "                              'very different vectors. In this work we take a '\n",
      "                              'multi-view approach to learning acoustic word '\n",
      "                              'embeddings, in which we jointly learn to embed '\n",
      "                              'acoustic sequences and their corresponding '\n",
      "                              'character sequences. We use deep bidirectional '\n",
      "                              'LSTM embedding models and multi-view '\n",
      "                              'contrastive losses. We study the effect of '\n",
      "                              'different loss variants, including fixed-margin '\n",
      "                              'and cost-sensitive losses. Our acoustic word '\n",
      "                              'embeddings improve over previous approaches for '\n",
      "                              'the task of word discrimination. We also '\n",
      "                              'present results on other tasks that are enabled '\n",
      "                              'by the multi-view approach, including '\n",
      "                              'cross-view word discrimination and word '\n",
      "                              'similarity.',\n",
      "              'authors': ['Wanjia He', 'Weiran Wang'],\n",
      "              'creator': 'LaTeX with hyperref package',\n",
      "              'emails': ['wanjia@ttic.edu',\n",
      "                         'weiranwang@ttic.edu',\n",
      "                         'klivescu@ttic.edu'],\n",
      "              'referenceMentions': [{'context': 'Word embeddings can be '\n",
      "                                                'learned using spectral '\n",
      "                                                'methods (Deerwester et al., '\n",
      "                                                '1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al.',\n",
      "                                     'endOffset': 79,\n",
      "                                     'referenceID': 9,\n",
      "                                     'startOffset': 54},\n",
      "                                    {'context': ', 1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al., '\n",
      "                                                '2003; Mnih & Hinton, 2007; '\n",
      "                                                'Mikolov et al., 2013; '\n",
      "                                                'Pennington et al., 2014).',\n",
      "                                     'endOffset': 151,\n",
      "                                     'referenceID': 3,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': ', 1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al., '\n",
      "                                                '2003; Mnih & Hinton, 2007; '\n",
      "                                                'Mikolov et al., 2013; '\n",
      "                                                'Pennington et al., 2014).',\n",
      "                                     'endOffset': 151,\n",
      "                                     'referenceID': 32,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': ', 1990) or, more commonly in '\n",
      "                                                'recent work, via neural '\n",
      "                                                'networks (Bengio et al., '\n",
      "                                                '2003; Mnih & Hinton, 2007; '\n",
      "                                                'Mikolov et al., 2013; '\n",
      "                                                'Pennington et al., 2014).',\n",
      "                                     'endOffset': 151,\n",
      "                                     'referenceID': 35,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 37,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 28,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 44,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Word embeddings can also be '\n",
      "                                                'composed to form embeddings '\n",
      "                                                'of phrases, sentences, or '\n",
      "                                                'documents (Socher et al., '\n",
      "                                                '2014; Kiros et al., 2015; '\n",
      "                                                'Wieting et al., 2016; Iyyer '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 175,\n",
      "                                     'referenceID': 24,\n",
      "                                     'startOffset': 92},\n",
      "                                    {'context': 'Such embeddings could be '\n",
      "                                                'useful for tasks like spoken '\n",
      "                                                'term detection (Fiscus et '\n",
      "                                                'al., 2007), spoken '\n",
      "                                                'query-by-example search '\n",
      "                                                '(Anguera et al.',\n",
      "                                     'endOffset': 90,\n",
      "                                     'referenceID': 11,\n",
      "                                     'startOffset': 69},\n",
      "                                    {'context': ', 2007), spoken '\n",
      "                                                'query-by-example search '\n",
      "                                                '(Anguera et al., 2014), or '\n",
      "                                                'even speech recognition using '\n",
      "                                                'a whole-word approach '\n",
      "                                                '(Gemmeke et al.',\n",
      "                                     'endOffset': 62,\n",
      "                                     'referenceID': 0,\n",
      "                                     'startOffset': 40},\n",
      "                                    {'context': ', 2014), or even speech '\n",
      "                                                'recognition using a '\n",
      "                                                'whole-word approach (Gemmeke '\n",
      "                                                'et al., 2011; Bengio & '\n",
      "                                                'Heigold, 2014).',\n",
      "                                     'endOffset': 110,\n",
      "                                     'referenceID': 12,\n",
      "                                     'startOffset': 64},\n",
      "                                    {'context': 'In tasks that involve '\n",
      "                                                'comparing speech segments to '\n",
      "                                                'each other, vector embeddings '\n",
      "                                                'can allow more efficient and '\n",
      "                                                'more accurate distance '\n",
      "                                                'computation than '\n",
      "                                                'sequence-based approaches '\n",
      "                                                'such as dynamic time warping '\n",
      "                                                '(Levin et al., 2013, 2015; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 296,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 205},\n",
      "                                    {'context': 'In tasks that involve '\n",
      "                                                'comparing speech segments to '\n",
      "                                                'each other, vector embeddings '\n",
      "                                                'can allow more efficient and '\n",
      "                                                'more accurate distance '\n",
      "                                                'computation than '\n",
      "                                                'sequence-based approaches '\n",
      "                                                'such as dynamic time warping '\n",
      "                                                '(Levin et al., 2013, 2015; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 296,\n",
      "                                     'referenceID': 8,\n",
      "                                     'startOffset': 205},\n",
      "                                    {'context': 'We refer to the objective in '\n",
      "                                                '(1) as the “classifier '\n",
      "                                                'network” objective, which has '\n",
      "                                                'been used in several prior '\n",
      "                                                'studies on acoustic word '\n",
      "                                                'embeddings (Bengio & Heigold, '\n",
      "                                                '2014; Kamper et al., 2016; '\n",
      "                                                'Settle & Livescu, 2016).',\n",
      "                                     'endOffset': 214,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 145},\n",
      "                                    {'context': 'An alternative approach, '\n",
      "                                                'based on Siamese networks '\n",
      "                                                '(Bromley et al., 1993), uses '\n",
      "                                                'supervision of the form '\n",
      "                                                '“segment x(1) is similar to '\n",
      "                                                'segment x(2), and is not '\n",
      "                                                'similar to segment x(3)”, '\n",
      "                                                'where two segments are '\n",
      "                                                'considered similar if they '\n",
      "                                                'have the same word label and '\n",
      "                                                'dissimilar otherwise.',\n",
      "                                     'endOffset': 73,\n",
      "                                     'referenceID': 4,\n",
      "                                     'startOffset': 51},\n",
      "                                    {'context': 'Models based on Siamese '\n",
      "                                                'networks have been used for a '\n",
      "                                                'variety of representation '\n",
      "                                                'learning problems in NLP (Hu '\n",
      "                                                'et al., 2014; Wieting et al., '\n",
      "                                                '2016), vision (Hadsell et al.',\n",
      "                                     'endOffset': 144,\n",
      "                                     'referenceID': 23,\n",
      "                                     'startOffset': 105},\n",
      "                                    {'context': 'Models based on Siamese '\n",
      "                                                'networks have been used for a '\n",
      "                                                'variety of representation '\n",
      "                                                'learning problems in NLP (Hu '\n",
      "                                                'et al., 2014; Wieting et al., '\n",
      "                                                '2016), vision (Hadsell et al.',\n",
      "                                     'endOffset': 144,\n",
      "                                     'referenceID': 44,\n",
      "                                     'startOffset': 105},\n",
      "                                    {'context': ', 2016), vision (Hadsell et '\n",
      "                                                'al., 2006), and speech '\n",
      "                                                '(Synnaeve et al.',\n",
      "                                     'endOffset': 38,\n",
      "                                     'referenceID': 16,\n",
      "                                     'startOffset': 16},\n",
      "                                    {'context': ', 2006), and speech (Synnaeve '\n",
      "                                                'et al., 2014; Kamper et al., '\n",
      "                                                '2015) including acoustic word '\n",
      "                                                'embeddings (Kamper et al.',\n",
      "                                     'endOffset': 64,\n",
      "                                     'referenceID': 40,\n",
      "                                     'startOffset': 20},\n",
      "                                    {'context': ', 2006), and speech (Synnaeve '\n",
      "                                                'et al., 2014; Kamper et al., '\n",
      "                                                '2015) including acoustic word '\n",
      "                                                'embeddings (Kamper et al.',\n",
      "                                     'endOffset': 64,\n",
      "                                     'referenceID': 25,\n",
      "                                     'startOffset': 20},\n",
      "                                    {'context': ', 2015) including acoustic '\n",
      "                                                'word embeddings (Kamper et '\n",
      "                                                'al., 2016; Settle & Livescu, '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 88,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 43},\n",
      "                                    {'context': 'The term “Siamese” (Bromley '\n",
      "                                                'et al., 1993; Chopra et al., '\n",
      "                                                '2005) refers to the fact that '\n",
      "                                                'the triplet (x(1), x(2), '\n",
      "                                                'x(3)) share the same '\n",
      "                                                'embedding network f .',\n",
      "                                     'endOffset': 62,\n",
      "                                     'referenceID': 4,\n",
      "                                     'startOffset': 19},\n",
      "                                    {'context': 'The term “Siamese” (Bromley '\n",
      "                                                'et al., 1993; Chopra et al., '\n",
      "                                                '2005) refers to the fact that '\n",
      "                                                'the triplet (x(1), x(2), '\n",
      "                                                'x(3)) share the same '\n",
      "                                                'embedding network f .',\n",
      "                                     'endOffset': 62,\n",
      "                                     'referenceID': 7,\n",
      "                                     'startOffset': 19},\n",
      "                                    {'context': 'While many deep multi-view '\n",
      "                                                'learning objectives are '\n",
      "                                                'applicable (Ngiam et al., '\n",
      "                                                '2011; Srivastava & '\n",
      "                                                'Salakhutdinov, 2014; Sohn et '\n",
      "                                                'al., 2014; Wang et al., '\n",
      "                                                '2015), we consider the '\n",
      "                                                'multi-view contrastive loss '\n",
      "                                                'objective of (Hermann & '\n",
      "                                                'Blunsom, 2014), which is '\n",
      "                                                'simple to optimize and '\n",
      "                                                'implement and performs well '\n",
      "                                                'in practice.',\n",
      "                                     'endOffset': 154,\n",
      "                                     'referenceID': 34,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'While many deep multi-view '\n",
      "                                                'learning objectives are '\n",
      "                                                'applicable (Ngiam et al., '\n",
      "                                                '2011; Srivastava & '\n",
      "                                                'Salakhutdinov, 2014; Sohn et '\n",
      "                                                'al., 2014; Wang et al., '\n",
      "                                                '2015), we consider the '\n",
      "                                                'multi-view contrastive loss '\n",
      "                                                'objective of (Hermann & '\n",
      "                                                'Blunsom, 2014), which is '\n",
      "                                                'simple to optimize and '\n",
      "                                                'implement and performs well '\n",
      "                                                'in practice.',\n",
      "                                     'endOffset': 154,\n",
      "                                     'referenceID': 38,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'While many deep multi-view '\n",
      "                                                'learning objectives are '\n",
      "                                                'applicable (Ngiam et al., '\n",
      "                                                '2011; Srivastava & '\n",
      "                                                'Salakhutdinov, 2014; Sohn et '\n",
      "                                                'al., 2014; Wang et al., '\n",
      "                                                '2015), we consider the '\n",
      "                                                'multi-view contrastive loss '\n",
      "                                                'objective of (Hermann & '\n",
      "                                                'Blunsom, 2014), which is '\n",
      "                                                'simple to optimize and '\n",
      "                                                'implement and performs well '\n",
      "                                                'in practice.',\n",
      "                                     'endOffset': 154,\n",
      "                                     'referenceID': 43,\n",
      "                                     'startOffset': 62},\n",
      "                                    {'context': 'This approach produced '\n",
      "                                                'improved performance on a '\n",
      "                                                'word discrimination task '\n",
      "                                                'compared to using raw DTW '\n",
      "                                                'distances, and was later also '\n",
      "                                                'applied successfully for a '\n",
      "                                                'query-by-example task (Levin '\n",
      "                                                'et al., 2015).',\n",
      "                                     'endOffset': 199,\n",
      "                                     'referenceID': 30,\n",
      "                                     'startOffset': 179},\n",
      "                                    {'context': 'Most evaluations have been '\n",
      "                                                'based on word discrimination '\n",
      "                                                '– the task of determining '\n",
      "                                                'whether two speech segments '\n",
      "                                                'correspond to the same word '\n",
      "                                                'or not – which can be seen as '\n",
      "                                                'a proxy for query-by-example '\n",
      "                                                'search (Levin et al., 2013; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 289,\n",
      "                                     'referenceID': 29,\n",
      "                                     'startOffset': 204},\n",
      "                                    {'context': 'Most evaluations have been '\n",
      "                                                'based on word discrimination '\n",
      "                                                '– the task of determining '\n",
      "                                                'whether two speech segments '\n",
      "                                                'correspond to the same word '\n",
      "                                                'or not – which can be seen as '\n",
      "                                                'a proxy for query-by-example '\n",
      "                                                'search (Levin et al., 2013; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 289,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 204},\n",
      "                                    {'context': 'Most evaluations have been '\n",
      "                                                'based on word discrimination '\n",
      "                                                '– the task of determining '\n",
      "                                                'whether two speech segments '\n",
      "                                                'correspond to the same word '\n",
      "                                                'or not – which can be seen as '\n",
      "                                                'a proxy for query-by-example '\n",
      "                                                'search (Levin et al., 2013; '\n",
      "                                                'Kamper et al., 2016; Settle & '\n",
      "                                                'Livescu, 2016; Chung et al., '\n",
      "                                                '2016).',\n",
      "                                     'endOffset': 289,\n",
      "                                     'referenceID': 8,\n",
      "                                     'startOffset': 204},\n",
      "                                    {'context': 'For example, the standard '\n",
      "                                                'task of spoken term detection '\n",
      "                                                '(Fiscus et al., 2007) '\n",
      "                                                'involves searching for '\n",
      "                                                'examples of a given text '\n",
      "                                                'query in spoken documents.',\n",
      "                                     'endOffset': 77,\n",
      "                                     'referenceID': 11,\n",
      "                                     'startOffset': 56},\n",
      "                                    {'context': 'This is analogous to the '\n",
      "                                                'evaluation of semantic word '\n",
      "                                                'embeddings via the rank '\n",
      "                                                'correlation between embedding '\n",
      "                                                'distances and human '\n",
      "                                                'similarity judgments '\n",
      "                                                '(Finkelstein et al., 2001; '\n",
      "                                                'Hill et al., 2015).',\n",
      "                                     'endOffset': 193,\n",
      "                                     'referenceID': 10,\n",
      "                                     'startOffset': 148},\n",
      "                                    {'context': 'This is analogous to the '\n",
      "                                                'evaluation of semantic word '\n",
      "                                                'embeddings via the rank '\n",
      "                                                'correlation between embedding '\n",
      "                                                'distances and human '\n",
      "                                                'similarity judgments '\n",
      "                                                '(Finkelstein et al., 2001; '\n",
      "                                                'Hill et al., 2015).',\n",
      "                                     'endOffset': 193,\n",
      "                                     'referenceID': 21,\n",
      "                                     'startOffset': 148},\n",
      "                                    {'context': 'The task and setup were first '\n",
      "                                                'developed by (Carlin et al., '\n",
      "                                                '2011).',\n",
      "                                     'endOffset': 64,\n",
      "                                     'referenceID': 5,\n",
      "                                     'startOffset': 43},\n",
      "                                    {'context': 'The data is drawn from the '\n",
      "                                                'Switchboard English '\n",
      "                                                'conversational speech corpus '\n",
      "                                                '(Godfrey et al., 1992).',\n",
      "                                     'endOffset': 98,\n",
      "                                     'referenceID': 14,\n",
      "                                     'startOffset': 76},\n",
      "                                    {'context': 'Method Test AP Test AP '\n",
      "                                                '(acoustic) (cross-view) MFCCs '\n",
      "                                                '+ DTW (Kamper et al., 2016) '\n",
      "                                                '0.',\n",
      "                                     'endOffset': 80,\n",
      "                                     'referenceID': 26,\n",
      "                                     'startOffset': 59},\n",
      "                                    {'context': '214 Correspondence '\n",
      "                                                'autoencoder + DTW (Kamper et '\n",
      "                                                'al., 2015) 0.',\n",
      "                                     'endOffset': 58,\n",
      "                                     'referenceID': 25,\n",
      "                                     'startOffset': 37}],\n",
      "              'references': [{'author': ['Xavier Anguera',\n",
      "                                         'Luis Javier Rodriguez-Fuentes',\n",
      "                                         'Igor Szöke',\n",
      "                                         'Andi Buzo',\n",
      "                                         'Florian Metze'],\n",
      "                              'citeRegEx': 'Anguera et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Anguera et al\\\\.',\n",
      "                              'title': 'Query by example search on speech at '\n",
      "                                       'mediaeval',\n",
      "                              'venue': 'In MediaEval,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Kartik Audhkhasi',\n",
      "                                         'Andrew Rosenberg',\n",
      "                                         'Abhinav Sethy',\n",
      "                                         'Bhuvana Ramabhadran',\n",
      "                                         'Brian Kingsbury'],\n",
      "                              'citeRegEx': 'Audhkhasi et al\\\\.,? \\\\Q2017\\\\E',\n",
      "                              'shortCiteRegEx': 'Audhkhasi et al\\\\.',\n",
      "                              'title': 'End-to-end ASR-free keyword search '\n",
      "                                       'from speech',\n",
      "                              'venue': 'arXiv preprint arXiv:1701.04313,',\n",
      "                              'year': 2017},\n",
      "                             {'author': ['Samy Bengio', 'Georg Heigold'],\n",
      "                              'citeRegEx': 'Bengio and Heigold.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Bengio and Heigold.',\n",
      "                              'title': 'Word embeddings for speech recognition',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Yoshua Bengio',\n",
      "                                         'Réjean Ducharme',\n",
      "                                         'Pascal Vincent',\n",
      "                                         'Christian Jauvin'],\n",
      "                              'citeRegEx': 'Bengio et al\\\\.,? \\\\Q2003\\\\E',\n",
      "                              'shortCiteRegEx': 'Bengio et al\\\\.',\n",
      "                              'title': 'A neural probabilistic language model',\n",
      "                              'venue': 'Journal of Machine Learing Research,',\n",
      "                              'year': 2003},\n",
      "                             {'author': ['Jane Bromley',\n",
      "                                         'Isabelle Guyon',\n",
      "                                         'Yann Lecun',\n",
      "                                         'Eduard Säckinger',\n",
      "                                         'Roopak Shah'],\n",
      "                              'citeRegEx': 'Bromley et al\\\\.,? \\\\Q1993\\\\E',\n",
      "                              'shortCiteRegEx': 'Bromley et al\\\\.',\n",
      "                              'title': 'Signature verification using a siamese '\n",
      "                                       'time delay neural network',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 1993},\n",
      "                             {'author': ['Michael A Carlin',\n",
      "                                         'Samuel Thomas',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Hynek Hermansky'],\n",
      "                              'citeRegEx': 'Carlin et al\\\\.,? \\\\Q2011\\\\E',\n",
      "                              'shortCiteRegEx': 'Carlin et al\\\\.',\n",
      "                              'title': 'Rapid evaluation of speech '\n",
      "                                       'representations for spoken term '\n",
      "                                       'discovery',\n",
      "                              'venue': 'In Proc. Interspeech,',\n",
      "                              'year': 2011},\n",
      "                             {'author': ['Guoguo Chen',\n",
      "                                         'Carolina Parada',\n",
      "                                         'Tara N Sainath'],\n",
      "                              'citeRegEx': 'Chen et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Chen et al\\\\.',\n",
      "                              'title': 'Query-by-example keyword spotting '\n",
      "                                       'using long short-term memory networks',\n",
      "                              'venue': 'In Proc. ICASSP,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Sumit Chopra',\n",
      "                                         'Raia Hadsell',\n",
      "                                         'Yann LeCun'],\n",
      "                              'citeRegEx': 'Chopra et al\\\\.,? \\\\Q2005\\\\E',\n",
      "                              'shortCiteRegEx': 'Chopra et al\\\\.',\n",
      "                              'title': 'Learning a similarity metric '\n",
      "                                       'discriminatively, with application to '\n",
      "                                       'face verification',\n",
      "                              'venue': 'In IEEE Computer Society Conf. '\n",
      "                                       'Computer Vision and Pattern '\n",
      "                                       'Recognition,',\n",
      "                              'year': 2005},\n",
      "                             {'author': ['Yu-An Chung',\n",
      "                                         'Chao-Chung Wu',\n",
      "                                         'Chia-Hao Shen',\n",
      "                                         'Hung-Yi Lee'],\n",
      "                              'citeRegEx': 'Chung et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Chung et al\\\\.',\n",
      "                              'title': 'Unsupervised learning of audio segment '\n",
      "                                       'representations using '\n",
      "                                       'sequence-to-sequence recurrent neural '\n",
      "                                       'networks',\n",
      "                              'venue': 'In Proc. Interspeech,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Scott Deerwester',\n",
      "                                         'Susan T Dumais',\n",
      "                                         'George W Furnas',\n",
      "                                         'Thomas K Landauer',\n",
      "                                         'Richard Harshman'],\n",
      "                              'citeRegEx': 'Deerwester et al\\\\.,? \\\\Q1990\\\\E',\n",
      "                              'shortCiteRegEx': 'Deerwester et al\\\\.',\n",
      "                              'title': 'Indexing by latent semantic analysis',\n",
      "                              'venue': 'Journal of the American society for '\n",
      "                                       'information science,',\n",
      "                              'year': 1990},\n",
      "                             {'author': ['Lev Finkelstein',\n",
      "                                         'Evgeniy Gabrilovich',\n",
      "                                         'Yossi Matias',\n",
      "                                         'Ehud Rivlin',\n",
      "                                         'Zach Solan',\n",
      "                                         'Gadi Wolfman',\n",
      "                                         'Eytan Ruppin'],\n",
      "                              'citeRegEx': 'Finkelstein et al\\\\.,? \\\\Q2001\\\\E',\n",
      "                              'shortCiteRegEx': 'Finkelstein et al\\\\.',\n",
      "                              'title': 'Placing search in context: The concept '\n",
      "                                       'revisited',\n",
      "                              'venue': 'In Proceedings of the 10th '\n",
      "                                       'international conference on World Wide '\n",
      "                                       'Web,',\n",
      "                              'year': 2001},\n",
      "                             {'author': ['Jonathan G Fiscus',\n",
      "                                         'Jerome Ajot',\n",
      "                                         'John S Garofolo',\n",
      "                                         'George Doddingtion'],\n",
      "                              'citeRegEx': 'Fiscus et al\\\\.,? \\\\Q2007\\\\E',\n",
      "                              'shortCiteRegEx': 'Fiscus et al\\\\.',\n",
      "                              'title': 'Results of the 2006 spoken term '\n",
      "                                       'detection evaluation',\n",
      "                              'venue': 'In Proc. SIGIR,',\n",
      "                              'year': 2007},\n",
      "                             {'author': ['Jort F Gemmeke',\n",
      "                                         'Tuomas Virtanen',\n",
      "                                         'Antti Hurmalainen'],\n",
      "                              'citeRegEx': 'Gemmeke et al\\\\.,? \\\\Q2011\\\\E',\n",
      "                              'shortCiteRegEx': 'Gemmeke et al\\\\.',\n",
      "                              'title': 'Exemplar-based sparse representations '\n",
      "                                       'for noise robust automatic speech '\n",
      "                                       'recognition',\n",
      "                              'venue': 'IEEE Transactions on Acoustics, '\n",
      "                                       'Speech, and Language Processing,',\n",
      "                              'year': 2011},\n",
      "                             {'author': ['Sahar Ghannay',\n",
      "                                         'Yannick Esteve',\n",
      "                                         'Nathalie Camelin',\n",
      "                                         'Paul Deleglise'],\n",
      "                              'citeRegEx': 'Ghannay et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Ghannay et al\\\\.',\n",
      "                              'title': 'Evaluation of acoustic word embeddings',\n",
      "                              'venue': 'In Proc. ACL Workshop on Evaluating '\n",
      "                                       'Vector-Space Representations for NLP,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['John J Godfrey',\n",
      "                                         'Edward C Holliman',\n",
      "                                         'Jane McDaniel'],\n",
      "                              'citeRegEx': 'Godfrey et al\\\\.,? \\\\Q1992\\\\E',\n",
      "                              'shortCiteRegEx': 'Godfrey et al\\\\.',\n",
      "                              'title': 'SWITCHBOARD: Telephone speech corpus '\n",
      "                                       'for research and development',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 1992},\n",
      "                             {'author': ['Alex Graves',\n",
      "                                         'Abdel rahman Mohamed',\n",
      "                                         'Geoffrey Hinton'],\n",
      "                              'citeRegEx': 'Graves et al\\\\.,? \\\\Q2013\\\\E',\n",
      "                              'shortCiteRegEx': 'Graves et al\\\\.',\n",
      "                              'title': 'Speech recognition with deep recurrent '\n",
      "                                       'neural networks',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2013},\n",
      "                             {'author': ['Raia Hadsell',\n",
      "                                         'Sumit Chopra',\n",
      "                                         'Yann LeCun'],\n",
      "                              'citeRegEx': 'Hadsell et al\\\\.,? \\\\Q2006\\\\E',\n",
      "                              'shortCiteRegEx': 'Hadsell et al\\\\.',\n",
      "                              'title': 'Dimensionality reduction by learning '\n",
      "                                       'an invariant mapping',\n",
      "                              'venue': 'In IEEE Computer Society Conf. '\n",
      "                                       'Computer Vision and Pattern '\n",
      "                                       'Recognition,',\n",
      "                              'year': 2006},\n",
      "                             {'author': ['David Harwath', 'James Glass'],\n",
      "                              'citeRegEx': 'Harwath and Glass.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Harwath and Glass.',\n",
      "                              'title': 'Deep multimodal semantic embeddings '\n",
      "                                       'for speech and images',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Automatic '\n",
      "                                       'Speech Recognition and Understanding '\n",
      "                                       '(ASRU),',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['David Harwath', 'James R Glass'],\n",
      "                              'citeRegEx': 'Harwath and Glass.,? \\\\Q2017\\\\E',\n",
      "                              'shortCiteRegEx': 'Harwath and Glass.',\n",
      "                              'title': 'Learning word-like units from joint '\n",
      "                                       'audio-visual analysis',\n",
      "                              'venue': 'arXiv preprint arXiv:1701.07481,',\n",
      "                              'year': 2017},\n",
      "                             {'author': ['David Harwath',\n",
      "                                         'Antonio Torralba',\n",
      "                                         'James Glass'],\n",
      "                              'citeRegEx': 'Harwath et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Harwath et al\\\\.',\n",
      "                              'title': 'Unsupervised learning of spoken '\n",
      "                                       'language with visual context',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Karl Moritz Hermann', 'Phil Blunsom'],\n",
      "                              'citeRegEx': 'Hermann and Blunsom.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Hermann and Blunsom.',\n",
      "                              'title': 'Multilingual distributed '\n",
      "                                       'representations without word alignment',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Felix Hill',\n",
      "                                         'Roi Reichart',\n",
      "                                         'Anna Korhonen'],\n",
      "                              'citeRegEx': 'Hill et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Hill et al\\\\.',\n",
      "                              'title': 'SimLex-999: Evaluating semantic models '\n",
      "                                       'with (genuine) similarity estimation',\n",
      "                              'venue': 'Computational Linguistics,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Sepp Hochreiter',\n",
      "                                         'Jürgen Schmidhuber'],\n",
      "                              'citeRegEx': 'Hochreiter and Schmidhuber.,? '\n",
      "                                           '\\\\Q1997\\\\E',\n",
      "                              'shortCiteRegEx': 'Hochreiter and Schmidhuber.',\n",
      "                              'title': 'Long short-term memory',\n",
      "                              'venue': 'Neural Computation,',\n",
      "                              'year': 1997},\n",
      "                             {'author': ['Baotian Hu',\n",
      "                                         'Zhengdong Lu',\n",
      "                                         'Hang Li',\n",
      "                                         'Qingcai Chen'],\n",
      "                              'citeRegEx': 'Hu et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Hu et al\\\\.',\n",
      "                              'title': 'Convolutional neural network '\n",
      "                                       'architectures for matching natural '\n",
      "                                       'language sentences',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Mohit Iyyer',\n",
      "                                         'Varun Manjunatha',\n",
      "                                         'Jordan Boyd-Graber',\n",
      "                                         'Hal Daumé III'],\n",
      "                              'citeRegEx': 'Iyyer et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Iyyer et al\\\\.',\n",
      "                              'title': 'Deep unordered composition rivals '\n",
      "                                       'syntactic methods for text '\n",
      "                                       'classification',\n",
      "                              'venue': 'In Proc. Association for Computational '\n",
      "                                       'Linguistics,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Herman Kamper',\n",
      "                                         'Micah Elsner',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Sharon J. Goldwater'],\n",
      "                              'citeRegEx': 'Kamper et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Kamper et al\\\\.',\n",
      "                              'title': 'Unsupervised neural network based '\n",
      "                                       'feature extraction using weak top-down '\n",
      "                                       'constraints',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Herman Kamper',\n",
      "                                         'Weiran Wang',\n",
      "                                         'Karen Livescu'],\n",
      "                              'citeRegEx': 'Kamper et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Kamper et al\\\\.',\n",
      "                              'title': 'Deep convolutional acoustic word '\n",
      "                                       'embeddings using word-pair side '\n",
      "                                       'information',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Diederik Kingma', 'Jimmy Ba'],\n",
      "                              'citeRegEx': 'Kingma and Ba.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Kingma and Ba.',\n",
      "                              'title': 'ADAM: A method for stochastic '\n",
      "                                       'optimization',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Ryan Kiros',\n",
      "                                         'Yukun Zhu',\n",
      "                                         'Ruslan R Salakhutdinov',\n",
      "                                         'Richard Zemel',\n",
      "                                         'Raquel Urtasun',\n",
      "                                         'Antonio Torralba',\n",
      "                                         'Sanja Fidler'],\n",
      "                              'citeRegEx': 'Kiros et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Kiros et al\\\\.',\n",
      "                              'title': 'Skip-thought vectors',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Keith Levin',\n",
      "                                         'Katharine Henry',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Karen Livescu'],\n",
      "                              'citeRegEx': 'Levin et al\\\\.,? \\\\Q2013\\\\E',\n",
      "                              'shortCiteRegEx': 'Levin et al\\\\.',\n",
      "                              'title': 'Fixed-dimensional acoustic embeddings '\n",
      "                                       'of variable-length segments in '\n",
      "                                       'low-resource settings',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Automatic '\n",
      "                                       'Speech Recognition and Understanding '\n",
      "                                       '(ASRU),',\n",
      "                              'year': 2013},\n",
      "                             {'author': ['Keith Levin',\n",
      "                                         'Aren Jansen',\n",
      "                                         'Benjamin Van Durme'],\n",
      "                              'citeRegEx': 'Levin et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Levin et al\\\\.',\n",
      "                              'title': 'Segmental acoustic indexing for zero '\n",
      "                                       'resource keyword search',\n",
      "                              'venue': 'In IEEE Int. Conf. Acoustics, Speech '\n",
      "                                       'and Sig. Proc.,',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['Andrew L Maas',\n",
      "                                         'Stephen D Miller',\n",
      "                                         'Tyler M O’neil',\n",
      "                                         'Andrew Y Ng',\n",
      "                                         'Patrick Nguyen'],\n",
      "                              'citeRegEx': 'Maas et al\\\\.,? \\\\Q2012\\\\E',\n",
      "                              'shortCiteRegEx': 'Maas et al\\\\.',\n",
      "                              'title': 'Word-level acoustic modeling with '\n",
      "                                       'convolutional vector regression',\n",
      "                              'venue': 'In Proc. ICML Workshop on '\n",
      "                                       'Representation Learning,',\n",
      "                              'year': 2012},\n",
      "                             {'author': ['Tomas Mikolov',\n",
      "                                         'Ilya Sutskever',\n",
      "                                         'Kai Chen',\n",
      "                                         'Greg S Corrado',\n",
      "                                         'Jeff Dean'],\n",
      "                              'citeRegEx': 'Mikolov et al\\\\.,? \\\\Q2013\\\\E',\n",
      "                              'shortCiteRegEx': 'Mikolov et al\\\\.',\n",
      "                              'title': 'Distributed representations of words '\n",
      "                                       'and phrases and their compositionality',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2013},\n",
      "                             {'author': ['Andriy Mnih', 'Geoffrey Hinton'],\n",
      "                              'citeRegEx': 'Mnih and Hinton.,? \\\\Q2007\\\\E',\n",
      "                              'shortCiteRegEx': 'Mnih and Hinton.',\n",
      "                              'title': 'Three new graphical models for '\n",
      "                                       'statistical language modelling',\n",
      "                              'venue': 'In ICML,',\n",
      "                              'year': 2007},\n",
      "                             {'author': ['Jiquan Ngiam',\n",
      "                                         'Aditya Khosla',\n",
      "                                         'Mingyu Kim',\n",
      "                                         'Juhan Nam',\n",
      "                                         'Honglak Lee',\n",
      "                                         'Andrew Ng'],\n",
      "                              'citeRegEx': 'Ngiam et al\\\\.,? \\\\Q2011\\\\E',\n",
      "                              'shortCiteRegEx': 'Ngiam et al\\\\.',\n",
      "                              'title': 'Multimodal deep learning',\n",
      "                              'venue': 'In ICML, pp',\n",
      "                              'year': 2011},\n",
      "                             {'author': ['Jeffrey Pennington',\n",
      "                                         'Richard Socher',\n",
      "                                         'Christopher D Manning'],\n",
      "                              'citeRegEx': 'Pennington et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Pennington et al\\\\.',\n",
      "                              'title': 'GloVe: Global vectors for word '\n",
      "                                       'representation',\n",
      "                              'venue': 'In Proc. Conference on Empirical '\n",
      "                                       'Methods in Natural Language '\n",
      "                                       'Processing,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Shane Settle', 'Karen Livescu'],\n",
      "                              'citeRegEx': 'Settle and Livescu.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Settle and Livescu.',\n",
      "                              'title': 'Discriminative acoustic word '\n",
      "                                       'embeddings: Recurrent neural '\n",
      "                                       'network-based approaches',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Spoken '\n",
      "                                       'Language Technology (SLT),',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Richard Socher',\n",
      "                                         'Andrej Karpathy',\n",
      "                                         'Quoc V Le',\n",
      "                                         'Christopher D Manning',\n",
      "                                         'Andrew Y Ng'],\n",
      "                              'citeRegEx': 'Socher et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Socher et al\\\\.',\n",
      "                              'title': 'Grounded compositional semantics for '\n",
      "                                       'finding and describing images with '\n",
      "                                       'sentences',\n",
      "                              'venue': 'Transactions of the Association for '\n",
      "                                       'Computational Linguistics,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Kihyuk Sohn',\n",
      "                                         'Wenling Shang',\n",
      "                                         'Honglak Lee'],\n",
      "                              'citeRegEx': 'Sohn et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Sohn et al\\\\.',\n",
      "                              'title': 'Improved multimodal deep learning with '\n",
      "                                       'variation of information',\n",
      "                              'venue': 'In Advances in Neural Information '\n",
      "                                       'Processing Systems (NIPS),',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Nitish Srivastava',\n",
      "                                         'Ruslan Salakhutdinov'],\n",
      "                              'citeRegEx': 'Srivastava and Salakhutdinov.,? '\n",
      "                                           '\\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Srivastava and Salakhutdinov.',\n",
      "                              'title': 'Multimodal learning with deep '\n",
      "                                       'boltzmann machines',\n",
      "                              'venue': 'Journal of Machine Learing Research,',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Gabriel Synnaeve',\n",
      "                                         'Thomas Schatz',\n",
      "                                         'Emmanuel Dupoux'],\n",
      "                              'citeRegEx': 'Synnaeve et al\\\\.,? \\\\Q2014\\\\E',\n",
      "                              'shortCiteRegEx': 'Synnaeve et al\\\\.',\n",
      "                              'title': 'Phonetics embedding learning with side '\n",
      "                                       'information',\n",
      "                              'venue': 'In Proc. IEEE Workshop on Spoken '\n",
      "                                       'Language Technology (SLT),',\n",
      "                              'year': 2014},\n",
      "                             {'author': ['Laurens J.P. van der Maaten',\n",
      "                                         'Geoffrey E. Hinton'],\n",
      "                              'citeRegEx': 'Maaten and Hinton.,? \\\\Q2008\\\\E',\n",
      "                              'shortCiteRegEx': 'Maaten and Hinton.',\n",
      "                              'title': 'Visualizing data using t-SNE',\n",
      "                              'venue': 'Journal of Machine Learing Research,',\n",
      "                              'year': 2008},\n",
      "                             {'author': ['Ivan Vendrov',\n",
      "                                         'Ryan Kiros',\n",
      "                                         'Sanja Fidler',\n",
      "                                         'Raquel Urtasun'],\n",
      "                              'citeRegEx': 'Vendrov et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Vendrov et al\\\\.',\n",
      "                              'title': 'Order-embeddings of images and '\n",
      "                                       'language',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2016},\n",
      "                             {'author': ['Weiran Wang',\n",
      "                                         'Raman Arora',\n",
      "                                         'Karen Livescu',\n",
      "                                         'Jeff Bilmes'],\n",
      "                              'citeRegEx': 'Wang et al\\\\.,? \\\\Q2015\\\\E',\n",
      "                              'shortCiteRegEx': 'Wang et al\\\\.',\n",
      "                              'title': 'On deep multi-view representation '\n",
      "                                       'learning',\n",
      "                              'venue': 'In ICML, pp',\n",
      "                              'year': 2015},\n",
      "                             {'author': ['John Wieting',\n",
      "                                         'Mohit Bansal',\n",
      "                                         'Kevin Gimpel',\n",
      "                                         'Karen Livescu'],\n",
      "                              'citeRegEx': 'Wieting et al\\\\.,? \\\\Q2016\\\\E',\n",
      "                              'shortCiteRegEx': 'Wieting et al\\\\.',\n",
      "                              'title': 'Towards universal paraphrastic '\n",
      "                                       'sentence embeddings',\n",
      "                              'venue': 'In Int. Conf. Learning '\n",
      "                                       'Representations,',\n",
      "                              'year': 2016}],\n",
      "              'sections': [{'heading': '1 INTRODUCTION',\n",
      "                            'text': 'Word embeddings—continuous-valued vector '\n",
      "                                    'representations of words—are an almost '\n",
      "                                    'ubiquitous component of recent natural '\n",
      "                                    'language processing (NLP) research. Word '\n",
      "                                    'embeddings can be learned using spectral '\n",
      "                                    'methods (Deerwester et al., 1990) or, '\n",
      "                                    'more commonly in recent work, via neural '\n",
      "                                    'networks (Bengio et al., 2003; Mnih & '\n",
      "                                    'Hinton, 2007; Mikolov et al., 2013; '\n",
      "                                    'Pennington et al., 2014). Word embeddings '\n",
      "                                    'can also be composed to form embeddings '\n",
      "                                    'of phrases, sentences, or documents '\n",
      "                                    '(Socher et al., 2014; Kiros et al., 2015; '\n",
      "                                    'Wieting et al., 2016; Iyyer et al., '\n",
      "                                    '2015).\\n'\n",
      "                                    'In typical NLP applications, such '\n",
      "                                    'embeddings are intended to represent the '\n",
      "                                    'semantics of the corresponding '\n",
      "                                    'words/sequences. In contrast, embeddings '\n",
      "                                    'that represent the way a word or sequence '\n",
      "                                    'sounds are rarely considered. In this '\n",
      "                                    'work we address this problem, starting '\n",
      "                                    'with embeddings of individual words. Such '\n",
      "                                    'embeddings could be useful for tasks like '\n",
      "                                    'spoken term detection (Fiscus et al., '\n",
      "                                    '2007), spoken query-by-example search '\n",
      "                                    '(Anguera et al., 2014), or even speech '\n",
      "                                    'recognition using a whole-word approach '\n",
      "                                    '(Gemmeke et al., 2011; Bengio & Heigold, '\n",
      "                                    '2014). In tasks that involve comparing '\n",
      "                                    'speech segments to each other, vector '\n",
      "                                    'embeddings can allow more efficient and '\n",
      "                                    'more accurate distance computation than '\n",
      "                                    'sequence-based approaches such as dynamic '\n",
      "                                    'time warping (Levin et al., 2013, 2015; '\n",
      "                                    'Kamper et al., 2016; Settle & Livescu, '\n",
      "                                    '2016; Chung et al., 2016).\\n'\n",
      "                                    'We consider the problem of learning '\n",
      "                                    'vector representations of acoustic '\n",
      "                                    'sequences and orthographic (character) '\n",
      "                                    'sequences corresponding to single words, '\n",
      "                                    'such that the learned embeddings '\n",
      "                                    'represent the way the word sounds. We '\n",
      "                                    'take a multi-view approach, where we '\n",
      "                                    'jointly learn the embeddings for '\n",
      "                                    'character and acoustic sequences. We '\n",
      "                                    'consider several contrastive losses, '\n",
      "                                    'based on learning from pairs of matched '\n",
      "                                    'acoustic-orthographic examples and '\n",
      "                                    'randomly drawn mismatched pairs. The '\n",
      "                                    'losses correspond to different goals for '\n",
      "                                    'learning such embeddings; for example, we '\n",
      "                                    'might want the embeddings of two '\n",
      "                                    'waveforms to be close when they '\n",
      "                                    'correspond to the same word and far when '\n",
      "                                    'they correspond to different ones, or we '\n",
      "                                    'might want the distances between '\n",
      "                                    'embeddings to correspond to some '\n",
      "                                    'ground-truth orthographic edit distance.\\n'\n",
      "                                    'One of the useful properties of this '\n",
      "                                    'multi-view approach is that, unlike '\n",
      "                                    'earlier work on acoustic word embeddings, '\n",
      "                                    'it produces both acoustic and '\n",
      "                                    'orthographic embeddings that can be '\n",
      "                                    'directly compared. This makes it possible '\n",
      "                                    'to use the same learned embeddings for '\n",
      "                                    'multiple single-view and cross-view '\n",
      "                                    'tasks. Our multi-view embeddings produce '\n",
      "                                    'improved results over earlier work on '\n",
      "                                    'acoustic word discrimination, as well as '\n",
      "                                    'encouraging results on cross-view '\n",
      "                                    'discrimination and word similarity.1'},\n",
      "                           {'heading': '2 OUR APPROACH',\n",
      "                            'text': 'In this section, we first introduce our '\n",
      "                                    'approach for learning acoustic word '\n",
      "                                    'embeddings in a multiview setting, after '\n",
      "                                    'briefly reviewing related approaches to '\n",
      "                                    'put ours in context. We then discuss the '\n",
      "                                    'particular neural network architecture we '\n",
      "                                    'use, based on bidirectional long '\n",
      "                                    'short-term memory (LSTM) networks '\n",
      "                                    '(Hochreiter & Schmidhuber, 1997).'},\n",
      "                           {'heading': '2.1 MULTI-VIEW LEARNING OF ACOUSTIC '\n",
      "                                       'WORD EMBEDDINGS',\n",
      "                            'text': 'Previous approaches have focused on '\n",
      "                                    'learning acoustic word embeddings in a '\n",
      "                                    '“single-view” setting. In the simplest '\n",
      "                                    'approach, one uses supervision of the '\n",
      "                                    'form “acoustic segment x is an instance '\n",
      "                                    'of the word y”, and trains the embedding '\n",
      "                                    'to be discriminative of the word '\n",
      "                                    'identity. Formally, given a dataset of '\n",
      "                                    'paired acoustic segments and word labels '\n",
      "                                    '{(xi, yi)}Ni=1, this approach solves the '\n",
      "                                    'following optimization:\\n'\n",
      "                                    'min f,h\\n'\n",
      "                                    'objclassify := 1\\n'\n",
      "                                    'N N∑ i ` (h(f(xi)),yi) , (1)\\n'\n",
      "                                    'where network f maps an acoustic segment '\n",
      "                                    'into a fixed-dimensional feature '\n",
      "                                    'vector/embedding, h is a classifier that '\n",
      "                                    'predicts the corresponding word label '\n",
      "                                    'from the label set of the training data, '\n",
      "                                    'and the loss ` measures the discrepancy '\n",
      "                                    'between the prediction and ground-truth '\n",
      "                                    'word label (one can use any multi-class '\n",
      "                                    'classification loss here, and a typical '\n",
      "                                    'choice is the cross-entropy loss where h '\n",
      "                                    'has a softmax top layer). The two '\n",
      "                                    'networks f and h are trained jointly. '\n",
      "                                    'Equivalently, one could consider the '\n",
      "                                    'composition h(f(x)) as a classifier '\n",
      "                                    'network, and use any intermediate layer’s '\n",
      "                                    'activations as the features. We refer to '\n",
      "                                    'the objective in (1) as the “classifier '\n",
      "                                    'network” objective, which has been used '\n",
      "                                    'in several prior studies on acoustic word '\n",
      "                                    'embeddings (Bengio & Heigold, 2014; '\n",
      "                                    'Kamper et al., 2016; Settle & Livescu, '\n",
      "                                    '2016).\\n'\n",
      "                                    'This objective, however, is not ideal for '\n",
      "                                    'learning acoustic word embeddings. This '\n",
      "                                    'is because the set of possible word '\n",
      "                                    'labels is huge, and we may not have '\n",
      "                                    'enough instances of each label to train a '\n",
      "                                    'good classifier. In downstream tasks, we '\n",
      "                                    'may encounter acoustic segments of words '\n",
      "                                    'that did not appear in the embedding '\n",
      "                                    'training set, and it is not clear that '\n",
      "                                    'the classifier-based embeddings will have '\n",
      "                                    'reasonable behavior on previously unseen '\n",
      "                                    'words.\\n'\n",
      "                                    'An alternative approach, based on Siamese '\n",
      "                                    'networks (Bromley et al., 1993), uses '\n",
      "                                    'supervision of the form “segment x1 is '\n",
      "                                    'similar to segment x2, and is not similar '\n",
      "                                    'to segment x3”, where two segments are '\n",
      "                                    'considered similar if they have the same '\n",
      "                                    'word label and dissimilar otherwise. '\n",
      "                                    'Models based on Siamese networks have '\n",
      "                                    'been used for a variety of representation '\n",
      "                                    'learning problems in NLP (Hu et al., '\n",
      "                                    '2014; Wieting et al., 2016), vision '\n",
      "                                    '(Hadsell et al., 2006), and speech '\n",
      "                                    '(Synnaeve et al., 2014; Kamper et al., '\n",
      "                                    '2015) including acoustic word embeddings '\n",
      "                                    '(Kamper et al., 2016; Settle & Livescu, '\n",
      "                                    '2016). A typical objective in this '\n",
      "                                    'category enforces that the distance '\n",
      "                                    'between (x1, x3) is larger than the '\n",
      "                                    'distance between (x1, x2) by some '\n",
      "                                    'margin:\\n'\n",
      "                                    'min f\\n'\n",
      "                                    'objsiamese := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x1i ), f(x 2 i '\n",
      "                                    ') ) − dis ( f(x1i ), f(x 3 i ) )) , (2)\\n'\n",
      "                                    'where the network f extracts the '\n",
      "                                    'fixed-dimensional embedding, the distance '\n",
      "                                    'function dis (·, ·) measures the distance '\n",
      "                                    'between the two embedding vectors, andm > '\n",
      "                                    '0 is the margin parameter. The term '\n",
      "                                    '“Siamese” (Bromley et al., 1993; Chopra '\n",
      "                                    'et al., 2005) refers to the fact that the '\n",
      "                                    'triplet (x1, x2, x3) share the same '\n",
      "                                    'embedding network f .\\n'\n",
      "                                    'Unlike the classification-based loss, the '\n",
      "                                    'Siamese network loss does not enforce '\n",
      "                                    'hard decisions on the label of each '\n",
      "                                    'segment. Instead it tries to learn '\n",
      "                                    'embeddings that respect distances between '\n",
      "                                    'word\\n'\n",
      "                                    '1Our tensorflow implementation is '\n",
      "                                    'available at '\n",
      "                                    'https://github.com/opheadacheh/Multi-view-neural-acoustic-words-embeddings\\n'\n",
      "                                    'pairs, which can be helpful for dealing '\n",
      "                                    'with unseen words. The Siamese network '\n",
      "                                    'approach also uses more examples in '\n",
      "                                    'training, as one can easily generate many '\n",
      "                                    'more triplets than (segment, label) '\n",
      "                                    'pairs, and it is not limited to those '\n",
      "                                    'labels that occur a sufficient number of '\n",
      "                                    'times in the training set.\\n'\n",
      "                                    'The above approaches treat the word '\n",
      "                                    'labels as discrete classes, which ignores '\n",
      "                                    'the similarity between different words, '\n",
      "                                    'and does not take advantage of the more '\n",
      "                                    'complex information contained in the '\n",
      "                                    'character sequences corresponding to word '\n",
      "                                    'labels. The orthography naturally '\n",
      "                                    'reflects some aspects of similarity '\n",
      "                                    'between the words’ pronunciations, which '\n",
      "                                    'should also be reflected in the acoustic '\n",
      "                                    'embeddings. One way to learn features '\n",
      "                                    'from multiple sources of complementary '\n",
      "                                    'information is using a multi-view '\n",
      "                                    'representation learning setting. We take '\n",
      "                                    'this approach, and consider the acoustic '\n",
      "                                    'segment and the character sequence to be '\n",
      "                                    'two different views of the pronunciation '\n",
      "                                    'of the word.\\n'\n",
      "                                    'While many deep multi-view learning '\n",
      "                                    'objectives are applicable (Ngiam et al., '\n",
      "                                    '2011; Srivastava & Salakhutdinov, 2014; '\n",
      "                                    'Sohn et al., 2014; Wang et al., 2015), we '\n",
      "                                    'consider the multi-view contrastive loss '\n",
      "                                    'objective of (Hermann & Blunsom, 2014), '\n",
      "                                    'which is simple to optimize and implement '\n",
      "                                    'and performs well in practice. In this '\n",
      "                                    'algorithm, we embed acoustic segments x '\n",
      "                                    'by a network f and character label '\n",
      "                                    'sequences c by another network g into a '\n",
      "                                    'common space, and use weak supervision of '\n",
      "                                    'the form “for paired segment x+ and its '\n",
      "                                    'character label sequence c+, the distance '\n",
      "                                    'between their embedding is much smaller '\n",
      "                                    'than the distance between embeddings of '\n",
      "                                    'x+ and an unmatched character label '\n",
      "                                    'sequence c−”. Formally, we optimize the '\n",
      "                                    'following objective with such '\n",
      "                                    'supervision:\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj0 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( f(x+i ), g(c − i ) )) , (3)\\n'\n",
      "                                    'where c−i is a negative character label '\n",
      "                                    'sequence of x + i to be contrasted with '\n",
      "                                    'the positive/correct character sequence '\n",
      "                                    'c+i , and m is the margin parameter. In '\n",
      "                                    'this paper we use the cosine distance,\\n'\n",
      "                                    'dis (a,b) = 1− 〈\\n'\n",
      "                                    'a ‖a‖ , b ‖b‖\\n'\n",
      "                                    '〉 .2\\n'\n",
      "                                    'Note that in the multi-view setting, we '\n",
      "                                    'have multiple ways of generating triplets '\n",
      "                                    'that contain one positive pair and one '\n",
      "                                    'negative pair each. Below are the other '\n",
      "                                    'three objectives we explore in this '\n",
      "                                    'paper:\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj1 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( g(c+i ), g(c − i ) )) , (4)\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj2 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( f(x−i ), g(c + i ) )) , (5)\\n'\n",
      "                                    'min f,g\\n'\n",
      "                                    'obj3 := 1\\n'\n",
      "                                    'N N∑ i max ( 0, m+ dis ( f(x+i ), g(c + i '\n",
      "                                    ') ) − dis ( f(x+i ), f(x − i ) )) . (6)\\n'\n",
      "                                    'x−i in (5) and (6) refers to a negative '\n",
      "                                    'acoustic feature sequence, that is one '\n",
      "                                    'with a different label from x+i . We note '\n",
      "                                    'that obj\\n'\n",
      "                                    '1 and obj3 contain distances between '\n",
      "                                    'same-view embeddings, and are less '\n",
      "                                    'thoroughly explored in the literature. We '\n",
      "                                    'will also consider combinations of obj0 '\n",
      "                                    'through obj3.\\n'\n",
      "                                    'Finally, thus far we have considered '\n",
      "                                    'losses that do not explicitly take into '\n",
      "                                    'account the degree of difference between '\n",
      "                                    'the positive and negative pairs (although '\n",
      "                                    'the learned embeddings may implicitly '\n",
      "                                    'learn this through the relationship '\n",
      "                                    'between sequences in the two views). We '\n",
      "                                    'also consider a costsensitive objective '\n",
      "                                    'designed to explicitly arrange the '\n",
      "                                    'embedding space such that word similarity '\n",
      "                                    'is respected. In (3), instead of a fixed '\n",
      "                                    'margin m, we use:\\n'\n",
      "                                    'm(c+, c−) := mmax · min (tmax, editdis(c\\n'\n",
      "                                    '+, c−))\\n'\n",
      "                                    'tmax , (7)\\n'\n",
      "                                    'where tmax > 0 is a threshold for edit '\n",
      "                                    'distances (all edit distances above tmax '\n",
      "                                    'are considered equally bad), and mmax is '\n",
      "                                    'the maximum margin we impose. The margin '\n",
      "                                    'is set to mmax if the edit distance '\n",
      "                                    'between two character sequences is above '\n",
      "                                    'tmax; otherwise it scales linearly with '\n",
      "                                    'the edit distance editdis(c+, c−)). We '\n",
      "                                    'use the Levenshtein distance as the edit '\n",
      "                                    'distance. Here we explore the '\n",
      "                                    'costsensitive margin with obj0, but it '\n",
      "                                    'could in principle be used with other '\n",
      "                                    'objectives as well.\\n'\n",
      "                                    '2In experiments, we use the unit-length '\n",
      "                                    'vector a‖a‖ as the embedding. It tends to '\n",
      "                                    'perform better than f(x) and more '\n",
      "                                    'directly reflects the cosine similarity. '\n",
      "                                    'This is equivalent to adding a nonlinear '\n",
      "                                    'normalization layer on top of f .'},\n",
      "                           {'heading': '2.2 RECURRENT NEURAL NETWORK '\n",
      "                                       'ARCHITECTURE',\n",
      "                            'text': 'Since the inputs of both views have a '\n",
      "                                    'sequential structure, we implement both f '\n",
      "                                    'and g with recurrent neural networks and '\n",
      "                                    'in particular long-short term memory '\n",
      "                                    'networks (LSTMs). Recurrent neural '\n",
      "                                    'networks are the state-of-the-art models '\n",
      "                                    'for a number of speech tasks including '\n",
      "                                    'speech recognition Graves et al. (2013), '\n",
      "                                    'and LSTM-based acoustic word embeddings '\n",
      "                                    'have produced the best results on one of '\n",
      "                                    'the tasks in our experiments (Settle & '\n",
      "                                    'Livescu, 2016).\\n'\n",
      "                                    'As shown in Figure 1, our f and g are '\n",
      "                                    'produced by multi-layer (stacked) '\n",
      "                                    'bidirectional LSTMs. The inputs can be '\n",
      "                                    'any frame-level acoustic feature '\n",
      "                                    'representation and vector representation '\n",
      "                                    'of the characters in the orthographic '\n",
      "                                    'input. At each layer, two LSTM cells '\n",
      "                                    'process the input sequence from left to '\n",
      "                                    'right and from right to left '\n",
      "                                    'respectively. At intermediate layers, the '\n",
      "                                    'outputs of the two LSTMs at each time '\n",
      "                                    'step are concatenated to form the input '\n",
      "                                    'sequence to the next layer. At the top '\n",
      "                                    'layer, the last time step outputs of the '\n",
      "                                    'two LSTMs are concatenated to form a '\n",
      "                                    'fixed-dimensional embedding of the view, '\n",
      "                                    'and the embeddings are then used to '\n",
      "                                    'calculate the cosine distances in our '\n",
      "                                    'objectives.'},\n",
      "                           {'heading': '3 RELATED WORK',\n",
      "                            'text': 'We are aware of no prior work on '\n",
      "                                    'multi-view learning of acoustic and '\n",
      "                                    'character-based word embeddings. However, '\n",
      "                                    'acoustic word embeddings learned in other '\n",
      "                                    'ways have recently begun to be studied. '\n",
      "                                    'Levin et al. (2013) proposed an approach '\n",
      "                                    'for embedding an arbitrary-length segment '\n",
      "                                    'of speech as a fixed-dimensional vector, '\n",
      "                                    'based on representing each word as a '\n",
      "                                    'vector of dynamic time warping (DTW) '\n",
      "                                    'distances to a set of template words. '\n",
      "                                    'This approach produced improved '\n",
      "                                    'performance on a word discrimination task '\n",
      "                                    'compared to using raw DTW distances, and '\n",
      "                                    'was later also applied successfully for a '\n",
      "                                    'query-by-example task (Levin et al., '\n",
      "                                    '2015). One disadvantage of this approach '\n",
      "                                    'is that, while DTW handles the issue of '\n",
      "                                    'variable sequence lengths, it is '\n",
      "                                    'computationally costly and involves a '\n",
      "                                    'number of DTW parameters that are not '\n",
      "                                    'learned.\\n'\n",
      "                                    'Kamper et al. (2016) and Settle & Livescu '\n",
      "                                    '(2016) later improved on Levin et al.’s '\n",
      "                                    'word discrimination results using '\n",
      "                                    'convolutional neural networks (CNNs) and '\n",
      "                                    'recurrent neural networks (RNNs) trained '\n",
      "                                    'with either a classification or '\n",
      "                                    'contrastive loss. Bengio & Heigold (2014) '\n",
      "                                    'trained convolutional neural network '\n",
      "                                    '(CNN)-based acoustic word embeddings for '\n",
      "                                    'rescoring the outputs of a speech '\n",
      "                                    'recognizer, using a loss combining '\n",
      "                                    'classification and ranking criteria. Maas '\n",
      "                                    'et al. (2012) trained a CNN to predict a '\n",
      "                                    'semantic word embedding from an acoustic '\n",
      "                                    'segment, and used the resulting '\n",
      "                                    'embeddings as features in a segmental '\n",
      "                                    'word-level speech recognizer. Harwath and '\n",
      "                                    'Glass Harwath & Glass (2015); Harwath et '\n",
      "                                    'al. (2016); Harwath & Glass (2017) '\n",
      "                                    'jointly trained CNN embeddings of images '\n",
      "                                    'and spoken captions, and showed that '\n",
      "                                    'word-like unit embeddings can be '\n",
      "                                    'extracted from the speech model. CNNs '\n",
      "                                    'require normalizing the duration of the '\n",
      "                                    'input sequences, which has typically been '\n",
      "                                    'done via padding. RNNs, on the other '\n",
      "                                    'hand, are more flexible in dealing with '\n",
      "                                    'very different-length sequences. Chen et '\n",
      "                                    'al. (2015) used long short-term memory '\n",
      "                                    '(LSTM) networks with a classification '\n",
      "                                    'loss to embed acoustic words for a simple '\n",
      "                                    '(single-query) query-by-example search '\n",
      "                                    'task. Chung et al. (2016) learned '\n",
      "                                    'acoustic word embeddings based on '\n",
      "                                    'recurrent neural network (RNN) '\n",
      "                                    'autoencoders, and found that they improve '\n",
      "                                    'over DTW for a word discrimination task '\n",
      "                                    'similar to that of Levin et al. (2013). '\n",
      "                                    'Audhkhasi et al. (2017) learned '\n",
      "                                    'autoencoders for acoustic and written '\n",
      "                                    'words, as well as a model for comparing '\n",
      "                                    'the two, and applied these to a keyword '\n",
      "                                    'search task.\\n'\n",
      "                                    'Evaluation of acoustic word embeddings in '\n",
      "                                    'downstream tasks such as speech '\n",
      "                                    'recognition and search can be costly, and '\n",
      "                                    'can obscure details of embedding models '\n",
      "                                    'and training approaches. Most evaluations '\n",
      "                                    'have been based on word discrimination – '\n",
      "                                    'the task of determining whether two '\n",
      "                                    'speech segments correspond to the same '\n",
      "                                    'word or not – which can be seen as a '\n",
      "                                    'proxy for query-by-example search (Levin '\n",
      "                                    'et al., 2013; Kamper et al., 2016; Settle '\n",
      "                                    '& Livescu, 2016; Chung et al., 2016). One '\n",
      "                                    'difference between word discrimination '\n",
      "                                    'and search/recognition tasks is that in '\n",
      "                                    'word discrimination the word boundaries '\n",
      "                                    'are given. However, prior work has been '\n",
      "                                    'able to apply results from word '\n",
      "                                    'discrimination Levin et al. (2013) to '\n",
      "                                    'improve a query-by-example system without '\n",
      "                                    'known word boundaries Levin et al. '\n",
      "                                    '(2015), by simply applying their '\n",
      "                                    'embeddings to non-word segments as well.\\n'\n",
      "                                    'The only prior work focused on vector '\n",
      "                                    'embeddings of character sequences '\n",
      "                                    'explicitly aimed at representing their '\n",
      "                                    'acoustic similarity is that of Ghannay et '\n",
      "                                    'al. (2016), who proposed evaluations '\n",
      "                                    'based on nearest-neighbor retrieval, '\n",
      "                                    'phonetic/orthographic similarity '\n",
      "                                    'measures, and homophone disambiguation. '\n",
      "                                    'We use related tasks here, as well as '\n",
      "                                    'acoustic word discrimination for '\n",
      "                                    'comparison with prior work on acoustic '\n",
      "                                    'embeddings.'},\n",
      "                           {'heading': '4 EXPERIMENTS AND RESULTS',\n",
      "                            'text': 'The ultimate goal is to gain improvements '\n",
      "                                    'in speech systems where word-level '\n",
      "                                    'discrimination is needed, such as speech '\n",
      "                                    'recognition and query-by-example search. '\n",
      "                                    'However, in order to focus on the content '\n",
      "                                    'of the embeddings themselves and to more '\n",
      "                                    'quickly compare a variety of models, it '\n",
      "                                    'is desirable to have surrogate tasks that '\n",
      "                                    'serve as intrinsic measures of '\n",
      "                                    'performance. Here we consider three forms '\n",
      "                                    'of evaluation, all based on measuring '\n",
      "                                    'whether cosine distances between learned '\n",
      "                                    'embeddings correspond well to desired '\n",
      "                                    'properties.\\n'\n",
      "                                    'In the first task, acoustic word '\n",
      "                                    'discrimination, we are given a pair of '\n",
      "                                    'acoustic sequences and must decide '\n",
      "                                    'whether they correspond to the same word '\n",
      "                                    'or to different words. This task has been '\n",
      "                                    'used in several prior papers on acoustic '\n",
      "                                    'word embeddings Kamper et al. (2015, '\n",
      "                                    '2016); Chung et al. (2016); Settle & '\n",
      "                                    'Livescu (2016) and is a proxy for '\n",
      "                                    'query-by-example search. For each given '\n",
      "                                    'spoken word pair, we calculate the cosine '\n",
      "                                    'distance between their embeddings. If the '\n",
      "                                    'cosine distance is below a threshold, we '\n",
      "                                    'output “yes” (same word), otherwise we '\n",
      "                                    'output “no” (different words). The '\n",
      "                                    'performance measure is the average '\n",
      "                                    'precision (AP), which is the area under '\n",
      "                                    'the precision-recall curve generated by '\n",
      "                                    'varying the threshold and has a maximum '\n",
      "                                    'value of 1.\\n'\n",
      "                                    'In our multi-view setup, we embed not '\n",
      "                                    'only the acoustic words but also the '\n",
      "                                    'character sequences. This allows us to '\n",
      "                                    'use our embeddings also for tasks '\n",
      "                                    'involving comparisons between written and '\n",
      "                                    'spoken words. For example, the standard '\n",
      "                                    'task of spoken term detection (Fiscus et '\n",
      "                                    'al., 2007) involves searching for '\n",
      "                                    'examples of a given text query in spoken '\n",
      "                                    'documents. This task is identical to '\n",
      "                                    'queryby-example except that the query is '\n",
      "                                    'given as text. In order to explore the '\n",
      "                                    'potential of multi-view embeddings for '\n",
      "                                    'such tasks, we design another proxy task, '\n",
      "                                    'cross-view word discrimination. Here we '\n",
      "                                    'are given a pair of inputs, one a written '\n",
      "                                    'word and one an acoustic word segment, '\n",
      "                                    'and our task is to determine if the '\n",
      "                                    'acoustic signal is an example of the '\n",
      "                                    'written word. The evalution proceeds '\n",
      "                                    'analogously to the acoustic word '\n",
      "                                    'discrimination task: We output “yes” if '\n",
      "                                    'the cosine distance between the '\n",
      "                                    'embeddings of the written and spoken '\n",
      "                                    'sequences are below some threshold, and '\n",
      "                                    'measure performance as the average '\n",
      "                                    'precision (AP) over all thresholds.\\n'\n",
      "                                    'Finally, we also would like to obtain a '\n",
      "                                    'more fine-grained measure of whether the '\n",
      "                                    'learned embeddings capture our intuitive '\n",
      "                                    'sense of similarity between words. Being '\n",
      "                                    'able to capture word similarity may also '\n",
      "                                    'be useful in building query or '\n",
      "                                    'recognition systems that fail gracefully '\n",
      "                                    'and produce humanlike errors. For this '\n",
      "                                    'purpose we measure the rank correlation '\n",
      "                                    'between embedding distances and character '\n",
      "                                    'edit distances. This is analogous to the '\n",
      "                                    'evaluation of semantic word embeddings '\n",
      "                                    'via the rank correlation between '\n",
      "                                    'embedding distances and human similarity '\n",
      "                                    'judgments (Finkelstein et al., 2001; Hill '\n",
      "                                    'et al., 2015). In our case, however, we '\n",
      "                                    'do not use human judgments since the '\n",
      "                                    'ground-truth edit distances themselves '\n",
      "                                    'provide a good measure. We refer to this '\n",
      "                                    'as the word similarity task, and we apply '\n",
      "                                    'this measure to both pairs of acoustic '\n",
      "                                    'embeddings and pairs of character '\n",
      "                                    'sequence embeddings. Similar measures '\n",
      "                                    'have been proposed by Ghannay et al. '\n",
      "                                    '(2016) to evaluate acoustic word '\n",
      "                                    'embeddings, although they considered only '\n",
      "                                    'near neighbors of each word whereas we '\n",
      "                                    'consider the correlation across the full '\n",
      "                                    'range of word pairs.\\n'\n",
      "                                    'In the experiments described below, we '\n",
      "                                    'first focus on the acoustic word '\n",
      "                                    'discrimination task for purposes of '\n",
      "                                    'initial exploration and hyperparameter '\n",
      "                                    'search, and then largely fix the models '\n",
      "                                    'for evaluation using the cross-view word '\n",
      "                                    'discrimination and word similarity '\n",
      "                                    'measures.'},\n",
      "                           {'heading': '4.1 DATA',\n",
      "                            'text': 'We use the same experimental setup and '\n",
      "                                    'data as in Kamper et al. (2015, 2016); '\n",
      "                                    'Settle & Livescu (2016). The task and '\n",
      "                                    'setup were first developed by (Carlin et '\n",
      "                                    'al., 2011). The data is drawn from the '\n",
      "                                    'Switchboard English conversational speech '\n",
      "                                    'corpus (Godfrey et al., 1992). The spoken '\n",
      "                                    'word segments range in duration from 50 '\n",
      "                                    'to 200 frames (0.5 - 2 seconds). The '\n",
      "                                    'train/dev/test splits contain '\n",
      "                                    '9971/10966/11024 pairs of acoustic '\n",
      "                                    'segments and character sequences, '\n",
      "                                    'corresponding to 1687/3918/3390 unique '\n",
      "                                    'words. In computing the AP for the dev or '\n",
      "                                    'test set, all pairs in the set are used, '\n",
      "                                    'yielding approximately 60 million word '\n",
      "                                    'pairs.\\n'\n",
      "                                    'The input to the embedding model in the '\n",
      "                                    'acoustic view is a sequence of '\n",
      "                                    '39-dimensional vectors (one per frame) of '\n",
      "                                    'standard mel frequency cepstral '\n",
      "                                    'coefficients (MFCCs) and their first and '\n",
      "                                    'second derivatives. The input to the '\n",
      "                                    'character sequence embedding model is a '\n",
      "                                    'sequence of 26-dimensional one-hot '\n",
      "                                    'vectors indicating each character of the '\n",
      "                                    'word’s orthography.'},\n",
      "                           {'heading': '4.2 MODEL DETAILS AND HYPERPARAMETER '\n",
      "                                       'TUNING',\n",
      "                            'text': 'We experiment with different neural '\n",
      "                                    'network architectures for each view, '\n",
      "                                    'varying the number of stacked LSTM '\n",
      "                                    'layers, the number of hidden units for '\n",
      "                                    'each layer, and the use of single- or '\n",
      "                                    'bidirectional LSTM cells. A coarse grid '\n",
      "                                    'search shows that 2-layer bidirectional '\n",
      "                                    'LSTMs with 512 hidden units per direction '\n",
      "                                    'per layer perform well on the acoustic '\n",
      "                                    'word discrimination task, and we keep '\n",
      "                                    'this structure fixed for subsequent '\n",
      "                                    'experiments (see Appendix A for more '\n",
      "                                    'details). We use the outputs of the '\n",
      "                                    'top-layer LSTMs as the learned embedding '\n",
      "                                    'for each view, which is 1024-dimensional '\n",
      "                                    'if bidirectional LSTMs are used.\\n'\n",
      "                                    'In training, we use dropout on the inputs '\n",
      "                                    'of the acoustic view and between stacked '\n",
      "                                    'layers for both views. The architecture '\n",
      "                                    'is illustrated in Figure 1. For each '\n",
      "                                    'training example, our contrastive losses '\n",
      "                                    'require a corresponding negative example. '\n",
      "                                    'We generate a negative character label '\n",
      "                                    'sequence by uniformly sampling a word '\n",
      "                                    'label from the training set that is '\n",
      "                                    'different from the positive label. We '\n",
      "                                    'perform a new negative label sampling at '\n",
      "                                    'the beginning of each epoch. Similarly, '\n",
      "                                    'negative acoustic feature sequences are '\n",
      "                                    'uniformly sampled from all of the '\n",
      "                                    'differently labeled acoustic feature '\n",
      "                                    'sequences in the training set.\\n'\n",
      "                                    'The network weights are initialized with '\n",
      "                                    'values sampled uniformly from the range '\n",
      "                                    '[−0.05, 0.05]. We use the Adam optimizer '\n",
      "                                    '(Kingma & Ba, 2015) for updating the '\n",
      "                                    'weights using mini-batches of 20 acoustic '\n",
      "                                    'segments, with an initial learning rate '\n",
      "                                    'tuned over {0.0001, 0.001}. Dropout is '\n",
      "                                    'used at each layer, with the rate tuned '\n",
      "                                    'over {0, 0.2, 0.4, 0.5}, in which 0.4 '\n",
      "                                    'usually outperformed others. The margin '\n",
      "                                    'in our basic contrastive objectives 0-3 '\n",
      "                                    'is tuned over {0.3, 0.4, 0.5, 0.6, 0.7}, '\n",
      "                                    'out of which 0.4 and 0.5 typically yield '\n",
      "                                    'best results. For obj0 with the '\n",
      "                                    'cost-sensitive margin, we tune the '\n",
      "                                    'maximum margin mmax over {0.5, 0.6, 0.7} '\n",
      "                                    'and the threshold tmax over {9, 11, 13}. '\n",
      "                                    'We train each model for up to 1000 '\n",
      "                                    'epochs. The model that gives the best AP '\n",
      "                                    'on the development set is used for '\n",
      "                                    'evaluation on the test set.'},\n",
      "                           {'heading': '4.3 EFFECTS OF DIFFERENT OBJECTIVES',\n",
      "                            'text': 'We presented four contrastive losses '\n",
      "                                    '(3)–(6) and potential combinations in '\n",
      "                                    'Section 2.1. We now explore the effects '\n",
      "                                    'of these different objectives on the word '\n",
      "                                    'discrimination tasks.\\n'\n",
      "                                    'Table 1 shows the development set AP for '\n",
      "                                    'acoustic and cross-view word '\n",
      "                                    'discrimination achieved using the various '\n",
      "                                    'objectives. We tuned the objectives for '\n",
      "                                    'the acoustic discrimination task, and '\n",
      "                                    'then used the corresponding converged '\n",
      "                                    'models for the cross-view task. Of the '\n",
      "                                    'simple contrastive objectives, obj0 and '\n",
      "                                    'obj2 (which involve only cross-view '\n",
      "                                    'distances) slightly outperform the other '\n",
      "                                    'two on the acoustic word discrimination '\n",
      "                                    'task. The best-performing objective is '\n",
      "                                    'the “symmetrized” objective obj0 + obj2, '\n",
      "                                    'which significantly outperforms all '\n",
      "                                    'individual objectives (and the '\n",
      "                                    'combination of the four). Finally, the '\n",
      "                                    'cost-sensitive objective is very '\n",
      "                                    'competitive as well, while falling '\n",
      "                                    'slightly short of the best performance. '\n",
      "                                    'We note that a similar objective to our '\n",
      "                                    'obj0 + obj2 was used by Vendrov et al. '\n",
      "                                    '(2016) for the task of caption-image '\n",
      "                                    'retrieval, where the authors essentially '\n",
      "                                    'use all non-paired\\n'\n",
      "                                    'Objective Dev AP Dev AP (acoustic) '\n",
      "                                    '(cross-view)\\n'\n",
      "                                    'obj0 0.659 0.791 obj1 0.654 0.807 obj2 '\n",
      "                                    '0.675 0.788 obj3 0.640 0.782 obj0 + obj2 '\n",
      "                                    '0.702 0.814∑3\\n'\n",
      "                                    'i=0 obj i 0.672 0.804 cost-sensitive '\n",
      "                                    '0.671 0.802\\n'\n",
      "                                    'Table 1: Word discrimination performance '\n",
      "                                    'with different objectives.\\n'\n",
      "                                    'examples from the other view in the '\n",
      "                                    'minibatch as negative examples (instead '\n",
      "                                    'of random sampling one negative example '\n",
      "                                    'as we do) to be contrasted with one '\n",
      "                                    'paired example.\\n'\n",
      "                                    'Figure 2 shows the progression of the '\n",
      "                                    'development set AP for acoustic word '\n",
      "                                    'discrimination over 1000 training epochs, '\n",
      "                                    'using several of the objectives, where AP '\n",
      "                                    'is evaluated every 5 epochs. We observe '\n",
      "                                    'that even after 1000 epochs, the '\n",
      "                                    'development set AP has not quite '\n",
      "                                    'saturated, indicating that it may be '\n",
      "                                    'possible to further improve performance.\\n'\n",
      "                                    'Overall, our best-performing objective is '\n",
      "                                    'the combined obj0+obj2, and we use it for '\n",
      "                                    'reporting final test-set results. Table 2 '\n",
      "                                    'shows the test set AP for both the '\n",
      "                                    'acoustic and cross-view tasks using our '\n",
      "                                    'final model (“multi-view LSTM”). For '\n",
      "                                    'comparison, we also include acoustic word '\n",
      "                                    'discrimination results reported '\n",
      "                                    'previously by Kamper et al. (2016); '\n",
      "                                    'Settle & Livescu (2016). Previous '\n",
      "                                    'approaches have not addressed the problem '\n",
      "                                    'of learning embeddings jointly with the '\n",
      "                                    'text view, so they can not be evaluated '\n",
      "                                    'on the cross-view task.'},\n",
      "                           {'heading': '4.4 WORD SIMILARITY TASKS',\n",
      "                            'text': 'Table 3 gives our results on the word '\n",
      "                                    'similarity tasks, that is the rank '\n",
      "                                    'correlation (Spearman’s ρ) between '\n",
      "                                    'embedding distances and orthographic edit '\n",
      "                                    'distance (Levenshtein distance between '\n",
      "                                    'character sequences). We measure this '\n",
      "                                    'correlation for both our acoustic word '\n",
      "                                    'embeddings and for our text embeddings. '\n",
      "                                    'In the case of the text embeddings, we '\n",
      "                                    'could of course directly measure the '\n",
      "                                    'Levenshtein distance between the inputs; '\n",
      "                                    'here we are simply measuring how much of '\n",
      "                                    'this information the text embeddings are '\n",
      "                                    'able to retain.\\n'\n",
      "                                    'Interestingly, while the cost-sensitive '\n",
      "                                    'objective did not produce substantial '\n",
      "                                    'gains on the word discrimination tasks '\n",
      "                                    'above, it does greatly improve the '\n",
      "                                    'performance on this word similarity '\n",
      "                                    'measure. This is a satisfying '\n",
      "                                    'observation, since the cost-sensitive '\n",
      "                                    'loss is trying to improve precisely this '\n",
      "                                    'relationship between distances in the '\n",
      "                                    'embedding space and the orthographic edit '\n",
      "                                    'distance.\\n'\n",
      "                                    'Although we have trained our embeddings '\n",
      "                                    'using orthographic labels, it is also '\n",
      "                                    'interesting to consider how closely '\n",
      "                                    'aligned the embeddings are with the '\n",
      "                                    'corresponding phonetic pronunciations. '\n",
      "                                    'For comparison, the rank correlation '\n",
      "                                    'between our acoustic embeddings and '\n",
      "                                    'phonetic edit distances is 0.226, and for '\n",
      "                                    'our text embeddings it is 0.241, which '\n",
      "                                    'are relatively close to the rank '\n",
      "                                    'correlations with orthographic edit '\n",
      "                                    'distance. A future direction is to '\n",
      "                                    'directly train embeddings with phonetic '\n",
      "                                    'sequence supervision rather than '\n",
      "                                    'orthography; this setting involves '\n",
      "                                    'somewhat stronger supervision, but it is '\n",
      "                                    'easy to obtain in many cases.\\n'\n",
      "                                    'Another interesting point is that the '\n",
      "                                    'performance is not a great deal better '\n",
      "                                    'for the text embeddings than for the '\n",
      "                                    'acoustic embeddings, even though the text '\n",
      "                                    'embeddings have at their disposal the '\n",
      "                                    'text input itself. We believe this has to '\n",
      "                                    'do with the distribution of words in our '\n",
      "                                    'data: While the data includes a large '\n",
      "                                    'variety of words, it does not include '\n",
      "                                    'many very similar pairs. In fact, of all '\n",
      "                                    'possible pairs of unique training set '\n",
      "                                    'words, fewer than 2% have an edit '\n",
      "                                    'distance below 5 characters. Therefore, '\n",
      "                                    'there may not be sufficient information '\n",
      "                                    'to learn to distinguish detailed '\n",
      "                                    'differences among character sequences, '\n",
      "                                    'and the cost-sensitive loss ultimately '\n",
      "                                    'does not learn much more than to separate '\n",
      "                                    'different words. In future work it would '\n",
      "                                    'be interesting to experiment with data '\n",
      "                                    'sets that have a larger variety of '\n",
      "                                    'similar words.'},\n",
      "                           {'heading': '4.5 VISUALIZATION OF LEARNED '\n",
      "                                       'EMBEDDINGS',\n",
      "                            'text': 'Figure 3 gives a 2-dimensional t-SNE (van '\n",
      "                                    'der Maaten & Hinton, 2008) visualization '\n",
      "                                    'of selected acoustic and character '\n",
      "                                    'sequences from the development set, '\n",
      "                                    'including some that were seen in the '\n",
      "                                    'training set and some previously unseen '\n",
      "                                    'words. The previously seen words in this '\n",
      "                                    'figure were selected uniformly at random '\n",
      "                                    'among those that appear at least 15 times '\n",
      "                                    'in the development set (the unseen words '\n",
      "                                    'are the only six that appear at least 15 '\n",
      "                                    'times in the development set). This '\n",
      "                                    'visualization demonstrates that the '\n",
      "                                    'acoustic embeddings cluster very tightly '\n",
      "                                    'and are very close to the text '\n",
      "                                    'embeddings, and that unseen words cluster '\n",
      "                                    'nearly as well as previously seen ones.\\n'\n",
      "                                    'While Figure 3 shows the relationship '\n",
      "                                    'among the multiple acoustic embeddings '\n",
      "                                    'and the text embeddings, the words are '\n",
      "                                    'all very different so we cannot draw '\n",
      "                                    'conclusions about the relationships '\n",
      "                                    'between words. Figure 4 provides another '\n",
      "                                    'visualization, this time exploring the '\n",
      "                                    'relationship among the text embeddings of '\n",
      "                                    'a number of closely related words, namely '\n",
      "                                    'all development set words ending in '\n",
      "                                    '“-ly”, “-ing”, and “-tion”. This '\n",
      "                                    'visualization confirms that related words '\n",
      "                                    'are embedded close together, with the '\n",
      "                                    'words sharing a suffix forming fairly '\n",
      "                                    'well-defined clusters.'},\n",
      "                           {'heading': '5 CONCLUSION',\n",
      "                            'text': 'We have presented an approach for jointly '\n",
      "                                    'learning acoustic word embeddings and '\n",
      "                                    'their orthographic counterparts. This '\n",
      "                                    'multi-view approach produces improved '\n",
      "                                    'acoustic word embedding performance over '\n",
      "                                    'previous approaches, and also has the '\n",
      "                                    'benefit that the same embeddings can be '\n",
      "                                    'applied for both spoken and written query '\n",
      "                                    'tasks. We have explored a variety of '\n",
      "                                    'contrastive objectives: ones with a fixed '\n",
      "                                    'margin that aim to separate same and '\n",
      "                                    'different word pairs, as well as a '\n",
      "                                    'cost-sensitive loss that aims to capture '\n",
      "                                    'orthographic edit distances. While the '\n",
      "                                    'losses generally perform similarly for '\n",
      "                                    'word discrimination tasks, the '\n",
      "                                    'cost-sensitive loss improves the '\n",
      "                                    'correlation between embedding distances '\n",
      "                                    'and orthographic distances. One '\n",
      "                                    'interesting direction for future work is '\n",
      "                                    'to directly use knowledge about phonetic '\n",
      "                                    'pronunciations, in both evaluation and '\n",
      "                                    'training. Another direction is to extend '\n",
      "                                    'our approach to directly train on both '\n",
      "                                    'word and non-word segments.'},\n",
      "                           {'heading': 'ACKNOWLEDGMENTS',\n",
      "                            'text': 'This research was supported by a Google '\n",
      "                                    'Faculty Award and by NSF grant '\n",
      "                                    'IIS-1321015. The opinions expressed in '\n",
      "                                    'this work are those of the authors and do '\n",
      "                                    'not necessarily reflect the views of the '\n",
      "                                    'funding agency. This research used GPUs '\n",
      "                                    'donated by NVIDIA Corporation. We thank '\n",
      "                                    'Herman Kamper and Shane Settle for their '\n",
      "                                    'assistance with the data and experimental '\n",
      "                                    'setup.'},\n",
      "                           {'heading': 'A ADDITIONAL ANALYSIS',\n",
      "                            'text': 'We first explore the effect of network '\n",
      "                                    'architectures for our embedding models. '\n",
      "                                    'We learn embeddings using objective obj0 '\n",
      "                                    'and evaluate them on the acoustic and '\n",
      "                                    'cross-view word discrimination tasks. The '\n",
      "                                    'resulting average precisions on the '\n",
      "                                    'development set are given in Table 4. All '\n",
      "                                    'of the models were trained for 1000 '\n",
      "                                    'epochs, except for the 1-layer '\n",
      "                                    'unidirectional models which converged '\n",
      "                                    'after 500 epochs. It is clear that '\n",
      "                                    'bidirectional LSTMs are more successful '\n",
      "                                    'than unidirectional LSTMs for these '\n",
      "                                    'tasks, and two layers of LSTMs are much '\n",
      "                                    'better than a single layer of LSTMs. We '\n",
      "                                    'did not observe significant further '\n",
      "                                    'improvement by using more than two layers '\n",
      "                                    'of LSTMs. For all other experiments, we '\n",
      "                                    'fix the architecture to 2-layer '\n",
      "                                    'bidirectional LSTMs for each view.\\n'\n",
      "                                    'In Figure 5 we also give the '\n",
      "                                    'precision-recall curve for our best '\n",
      "                                    'models, as well as the scatter plot of '\n",
      "                                    'cosine distances between acoustic '\n",
      "                                    'embeddings vs. orthographic edit '\n",
      "                                    'distances.'}],\n",
      "              'source': 'CRF',\n",
      "              'title': 'MULTI-VIEW RECURRENT NEURAL ACOUSTIC WORD EMBEDDINGS',\n",
      "              'year': 2017},\n",
      " 'name': 'rJxDkvqee.pdf'}\n"
     ]
    }
   ],
   "source": [
    "pprint(paper_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['metadata', 'name'])\n",
      "dict_keys(['creator', 'sections', 'authors', 'referenceMentions', 'title', 'source', 'abstractText', 'emails', 'references', 'year'])\n"
     ]
    }
   ],
   "source": [
    "print(paper_data[0].keys())\n",
    "print(paper_data[0]['metadata'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
